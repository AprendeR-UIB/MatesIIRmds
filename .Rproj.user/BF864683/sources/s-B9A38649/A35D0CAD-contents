---
title: "Regressió lineal"
output:
  html_document: default
  pdf_document: default
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.height=6, fig.align="center", echo=TRUE, warning=FALSE, message=FALSE,error=FALSE)
library(knitr)
set.seed(42)
```

#### Exemple 1

La taula següent dóna l'alçada mitjana (en cm) dels nins a unes determinades edats (en anys):


```{r, echo=FALSE}
edat=c(1,3,5, 7, 9, 11, 13)
alçada=c( 75, 92, 108, 121, 130 , 142, 155)
df_edat=data.frame(edat,alçada)
kable(df_edat)
```

Volem calcular la millor relació lineal $\mbox{alçada}\approx b_0+b_1\cdot\mbox{edat}$. A Matemàtiques I obteníem aquesta recta de la manera següent:  


```{r}
edat=c(1,3,5,7,9,11,13)
alçada=c(75,92,108,121,130,142,155)
lm(alçada~edat)
plot(edat,alçada)
abline(lm(alçada~edat),col="red")
```

Obtenim la recta $y=`r round(lm(alçada~edat)$coefficients[1],3)`+`r round(lm(alçada~edat)$coefficients[2],3)`x$. 


Com es calcula aquesta recta? A mà, amb les fórmules explicades a teoria, faríem:


```{r}
x.b=mean(edat)
y.b=mean(alçada)
s2.x=var(edat)
s2.y=var(alçada)
s.xy=cov(edat,alçada)
round(c(x.b,y.b,s2.x,s2.y,s.xy),3)
b1=s.xy/s2.x
b0=y.b-b1*x.b
c(b0,b1)
```

Obtenim la recta $y=`r round(b0,3)`+`r round(b1,3)`x$, que és la d'abans. Aquests coeficients es calculen de manera directa amb

```{r}
lm(alçada~edat)$coefficients
```

El coeficient de determinació es calcularia a mà amb:

```{r}
y.cap=b0+b1*edat
errors=alçada-y.cap
SST=sum((alçada-mean(alçada))^2)
SSR=sum((y.cap-mean(alçada))^2)
SSE=sum(errors^2)
R2=SSR/SST
R2
```

Comprovem que dóna la correlació al quadrat i també que coincideix amb el que dóna R amb `lm`

```{r}
cor(edat,alçada)^2
summary(lm(alçada~edat))$r.squared
```

I ja que hi som, comprovem en aquest exemple la identitat de les sumes de quadrats

```{r}
SST
SSR+SSE
```

Si suposam que els errors tenen la mateixa variància $\sigma_E^2$ i són incorrelats, podem estimar aquesta variància $\sigma_E^2$ a mà amb:

```{r}
S2=SSE/(length(edat)-2)
S2
```

Amb R, tot això es fa amb la funció `lm`. Aquesta funció `lm` ens dóna un munter d'informació que podem cridar amb `summary`:

```{r}
summary(lm(alçada~edat))
```

Aquí:

* La filera **Residuals** són els errors
```{r}
lm(alçada~edat)$residuals
errors
```


* La columna **Estimate** ens dóna el terme independendent ($b_0$, filera **(Intercept)**) i el pendent ($b_1$, filera **edat**) de la recta de regressió
* La columna **Std. Error** dóna els errors típics sobre la nostra mostra d'aquests valors com a estimadors dels valors poblacionals $\beta_0$ i $\beta_1$
* La columna **Pr(>|t|)** dóna els p-valors dels contrastos si $\beta_0$ i $\beta_1$ són 0 o no, cada un a la seva filera.
* El **Residual standard error** és l'estimació de la desviació típica comuna $\sigma_E$ (l'arrel  quadrada del nostre $S^2$) de les poblacions definides pels nivells
* La **Multiple R-squared** és el coeficient de determinació $R^2$
* Els tres darrers valors, **Adjusted R-squared**, **F-statistic** i  **p-value** no són interessants en la regressió simple (bé, el *p-value* torna a ser el de la filera **edat** i per tant és el del contrast si $\beta_1=0$ o $\beta_1\neq 0$)

Comprovar les condicions per poder emprar les distribucions mostrals de $b_0$ i $b_1$
(que les variables aleatòries que defienixen els errors segueixin distribucions normals de mitjana 0 i la mateixa desviació típica $\sigma_E$ i que siguin dues a dues incorrelades) no es pot contrastar si a la nostra mostra hi  tenim un únic valor de la variable dependent per a cada valor de la variable independent de la nostra mostra, però aquestes condicions impliquen que els errors provenen d'una $N(0,\sigma_E)$, amb $\sigma_E^2$ estimada per $S^2$, i això sí que ho podem contrastar:

```{r}
ks.test(errors,"pnorm",0,sqrt(S2))
```

Com que el p-valor és gran, no podem rebutjar que se satisfacin les condicions demanades.
Aleshores, podem calcular intervals de confiança per als coeficients de la recta de regressió aplicant la funció `confint` al resultat de `lm`. En el nostre exemple, els IC de 95% de confiança es calculen amb

```{r}
confint(lm(alçada~edat),level=0.95)
```

De la mateixa manera, podem calcular intervals de confiança per al valor esperat de la variable dependent i el valor de la variable dependent sobre un individu concret que tengui valor donat de la variable independent.

Per exemple, els IC 95% per a l'alçada mitjana dels nins de 10 anys i per a l'alçada predita d'un nin de 10 anys es calculen, respectivament, amb:

```{r}
newdata=data.frame(edat=10) #Introduim el nou valor de l'edat
predict.lm(lm(alçada~edat),newdata,interval="prediction",level =0.95)
predict.lm(lm(alçada~edat),newdata,interval="confidence",level =0.95)
```

Observau que el valor predit ens els dos casos és el mateix, però l'interval de confiança per al valor mitjà (el primer) és més estret.

#### La "regressió a la mediocritat"

La taula de dades `Galton` del paquet **HistData**, que conté una sèrie de conjunts històrics de dades, conté la següent informació sobre 928 homes i dones adults: `parent`, amb l'alçada mitjana del pare i la mare, i `child`, l'alçada de l'individu. Anem a dibuixar aquests punts, la recta de regressió lineal i la diagonal (amb `col=rgb(0,0,0,alpha=0.1)` fem que els punts siguin lleugerament "transparents" i d'aquesta manera com més fosc és el punt, més punts s'hi acumulen a sobre..

```{r}
library(HistData)
plot(Galton, xlab="pares",ylab="fills",pch=20,xlim=c(62,74),ylim=c(62,74),col=rgb(0,0,0,alpha=0.1))
abline(lm(child~parent,data=Galton),col="red")
abline(0,1,lty=2)
```


La recta de regressió lineal té pendent $b_1=`r round(lm(child~parent,data=Galton)$coefficient[2],3)`$, i com veieu és inferior a 1, amb la qual cosa els fills de pares alts tendeixen a ser més baixos que els pares i els fills de pares baixos tendeixen a ser més alts que els pares. En Galton, que és qui recollí les dades, va descriure aquest efecte com a *regressió a la mediocritat* i d'aquí es va quedar el terme *regressió* per descriure la tècnica d'ajustar una funció a un conjunt de punts. Però la fórmula per calcular la recta que minimitza la suma dels quadrats dels errors és 100 anys anterior, obra de Gauss (1795).


#### Exemple 2

En un experiment on es volia estudiar l'associació entre consum de sal i pressió arterial, a alguns individus se'ls assignà aleatòriament una quantitat diària constant de sal en la seva dieta, i al cap d'un mes se'ls mesurà la tensió mitjana. La taula següent dóna alguns dels resultats obtinguts, on hi donam la quantitat diària de sal en grams, i l tensió en mm de Hg:


```{r, echo=FALSE}
sal=c(1.8, 2.2,3.5,4.0,4.3,5.0)
tensió=c(100,98,110,110,112,120)
df_sal=data.frame(sal,tensió)
kable(df_sal)
```



Volem trobar la recta de regressió lineal per mínims quadrats de la variable $Y$, tensió mitjana, en funció de la variable $X$, quantitat de sal assignada.


```{r}
sal=c(1.8,2.2,3.5,4,4.3,5)
ten=c(100,98,110,110,112,120)
summary(lm(ten~sal))
```


Obtenim la recta $y=`r round(lm(ten~sal)$coefficients[1],3)`+`r round(lm(ten~sal)$coefficients[2],3)`x$ amb $R^2=`r round(summary(lm(ten~sal))$r.squared,4)`$. A més el p-valor del contrast bilateral amb hipòtesi nul·la $\beta_1=0$ és `r round(summary(lm(ten~sal))$coefficients[2,4],6)`, i per tant tenim evidència significativa que $\beta_1\neq 0$. 


#### El quartet d'Anscombe

No es pot valorar la bondat del model només amb $R^2$. Considerau les 4 parelles de variables $(x_i,y_i)$ del data frame **anscombe** de R, construït per F. Anscombe l'any 1973

```{r}
data(anscombe)
str(anscombe)
```

Anem a fer-ne les regressions i a mostrar-ne els $R^2$ respectius i l'ajustament gràfic de les rectes.

```{r}
summary(lm(y1~x1,data=anscombe))$r.squared
summary(lm(y2~x2,data=anscombe))$r.squared
summary(lm(y3~x3,data=anscombe))$r.squared
summary(lm(y4~x4,data=anscombe))$r.squared
```
```{r}
plot(y1~x1,data=anscombe)
abline(lm(y1~x1,data=anscombe),col=2)
```
```{r}
plot(y2~x2,data=anscombe)
abline(lm(y2~x2,data=anscombe),col=2)
```
```{r}
plot(y3~x3,data=anscombe)
abline(lm(y3~x3,data=anscombe),col=2)
```
```{r}
plot(y4~x4,data=anscombe)
abline(lm(y4~x4,data=anscombe),col=2)
```


#### El datasaure

El fitxer "Datasaurus.txt"  que acompanya aquesta lliçó conté alguns conjunts de dades de diferents formes i  estadístics bàsics molt similars. Té tres variables: una variable "dataset" que indica el conjunt de dades, i les variables "x" i "y" que donen les coordenades dels punts que formen cada conjunt de dades. Considerarem dos conjunts d'aquests: el dinosaure i l'estrella.

```{r}
datasaurus=read.table("Datasaurus.txt",header=TRUE,sep="\t")
str(datasaurus)
dino=datasaurus[datasaurus$dataset=="dino",2:3]
star=datasaurus[datasaurus$dataset=="star",2:3]
plot(dino,pch=20)
abline(lm(dino$y~dino$x),col="red")
summary(lm(dino$y~dino$x))$r.squared
plot(star,pch=20)
abline(lm(star$y~star$x),col="red")
summary(lm(star$y~star$x))$r.squared
```

Com veieu les dues regressions lineals tenen $R^2=`r summary(lm(star$y~star$x))$r.squared`.

#### Correlació no és causalitat

Les dades sobre nombres de morts accidentals als EEUU per estrangulament amb els llençols i consum *per capita* de formatge als EEUU (en lliures), davallades de https://tylervigen.com/spurious-correlations, són les següents:



```{r,echo=FALSE}
any=c(2000,2001,2002,2003,2004,2005,2006,2007,2008,2009)
morts=c(327,456,509,497,596,573,661,741,809,717)
formatge=c(29.8,30.1,30.5,30.6,31.3,31.7,32.6,33.1,32.7,32.8)
df_formatge=data.frame(any,morts,formatge)
kable(df_formatge,col.names=c("any","morts","consum de formatge"))
```


Calculem i estudiem la recta de regressió del nombre de morts en funció del consum de formatge


```{r}
any=c(2000,2001,2002,2003,2004,2005,2006,2007,2008,2009)
morts=c(327,456,509,497,596,573,661,741,809,717)
formatge=c(29.8,30.1,30.5,30.6,31.3,31.7,32.6,33.1,32.7,32.8)
plot(formatge,morts,pch=20,xlab="morts",ylab="consum de formatge")
abline(lm(morts~formatge),lwd=2,col="red")
summary(lm(morts~formatge))
```


Obtenim la recta $y=`r round(lm(morts~formatge)$coefficients[1],3)`+`r round(lm(morts~formatge)$coefficients[2],3)`x$ amb $R^2=`r round(summary(lm(morts~formatge))$r.squared,4)`$, que correspon a una correlació $r_{formatge,morts}=`r round(cor(morts,formatge),3)`$. 

Per tant, hi ha una forta relació lineal entre aquestes dues variables, i estimam que un augment en el consum mitjà de formatge de 1 lliura per persona correspon a un increment del nombre de morts per estrangulament accidental amb els llençols `r round(lm(morts~formatge)$coefficients[2],1)`, però hauria de ser clar que l'augment en el consum de formatge **no causa** un increment del nombre de morts per estrangulament accidental amb els llençols.


#### Exemple 3

Es postula que l'alçada (esperada) d'un nadó ($y$) té una relació  lineal amb la seva edat en dies ($x_1$), la seva alçada en néixer en cm ($x_2$), el seu pes en kg en néixer ($x_3$) i l'augment en tant per cent del seu pes actual respecte del seu pes en néixer ($x_4$)

Suposam que
$$
\mu_{Y|x_1,x_2,x_3,x_4}=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3+\beta_4x_4
$$
i volem estimar els coeficients $\beta_0,\beta_1,\beta_2,\beta_3,\beta_4$ a partir d'una mostra.

En una mostra de 9 nins, els resultats varen ser els següents:

```{r,echo=FALSE}
x1=c(78,69,77,88,67,80,74,94,102)
x2=c(48.2,45.5,46.3,49.0,43.0,48.0,48.0,53.0,58.0)
x3=c(2.75,2.15,4.41,5.52,3.21,4.32,2.31,4.30,3.71)
x4=c(29.5,26.3,32.2,36.5,27.2,27.7,28.3,30.3,28.7)
y=c(57.5,52.8,61.3,67,53.5,62.7,56.2,68.5,69.2)
df_nins=data.frame(x1,x2,x3,x4,y)
kable(df_nins,col.names=c("x~1~","x~2~","x~3~","x~4~","y"))
```


És un problema de regressió lineal *múltiple*

Per trobar  a mà un vector $(b_0,b_1,b_2,b_3,b_4)$ de coeficients que estimi $(\beta_0,\beta_1,\beta_2,\beta_3,\beta_4)$ pel mètode dels mínims quadrats fem el següent:


```{r}
x1=c(78,69,77,88,67,80,74,94,102)
x2=c(48.2,45.5,46.3,49.0,43.0,48.0,48.0,53.0,58.0)
x3=c(2.75,2.15,4.41,5.52,3.21,4.32,2.31,4.30,3.71)
x4=c(29.5,26.3,32.2,36.5,27.2,27.7,28.3,30.3,28.7)
y=c(57.5,52.8,61.3,67,53.5,62.7,56.2,68.5,69.2)
X=cbind(x0=rep(1,9),x1,x2,x3,x4)
Y=cbind(y)
b=solve(t(X)%*%X)%*%(t(X)%*%Y)
b
```


Obtenim

```{r,echo=FALSE}
b0=round(b[1,1],3)
b1=round(b[2,1],3)
b2=round(b[3,1],3)
b3=round(b[4,1],3)
b4=round(b[5,1],3)
df_bs=data.frame(b0,b1,b2,b3,b4)
kable(df_bs,col.names=c("b~0~","b~1~","b~2~","b~3~","b~4~"))
```

i per tant la funció
$$
y=`r round(b[1,1],3)`+ `r round(b[2,1],3)`x_1+ `r round(b[3,1],3)`x_2+ `r round(b[4,1],3)`x_3 `r round(b[5,1],3)`x_4
$$

Amb R, si només volem els coeficients, podem fer (primer entram les dades en un *dataframe*):

```{r}
X.df=data.frame(y,x1,x2,x3,x4)
str(X.df)
lm(y~x1+x2+x3+x4,data=X.df)$coefficients
```
i si volem la informació completa
```{r}
summary(lm(y~x1+x2+x3+x4,data=X.df))
```

Aquí cal destacar:

* La columna **Estimate** ens dóna els coeficients de la funció lineal
* La columna **Std. Error** ens dóna els errors típics dels $b_i$ com a estimadors dels $\beta_i$
* La columna de p-valors **Pr(>|t|)** ens dóna els p-valors dels contrastos si $\beta_i=0$ o $\beta_i\neq 0$ corresponents. Només obtenim evidència que $\beta_3\neq 0$
* El valor de **Residual standard error** és l'estimació de la desviació típica comuna $\sigma_E$ dels errors
* El valor **Multiple R-squared** és el coeficient de determinació  $R^2$
* El valor **Adjusted R-squared** és el coeficient de determinació ajustat $R_{adj}^2$, que serveix per comparar models de regressió lineal amb diferents nombres de variables independents
* El **p-value** és el de l'ANOVA amb hipòtesi nul·la $\beta_1=\cdots=\beta_4=0$. Com que és petit, concloem que no totes les $\beta_i$ són 0 i per tant que el model lineal és adequat.


Els intervals de confiança dels $\beta_i$ s'obtenen aplicant **confint** al resultat de `lm`

```{r}
round(confint(lm(y~x1+x2+x3+x4,data=X.df)),3)
```

Els intervals de confiança per al valor esperat i el valor predit de la variable dependent $y$
per a uns valors donats de les variables independents s'obtenen com a la regressió simple, amb **predict.lm**:

```{r}
regressio=lm(y~x1+x2+x3+x4,data=X.df)
newdata=data.frame(x1=69,x2=45.5,x3=2.15,x4=26.3)
predict.lm(regressio,newdata,interval="prediction",level=0.95)
predict.lm(regressio,newdata,interval="confidence",level=0.95)
```

