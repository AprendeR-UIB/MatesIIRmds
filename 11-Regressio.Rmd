# Regressió lineal 

Comencem recordant un exemple de Matemàtiques I. 

```{example, edats1}
La taula següent dóna l'alçada mitjana (en cm) dels nins a determinades edats (en anys):

```

```{r, echo=FALSE}
edat=c(1,3,5, 7, 9, 11, 13)
alçada=c(75, 92, 108, 121, 130 , 142, 155)
df_edat=data.frame(edat,alçada)
knitr::kable(df_edat)
```

A Matemàtiques I aprenguéreu a calcular amb R la "millor" relació lineal
$$
\text{alçada}= b_0+b_1\cdot\text{edat}
$$ 
de la manera següent:
```{r}
edat=c(1,3,5,7,9,11,13)
alçada=c(75,92,108,121,130,142,155)
lm(alçada~edat)
```
Obteníeu d'aquesta manera la recta
$$
\text{alçada}=`r round(lm(alçada~edat)$coefficients[1],3)`+`r round(lm(alçada~edat)$coefficients[2],3)`x
$$
i la representàveu amb:
```{r}
plot(edat,alçada,pch=20)
abline(lm(alçada~edat),col="red",xlab="Edat",ylab="Alçada")
```

Ara podíeu emprar aquesta recta per estimar l'alçada d'un nin d'una edat concreta. Per exemple, ens permet estimar que l'alçada d'un nin d'10 anys és
$$
72.321+6.464\cdot 10=136.964,
$$
uns 137 cm.


En aquest tema estudiarem com es calcula aquella recta, què vol dir que sigui "la millor recta" que explica l'alçada dels nins en funció de l'edat, com trobar intervals de confiança per a les estimacions associades a aquesta recta i com tractar el problema més general de trobar "la millor funció lineal" que explica una variable $Y$ en funció de diverses variables $X_1,\ldots,X_k$.

## Regressió lineal simple 

El problema plantejat a l'Exemple \@ref(exm:edats1) és una instància de la situació general en la qual tenim parelles d'observacions de dues variables $X$ i $Y$ sobre una mostra de $n\geqslant 2$ subjectes,
$$
(x_i,y_i)_{i=1,2,\ldots,n},
$$
i volem estudiar com depèn el valor de la variable $Y$ del de $X$. En aquest context:

* Direm que $X$ és la variable **de control** o  **independent**

* Direm que $Y$ l'anomenam la variable **de resposta** o **dependent** 


```{block2,type="rmdnote"}
La variable de control no té per què ser aleatòria: nosaltres podem fixar el seu valor sobre els subjectes. Per exemple, la variable $X$ podria ser la dosi d'una medicació i que nosaltres  decidíssim a cada individu, de manera planificada i gens aleatòria, quina dosi li administram. En canvi, la variable de resposta ha de ser aleatòria. Si no, no té sentit estimar res sobre ella.
```


En general, volem trobar la millor relació funcional (el millor **model estadístic**, amb la terminologia introduïda en el tema anterior) que  expliqui la variable $Y$ en funció de la variable $X$. En aquest tema, cercarem un **model lineal**. Les tècniques que es fan servir per resoldre aquest problema s'anomenen genèricament de **regressió lineal**. Nosaltres n'estudiarem una de concreta: la **regressió lineal per mínims quadrats**.


```{block2,type="rmdnote"}
El nom "regressió" per parlar del tipus de tècniques que permeten ajustar una recta a un conjunt de punts prové del títol d'un article de Galton d'1886, *Regression Towards Mediocrity in Hereditary Stature*. En aquest article hi va analitzar les  alçades d'una mostra de 928 adults i les alçades mitjanes dels seus pares. Hi observà que els pares alts tendien a tenir fills més baixos que ells i que els pares baixos tendien a tenir fills més alts que ells. D'aquest efecte en digué "regressió a la mediocritat" al títol de l'article. D'aquí s'adoptà el terme "regressió" per descriure la tècnica que emprà per obtenir la recta vermella del gràfic següent, amb la qual suportava la seva conclusió comparant-la amb la diagonal (la línia discontínua), i amb el temps el nom de l'efecte que observà es canvià al menys ofensiu "regressió a la mitjana". Més endavant tornarem sobre aquest exemple.
```

```{r, echo=FALSE}
library(HistData)
plot(Galton, xlab="Alçades dels pares",ylab="Alçades dels fills",pch=20,xlim=c(62,74),ylim=c(62,74), col=rgb(0,0,0,alpha=0.1))
abline(lm(child~parent,data=Galton),col="red")
abline(0,1,lty=2)
```



### El model

En el **model de regressió lineal** suposam que existeixen $\beta_0,\beta_1\in \mathbb{R}$ tals que
$$
\mu_{Y|x}=\beta_0+\beta_1 x
$$
on $\mu_{Y|x}$ és el valor esperat de $Y$ sobre els subjectes per als quals $X$ val $x$. Volem estimar aquests paràmetres $\beta_0$ (el **terme independent** del model) i $\beta_1$ (la **pendent** del model) a partir d'una mostra.


```{block2,type="rmdimportant"}
Recordau la interpretació d'una funció lineal $y=a_0+a_1x$: 

* El terme independent $a_0$ és el valor de $y$ quan $x=0$

* La pendent $a_1$ és la variació de $y$ quan $x$ augmenta en 1 unitat

Per tant, en el nostre model de regressió lineal:

* $\beta_0$ és el valor esperat de $Y$ en els subjectes en els quals $X$ val 0

* $\beta_1$ és la variació del valor esperat de $Y$ quan el valor de $X$ augmenta 1 unitat

```

Amb una mostra $(x_i,y_i)_{i=1,2,\ldots,n}$, calcularem estimacions $b_0$ i $b_1$ de
$\beta_0$ i  $\beta_1$. Això ens donarà la **recta de regressió** per a la nostra mostra:
$$
\widehat{Y}=b_0+b_1 X.
$$
Aquesta recta, donat un valor $x_0$ de $X$,  permet estimar el valor  $\widehat{y}_0=b_0+b_1 x_0$ de $Y$ sobre un  subjecte en el qual $X$ valgui $x_0$. Hi empram $\widehat{Y}$ a la dreta per posar èmfasi que no és que $Y$ sigui $b_0+b_1X$, sinó que això darrer estima el valor de $Y$ a partir del valor de $X$. En concret, si $\widehat{y}_0=b_0+b_1 x_0$, direm a $\widehat{y}_0$ el **valor estimat** de $Y$ quan $X=x_0$.

```{block2,type="rmdcaution"}
Fixau-vos que, d'aquesta manera, donada una observació $(x_i,y_i)$ de la nostra mostra, distingim entre

* $y_i$: el valor de $Y$ sobre l'individu corresponent

* $\widehat{y}_i=b_0+b_1 x_i$: l'estimació del valor de $Y$ sobre l'individu corresponent a partir del seu valor de $X$ i la recta de regressió obtinguda

```


El model anterior el reescrivim com a
$$
Y|x=\mu_{Y|x}+ E_x=\beta_0+\beta_1 x+ E_x
$$
on

* $Y|x$ és la variable aleatòria **"valor de $Y$ quan $X$ val $x$"**: Prenem un subjecte en el qual $X$ val $x$ i hi mesuram $Y$

* $\mu_{Y|x}$ és el valor esperat de $Y|x$, és a dir, la mitjana dels valors de $Y$ sobre tots els individus en els quals $X$ valgui $x$

* $E_x=Y|x -\mu_{Y|x}$ és la variable aleatòria **error** o **residu**, que dóna la diferència entre el valor de $Y$ en un individu amb $X=x$ i el seu valor esperat

Prenent valors esperats als dos costats de la igualtat $Y|x=\mu_{Y|x}+ E_x$ obtenim que $\mu_{Y|x}=\mu_{Y|x}+ \mu_{E_x}$ i per tant que $\mu_{E_x}=0$. Així doncs, aquest model implica que **els valors esperats de les variables error $E_x$ són tots 0**.


### Mínims quadrats

L'**error** que cometem amb l'estimació $\widehat{y}_i=b_0+b_1x_i$ a cada observació $(x_i,y_i)$ de la mostra és
$$
e_i=y_i-\widehat{y}_i=y_i-(b_0+b_1 x_i)
$$


La **Suma dels Quadrats dels Errors** d'aquesta estimació és
$$
SS_E=\sum_{i=1}^n e_i^2=\sum_{i=1}^n (y_i-b_0-b_1 x_i)^2
$$
A la **regressió lineal per mínims quadrats**, s'estimen $\beta_0$ i $\beta_1$ per mitjà dels valors de $b_0$ i $b_1$ que minimitzen aquesta  $SS_E$. Aquests valors són donats pel resultat següent:

```{theorem,RLS}
Els estimadors $b_0$ i $b_1$ per mínims quadrats de $\beta_0$ i $\beta_1$ són
$$
b_1 =\frac{{s}_{xy}}{{s}_x^2}=\frac{\widetilde{s}_{xy}}{\widetilde{s}_x^2},\quad b_0 = \overline{y}-b_1 \overline{x}.
$$
```


```{block2,type="rmdcorbes"}
Per trobar-los, empram que els valors de $b_0,b_1$ que fan mínim 
$$
SS_E=\sum_{i=1}^n (y_i-b_0-b_1 x_i)^2
$$
anul·len les derivades de $SS_E$ respecte de $b_0$ i $b_1$.

Derivem:
$$
\begin{array}{l}
\displaystyle\dfrac{\partial SS_E}{\partial b_0}=-2\sum\limits_{i=1}^n (y_i -b_0-b_1 x_i)\\[2ex]
\displaystyle\dfrac{\partial SS_E}{\partial b_1}=-2\sum\limits_{i=1}^n (y_i -b_0-b_1 x_i) x_i 
\end{array}
$$
El $(b_0,b_1)$ que cercam satisfà
$$
\begin{array}{l}
\displaystyle 2\sum\limits_{i=1}^n (y_i -b_0-b_1 x_i)=0\\[2ex]
\displaystyle 2\sum\limits_{i=1}^n (y_i -b_0-b_1 x_i) x_i =0
\end{array}
$$
Ho reescrivim:
$$
\begin{array}{rl}
\displaystyle n b_0 + \Big(\sum\limits_{i=1}^n x_i\Big) b_1 & =\sum\limits_{i=1}^n y_i\\[1ex]
\displaystyle \Big(\sum\limits_{i=1}^n x_i\Big) b_0 + \Big(\sum\limits_{i=1}^n x_i^2\Big) b_1 &=\sum\limits_{i=1}^n x_iy_i
\end{array}
$$
Les solucions són
$$
\begin{array}{rl}
b_1& \displaystyle=\frac{n \sum\limits_{i=1}^n x_i y_i-\sum\limits_{i=1}^n x_i\sum\limits_{i=1}^n y_i} {n\sum\limits_{i=1}^n
x_i^2-\big(\sum\limits_{i=1}^n x_i\big)^2}\\[6ex]
b_0& \displaystyle=\frac{\sum\limits_{i=1}^n y_i -b_1 \sum\limits_{i=1}^n x_i}{n}
\end{array}
$$
i es pot comprovar que donen el mínim de $SS_E$.

Ara, recordant que 
$$
\begin{array}{l}
\displaystyle\overline{x}=\frac{1}{n}\sum\limits_{i=1}^n x_i,
\quad \overline{y}=\frac{1}{n} \sum\limits_{i=1}^n y_i\\[2ex]
\displaystyle s_x^2  =\frac{1}{n}\Big(\sum_{i=1}^n x_i^2\Big) -\overline{x}^2,\quad
\displaystyle s_y^2  =\frac{1}{n}\Big(\sum_{i=1}^n y_i^2\Big) -\overline{y}^2\\[2ex]
\displaystyle s_{xy}  =\frac{1}{n}\Big(\sum_{i=1}^n x_i y_i\Big)-\overline{x}\cdot\overline{y}
\end{array}
$$
s'obté finalment que 
$$
b_1 =\frac{{s}_{xy}}{{s}_x^2},\quad b_0 = \overline{y}-b_1 \overline{x}
$$
```

La igualtat 
$$
\frac{{s}_{xy}}{{s}_x^2}=\frac{\widetilde{s}_{xy}}{\widetilde{s}_x^2}
$$
és conseqüència que, a les dues fraccions, els denominadors del numerador i el denominador se cancel·len:
$$
\frac{{s}_{xy}}{{s}_x^2}=\frac{\frac{\sum_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})}{n}}{\frac{\sum_{i=1}^n (x_i-\overline{x})^2}{n}}=\frac{{\sum_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})}}{{\sum_{i=1}^n (x_i-\overline{x})^2}}=\frac{\frac{\sum_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})}{n-1}}{\frac{\sum_{i=1}^n (x_i-\overline{x})^2}{n-1}}=
\frac{\widetilde{s}_{xy}}{\widetilde{s}_x^2}
$$


Aquests $b_0$ i $b_1$ són els que calcula la funció `lm`. 


```{example, edats}
Calculem la recta de regressió per mínims quadrats de les edats i alçades de l'Exemple \@ref(exm:edats1), que eren

```

```{r, echo=FALSE}
edat=c(1,3,5, 7, 9, 11, 13)
alçada=c(75, 92, 108, 121, 130 , 142, 155)
df_edat=data.frame(edat,alçada)
knitr::kable(df_edat)
```

Començarem trobant els estadístics que ens calen per calcular els coeficients $b_0$ i $b_1$. Ja que hi som, també trobarem  la variància de les alçades, que per calcular $b_0$ i $b_1$ no ens fa falta però que més tard sí que  necessitarem saber-la:

```{r}
edat=c(1,3,5, 7, 9, 11, 13)
alçada=c(75, 92, 108, 121, 130 , 142, 155)
x.b=mean(edat)
y.b=mean(alçada)
s2.x=var(edat)
s2.y=var(alçada)
s.xy=cov(edat,alçada)
round(c(x.b,y.b,s2.x,s2.y,s.xy),3)
```

Obtenim
$$
\begin{array}{cccccccc}
\overline{x} &  \overline{y} & \widetilde{s}_x^2 & \widetilde{s}_y^2 & \widetilde{s}_{xy}\\ \hline
`r round(x.b,3)` & `r round(y.b,3)` & `r round(s2.x,3)` & `r round(s2.y,3)` & `r round(s.xy,3)`
\end{array}
$$
Aleshores
$$
\begin{array}{l}
\displaystyle b_1 =\frac{\widetilde{s}_{xy}}{\widetilde{s}_x^2}=\frac{`r round(s.xy,3)`}{`r round(s2.x,3)`}=`r round(s.xy/s2.x,3)`\\[2ex]
\displaystyle b_0 = \overline{y}-b_1 \overline{x} =`r round(y.b,3)`-`r round(s.xy/s2.x,3)`\cdot `r round(x.b,3)`=`r round(y.b-(s.xy/s2.x)*x.b,3)`
\end{array}
$$
Trobam la recta de regressió
$$
\widehat{Y}=`r round(y.b-(s.xy/s2.x)*x.b,3)`+`r round(s.xy/s2.x,3)` X
$$
que coincideix amb la recta que calcula `lm`:
```{r}
lm(alçada~edat)
```

Els coeficients $b_0,b_1$ s'obtenen, respectivament, afegint els sufixos `$coefficients[1]` i `$coefficients[2]` al resultat de la funció `lm`.

```{r}
b0.edat=lm(alçada~edat)$coefficients[1]
b0.edat
b1.edat=lm(alçada~edat)$coefficients[2]
b1.edat
```


Segons aquesta estimació,  l'alçada mitjana dels nins augmenta  6.46 cm anuals, partint d'una alçada mitjana de  72.3 cm en néixer.

```{block2,type="rmdcaution"}
Els càlculs involucrats en la regressió lineal són molt poc robusts, en el sentit que els arrodoniments poden influir molt en el resultat final. A [l'entrada sobre regressió lineal de la Wikipedia](\url{http://en.wikipedia.org/wiki/Simple_linear_regression}) hi trobareu un exemple detallat d'una regressió de pes en funció d'alçada. Calculada en metres dóna:
$$
\widehat{Y}=61.272X-39.062
$$
Si es passen les alçades a polzades, s'arrodoneixen, es calcula la recta de regressió, i es torna a passar  el resultat a metres, dóna
$$
\widehat{Y}=61.675X-39.746
$$
La moralitat d'aquesta història és que, si feu els càlculs a mà, procurau no arrodonir fins al resultat final.
```


```{example, sal}
En un experiment on es volia estudiar l'associació entre el consum de sal i la tensió arterial, a alguns individus se'ls assignà aleatòriament una quantitat diària constant de sal en la seva dieta, i al cap d'un mes se'ls mesurà la tensió mitjana. Alguns resultats varen ser els següents:

```

```{r, echo=FALSE}
sal=c(1.8, 2.2,3.5,4.0,4.3,5.0)
tensió=c(100,98,110,110,112,120)
df_sal=data.frame(sal,tensió)
names(df_sal)=c("X (sal, en g)","Y (pressió, en mm de Hg)")
knitr::kable(df_sal)
```


Volem trobar la recta de regressió lineal per mínims quadrats de $Y$ en funció de $X$ a partir d'aquesta mostra.

Calculem els estadístics que necessitam:
```{r}
sal=c(1.8, 2.2,3.5,4.0,4.3,5.0)
tensió=c(100,98,110,110,112,120)
x.b=mean(sal)
y.b=mean(tensió)
s2.x=var(sal)
s2.y=var(tensió)
s.xy=cov(sal,tensió)
round(c(x.b,y.b,s2.x,s2.y,s.xy),3)
```

$$
\begin{array}{ccccc}
\overline{x} &  \overline{y} & \widetilde{s}_x^2 & \widetilde{s}_y^2 & \widetilde{s}_{xy}\\ \hline
`r round(x.b,3)` & `r round(y.b,3)` & `r round(s2.x,3)` & `r round(s2.y,3)` & `r round(s.xy,3)`
\end{array}
$$

Per tant els coeficients de la recta de regressió lineal per mínims quadrats de $Y$ (la tensió) en funció de $X$ (la quantitat de sal) són
```{r}
b1.sal=s.xy/s2.x
b0.sal=y.b-b1.sal*x.b
round(c(b0.sal,b1.sal),3)
```



Obtenim la recta
$$
\widehat{Y}= `r round(b0.sal,3)`+`r round(b1.sal,3)` X
$$
Segons aquest model, a un augment d'1 g de sal consumida li correspon un augment mitjà de `r round(b1.sal,1)` mm Hg de pressió arterial. 

Així mateix, amb aquest model estimam, per exemple, que la pressió arterial d'una persona que consumeix 3 g diaris de sal és
$$
`r round(b0.sal,3)`+`r round(b1.sal,3)` \cdot 3=`r round(b0.sal+3*b1.sal,3)`\text{ mm Hg}
$$

Comprovem que aquesta és la recta que obtenim amb la funció `lm`:

```{r}
lm(tensió~sal)$coefficients
```

```{example,Galton}
Estimem la recta de regressió de les alçades dels fills en funció de les dels pares emprant les dades recollides per Galton. Aquestes dades formen el dataframe `Galton` del paquet **HistData**.
```

```{r}
library(HistData)
str(Galton)
```

Cada filera del dataframe correspon a un adult: la variable `child` dóna la seva alçada i la variable `parent` la mitjana de les alçades dels seus pares, totes dues en polzades (recordau que 1 polzada són 2.54 cm). Calculem a mà i amb R la recta de regressió de la variable de resposta `child` en funció de la variable de control `parent`:

```{r}
x.b=mean(Galton$parent)
y.b=mean(Galton$child)
s2.x=var(Galton$parent)
s2.y=var(Galton$child)
s.xy=cov(Galton$parent,Galton$child)
round(c(x.b,y.b,s2.x,s2.y,s.xy),3)
```

$$
\begin{array}{ccccc}
\overline{x} &  \overline{y} & \widetilde{s}_x^2 & \widetilde{s}_y^2 & \widetilde{s}_{xy}\\ \hline
`r round(x.b,3)` & `r round(y.b,3)` & `r round(s2.x,3)` & `r round(s2.y,3)` & `r round(s.xy,3)`
\end{array}
$$

Per tant els coeficients de la recta de regressió lineal per mínims quadrats de $Y$ (l'alçada dels fills) en funció de $X$ (la mitjana de les alçades dels pares) són
```{r}
b1.Galton=s.xy/s2.x
b0.Galton=y.b-b1.Galton*x.b
round(c(b0.Galton,b1.Galton),3)
```


Obtenim la recta
$$
\widehat{Y}= `r round(b0.Galton,3)`+`r round(b1.Galton,3)` X
$$
Segons aquest model, a un augment d'1 polzada (2.54 cm) en l'alçada mitjana dels pares li correspon, de mitjana, un augment  de l'alçada del fill  de només `r round(b1.Galton,3)` polzades (`r round(b1.Galton*2.54,1)` cm).

Amb la funció `lm` obtenim la mateixa recta. Observau la sintaxi per especificar-hi el dataframe

```{r}
lm(child~parent, data=Galton)$coefficients
```

El fet que la pendent d'aquesta recta sigui més petita que 1 és el que dóna l'efecte de "regressió a la mediocritat" que observà Galton. En efecte, calculem per a quines alçades mitjanes dels pares esperam que els fills siguin més baixos que ells. Si resolem la desigualtat "alçada dels pares més gran que l'alçada esperada dels fills"
$$
X\geqslant \widehat{Y}=  `r round(b0.Galton,3)`+`r round(b1.Galton,3)` X
$$
obtenim
$$
X\geqslant \frac{`r round(b0.Galton,3)`}{1-`r round(b1.Galton,3)`}=`r round(b0.Galton/(1-b1.Galton),2)`
$$
i això ens diu que si l'alçada mitjana dels pares és més gran que `r round(b0.Galton/(1-b1.Galton),2)` polzades, uns `r round(b0.Galton/(1-b1.Galton)*2.54/100,2)` m,  esperam que els  fills siguin més baixos que els pares, mentre que, pel contrari, si l'alçada mitjana dels pares està per davall dels `r round(b0.Galton/(1-b1.Galton)*2.54/100,2)` m,  esperam que els  fills siguin més alts que els pares.


Algunes de les propietats importants de la regressió per mínims quadrats són:

* Tal i com hem calculat el terme independent $b_0$, la recta de regressió passa pel punt mitjà $(\overline{x},\overline{y})$ de la mostra:
$$
b_0+b_1 \overline{x}=\overline{y}
$$

* La mitjana dels valors estimats de la variable $Y$ als nostres punts és igual a la mitjana dels valors observats:
$$
\overline{\widehat{y}}=\frac{1}{n}\sum_{i=1}^n\widehat{y}_i
=\frac{1}{n}\sum_{i=1}^n(b_0+b_1x_i)= b_0+b_1 \overline{x}=\overline{y}
$$

* Els errors $(e_i)_{i=1,\ldots,n}$ de la mostra tenen mitjana 0:
$$
\begin{array}{l}
\overline{e}
 & \displaystyle =\frac{1}{n}\sum_{i=1}^n e_i
=\frac{1}{n}\sum_{i=1}^n (y_i-b_0-b_1x)
=\frac{1}{n}\sum_{i=1}^n (y_i-\widehat{y}_i)\\[2ex]
& \displaystyle
=\frac{1}{n}\sum_{i=1}^n{y}_i-\frac{1}{n}\sum_{i=1}^n\widehat{y}_i=
\overline{y}-\overline{\widehat{y}}
=0
\end{array}
$$

* Els errors $(e_i)_{i=1,\ldots,n}$ de la mostra tenen variància
$$
s_e^2=\frac{1}{n}\Big(\sum_{i=1}^{n}
e^2_i\Big)-\overline{e}^2=\frac{\sum_{i=1}^{n}
e^2_i}{n}=\frac{SS_E}{n}
$$
perquè $\overline{e}=0$ (i recordau que hem dit a $\sum_{i=1}^{n} e^2_i$ la **Suma de Quadrats dels Errors**,  
$SS_E$).

El teorema següent recull les propietats de la regressió lineal per mínims quadrats com a tècnica d'estimació dels coeficients $\beta_0$ i $\beta_1$:


```{theorem}
Si les variables aleatòries error $E_{x_i}$ tenen totes mitjana 0 i la mateixa variància $\sigma^2_E$ i són, dues a dues, incorrelades, aleshores:

* $b_0$ i $b_1$ són els estimadors lineals no esbiaixats més eficients (**òptims**) de $\beta_0$ i $\beta_1$

* Un estimador no esbiaixat de $\sigma_E^2$ és
$$
S^2=\frac{SS_E}{n-2}
$$

Si **a més** les variables aleatòries error $E_{x_i}$ són totes **normals**, aleshores:

* $b_0$ i $b_1$ són els estimadors màxim versemblants de $\beta_0$ i $\beta_1$ (a més de no esbiaixats òptims).
```

```{example, edats5}
Si suposam a l'Exemple \@ref(exm:edats1) que els errors tenen la mateixa variància i són incorrelats, podem estimar aquesta variància de la manera següent:

```

```{r}
n=length(edat)
alçada.cap=b0.edat+b1.edat*edat   #Els valors estimats
errors.edat=alçada-alçada.cap  #Els errors
SS.E=sum(errors.edat^2)  #La suma dels quadrats dels errors
S2.edat=SS.E/(n-2)  #L'estimació de la variància
S2.edat
```
Tenim que $S^2=`r round(SS.E/(n-2),3)`$, i estimam que $\sigma_E^2$ val això.

```{block2,type="rmdnote"}
No sabem si us hi heu fixat, però perquè la regressió lineal per mínims quadrats tengui bones propietats, **no cal que la variable $Y$**  (i molt menys la $X$, que no cal ni que sigui aleatòria) **sigui normal**. Qui han de ser normals (i amb mitjana 0, la mateixa variància i dues a dues incorrelades) han de ser les variables error.
```


Bé, fins ara hem explicat com s'estimen per mínims quadrats els coeficients $\beta_0$ i $\beta_1$ al model
$$
\mu_{Y|x}=\beta_0+\beta_1 x
$$
però ens pot interessar més:

* Com és de significativa l'estimació obtinguda?

* Quin és l'error típic d'aquests estimadors?

* Quins serien els intervals de confiança d'aquests coeficients per a un nivell de confiança donat?

* Com obtenim un interval de confiança per al valor estimat de $Y$ sobre un subjecte a partir del seu valor de $X$?

Amb la funció `lm`,  R calcula molt més que els coeficients de la recta:

```{r}
summary(lm(alçada~edat))
```

Veurem què és tot això que ens dóna R i per què serveix.

D'entrada, pot ser útil saber  que el vector `Residuals` (que s'obté amb el sufix `$residuals`) conté el vector dels errors $(e_i)_i$. Comprovem-ho amb les dades de l'Exemple \@ref(exm:edats1), els residus de les quals hem calculat a l'Exemple \@ref(exm:edats5):

```{r}
errors.edat
summary(lm(alçada~edat))$residuals
```

### Coeficient de determinació

Una primera pregunta que ens hem de fer és si la recta de regressió lineal que hem obtingut s'ajusta bé a la mostra obtinguda. Amb un enfocament proper al de l'ANOVA, 

> Consideram que la recta de regressió $\widehat{Y}=b_0+b_1X$ ens dóna una bona aproximació de $Y$ com a funció lineal de $X$ sobre la nostra mostra quan la variabilitat dels valors estimats $\widehat{y}_i$ representa una fracció molt gran de la variabilitat dels valors observats $y_i$.

Això es quantifica amb el **coeficient de determinació** $R^2$ que tot seguit definim.

Siguin:

* $SS_{Tot} =\sum\limits_{i=1}^n(y_i-\overline{y})^2$: és la **Suma Total de Quadrats** i representa la **variabilitat dels valors observats $y_i$**. Fixau-vos que
$$
SS_{Tot}=n\cdot s_y^2
$$

* $SS_R=\sum\limits_{i=1}^n(\widehat{y}_i-\overline{y})^2$: és la **Suma de Quadrats de la Regressió** i representa la **variabilitat dels valors estimats $\widehat{y}_i$**. Fixau-vos que
$$
SS_R=n\cdot s_{\widehat{y}}^2
$$


Considerarem que la recta $\widehat{y}=b_0+b_1x$ és una bona aproximació de $Y$ com a funció lineal de $X$ sobre la nostra mostra quan $s^2_{\widehat{y}}$ sigui molt proper a $s^2_y$. Per mesurar-ho, emprarem el **coeficient de determinació** $R^2$, que és simplement el seu quocient: 
$$
R^2=\frac{SS_R}{SS_{Tot}}=\frac{s_{\widehat{y}}^2}{s_y^2}
$$



Recordau ara que hem definit la **Suma de Quadrats dels Errors** $SS_E=\sum\limits_{i=1}^n(y_i-\widehat{y}_i)^2$ i que 
$$
SS_E=n\cdot s_e^2
$$
on $s_e^2$ és la variància dels errors. A la regressió lineal per mínims quadrats s'hi satisfà  la **identitat de les sumes de quadrats** següent:


```{theorem}
En una regressió lineal pel mètode de mínims quadrats, 
$$
SS_{Tot}=SS_R+SS_E
$$
o equivalentment (dividint per la mida $n$ de la mostra),
$$
s^2_y=s^2_{\widehat{y}}+s^2_e.
$$
```


```{example}
Comprovem aquesta igualtat amb les dades de l'Exemple \@ref(exm:edats1):


```

```{r}
SS.Tot=sum((alçada-mean(alçada))^2)
SS.R=sum((alçada.cap-mean(alçada))^2)
SS.E=sum(errors.edat^2)
c(SS.Tot,SS.R,SS.E)
SS.R+SS.E
```



Així, doncs, a la regressió per mínims quadrats 

> la variabilitat dels valors observats $y_i$ de $Y$ és igual a la suma de la variabilitat dels valors estimats $\widehat{y}_i$ de $Y$ més la variabilitat dels errors.


Aleshores, si la regressió lineal és per mínims quadrats,
$$
R^2=\frac{SS_R}{SS_{Tot}}=\frac{SS_{Tot}-SS_E}{SS_{Tot}}=1-\frac{SS_E}{SS_{Tot}}=1-\frac{s_e^2}{s_y^2}
$$
En particular:

```{block2,type="rmdimportant"}
En una regressió per mínims quadrats, $R^2\leqslant 1$, i $R^2= 1$ exactament quan tots els $e_i$ són 0, és a dir, quan $\widehat{y}_i=y_i$ per a tot $i=1,\ldots,n$. Com més gran (més proper a 1) sigui $R^2$, més bona entendrem que és la regressió lineal.
```

```{block2,type="rmdnote"}
Per si de cas no hi heu caigut, observau que $R^2\geqslant 0$, perquè és un quocient de quadrats. Només val 0 quan $s^2_{\widehat{y}}=0$, és a dir, quan tots els valors estimats $\widehat{y}_i$ són iguals (i això només passa quan $b_1=0$, és a dir, quan $s_{x,y}=0$).
```


R dóna el $R^2$ en el `summary(lm( ))`: és el valor `Multiple R-squared` a la penúltima línia de la seva sortida:

```{r}
summary(lm(alçada~edat))
```

S'obté directament del `summary(lm( ))` amb el sufix `$r.squared`

```{r}
summary(lm(alçada~edat))$r.squared
```

El resultat següent ja l'anunciàrem al Tema \@ref(chap:ED).

```{theorem}
En una regressió lineal per mínims quadrats, el coeficient de determinació és el quadrat de la correlació de Pearson de les mostres de les dues variables:
$$
R^2=r_{x,y}^2
$$
``` 

```{block2,type="rmdcorbes"}
En efecte:
$$
\begin{array}{rl}
R^2 & \displaystyle =\frac{SS_R}{SS_{Tot}}=\frac{\sum\limits_{i=1}^n(b_1x_i+b_0-\overline{y})^2}{ns_y^2}\\[2ex] 
& \displaystyle =\frac{\sum\limits_{i=1}^n\Big(\dfrac{s_{xy}}{s_x^2}x_i-\dfrac{s_{xy}}{s_x^2}\overline{x}\Big)^2}{ns_y^2} =\frac{\dfrac{s_{xy}^2}{s_x^4}\sum\limits_{i=1}^n(x_i-\overline{x})^2}{ns_y^2}\\[2ex] & \displaystyle =\dfrac{s_{xy}^2}{s_x^4}\cdot \frac{s_x^2}{s_y^2}=\frac{s_{xy}^2}{s_x^2\cdot s_y^2}=r_{xy}^2
\end{array}
$$

```


```{example}
Comprovem-ho a  l'Exemple \@ref(exm:edats1):

```

```{r}
summary(lm(alçada~edat))$r.squared
cor(edat,alçada)^2
```

```{example}
Comprovem ara la identitat de les sumes de quadrats i la igualtat $R^2=r^2$  a l'Exemple \@ref(exm:sal):


```

```{r}
tensió.cap=b0.sal+b1.sal*sal #Els valors estimats
SS.Tot=sum((tensió-mean(tensió))^2) #La Suma Total de Quadrats
SS.Tot
SS.R=sum((tensió.cap-mean(tensió))^2) #La Suma de Quadrats de la Regressió
SS.R
SS.E=sum((tensió-tensió.cap)^2) #La suma de Quadrats dels Errors
SS.E
```
Vegem que $SS_R+SS_E$ és igual a $SS_{Tot}$:

```{r}
SS.R+SS.E
```



Calculem ara $R^2=SS_R/SS_{Tot}$ i comprovem que coincideix amb el valor que dóna R i amb el quadrat de la correlació de Pearson de les mostres de quantitats de sal i tensions:
```{r}
R2=SS.R/SS.Tot
R2
summary(lm(tensió~sal))$r.squared
cor(sal,tensió)^2
```


```{block2,type="rmdnote"}
Fixau-vos que si coneixeu $\widetilde{s}_y^2$ (`var(y)`) i $r_{x,y}$ (`cor(x,y)`), llavors
$$
r_{x,y}^2=R^2=1-\frac{s_e^2}{s_y^2}\Longrightarrow  s_e^2=s_y^2(1-r_{x,y}^2)
$$
i per tant podeu calcular la $S^2$  que estima la variància comuna dels errors $E_{x_i}$ de la manera següent:
$$
S^2=\frac{SS_E}{n-2}=\frac{n s_e^2}{n-2}=\frac{ns_y^2(1-r_{x,y}^2)}{n-2}=\frac{(n-1)\widetilde{s}_y^2(1-r_{x,y}^2)}{n-2}
$$
Això us pot ser útil als exercicis.
```


```{block2,type="rmdcaution"}
El valor de $R^2$ no és suficient per valorar la bondat d'un model de regressió lineal. És sempre convenient també dibuixar els punts i la recta de regressió i donar una ullada.
```

Un exemple clàssic de les mancances del $R^2$ són els quatre conjunts de  dades $(x_{1,i},y_{1,i})_{i=1,\ldots,11}$, $(x_{2,i},y_{2,i})_{i=1,\ldots,11}$,  $(x_{3,i},y_{3,i})_{i=1,\ldots,11}$,  $(x_{4,i},y_{4,i})_{i=1,\ldots,11}$ que formen el *dataframe* `anscombe` de R i que ja empràrem al Tema \@ref(chap:ED):

```{r}
str(anscombe)
```

Les rectes de regressió per mínims quadrats dels quatre conjunts de dades són gairebé iguals i donen valors de $R^2$ molt semblants: 

```{r}
lm(y1~x1,data=anscombe)$coefficients
summary(lm(y1~x1,data=anscombe))$r.squared
lm(y2~x2,data=anscombe)$coefficients
summary(lm(y2~x2,data=anscombe))$r.squared
lm(y3~x3,data=anscombe)$coefficients
summary(lm(y3~x3,data=anscombe))$r.squared
lm(y4~x4,data=anscombe)$coefficients
summary(lm(y4~x4,data=anscombe))$r.squared
```

Però si els dibuixam veureu que els seus ajusts a la recta de regressió  són molt diferents:
```{r,fig.width=10,out.width="90%"}
par(mfrow=c(2,2))
plot(anscombe$x1,anscombe$y1,main="Conjunt de dades 1",pch=20)
abline(lm(y1~x1,data=anscombe),col="red",lwd=1.5)
plot(anscombe$x2,anscombe$y2,data=anscombe,main="Conjunt de dades 2",pch=20)
abline(lm(y2~x2,data=anscombe),col="red",lwd=1.5)
plot(anscombe$x3,anscombe$y3,main="Conjunt de dades 3",pch=20)
abline(lm(y3~x3,data=anscombe),col="red",lwd=1.5)
plot(anscombe$x4,anscombe$y4,main="Conjunt de dades 4",pch=20)
abline(lm(y4~x4,data=anscombe),col="red",lwd=1.5)
```


````{block2,type="rmdnote"}
Al Tema \@ref(chap:ED) ja us parlàrem del paquet **datasaurus**, les funcions del qual us permeten crear conjunts de punts de "formes" diferents i els mateixos estadístics. Vegem com el nostres dinosaure i estrella tenen rectes de regressió i valors de $R^2$ molt semblants. 
```


```{r}
datasaure=read.table("https://raw.githubusercontent.com/AprendeR-UIB/MatesII/master/Dades/Datasaurus.txt",header=TRUE,sep="\t")
dino=datasaure[datasaure$dataset=="dino",2:3]
star=datasaure[datasaure$dataset=="star",2:3]
lm(dino$y~dino$x)$coefficients
summary(lm(dino$y~dino$x))$r.squared
lm(star$y~star$x)$coefficients
summary(lm(star$y~star$x))$r.squared
```


```{r,fig.width=10,out.width="90%",fig.asp=0.5,echo=FALSE}
par(mfrow=c(1,2))
plot(dino,pch=20)
abline(lm(dino$y~dino$x),col="red",lwd=1.5)
plot(star,pch=20)
abline(lm(star$y~star$x),col="red",lwd=1.5)
par(mfrow=c(1,1))
```



### Intervals de confiança dels coeficients

Suposarem d'ara endavant que **cada $E_{x_i}$ segueix una distribució normal amb mitjana $\mu_{E_{x_i}}=0$ i totes amb la mateixa variància $\sigma_E^2$, i que $\sigma_{E_{x_i},E_{x_j}}=0$ per a cada parella $i,j$**. Recordau que sota aquestes condicions, els estimadors per mínims quadrats $b_0$ i $b_1$ de $\beta_0$ i $\beta_1$ són màxim versemblants i no esbiaixats òptims.

Si  tenim molt pocs valors $y$ per a cada $x$ a la mostra, això no es pot contrastar amb un mínim raonable de potència, però si és veritat, implica que els $(e_i)_{i=1,\ldots,n}$ s'ajusten a una variable $N(0,\sigma_E^2)$, amb $\sigma_E^2$ estimada per $S^2$, i això sí que ho podem contrastar. Si ho podem rebutjar, hem de rebutjar que els $E_{x_i}$ satisfan les condicions requerides.

```{example}
A l'Exemple \@ref(exm:edats):

```

```{r}
SS.E.edat=sum(errors.edat^2)
S2.edat=SS.E.edat/(length(edat)-2)  #L'estimació de la variància comuna dels errors
ks.test(errors.edat,"pnorm",0,sqrt(S2.edat))
```

Podem acceptar que els errors s'ajusten a una variable normal de mitjana 0.

```{example}
A l'Exemple \@ref(exm:sal):

```

```{r}
errors.sal=summary(lm(tensió~sal))$residuals
SS.E.sal=sum(errors.sal^2)
S2.sal=SS.E.sal/(length(sal)-2)
ks.test(errors.sal,"pnorm",0,sqrt(S2.sal))
```

També podem acceptar que els errors s'ajusten a una variable normal de mitjana 0.

Per cert, R calcula la $S$, l'arrel quadrada d'aquesta $S^2$, en fer la `lm`. És el `Residual standard error` de la tercera línia començant per avall a la sortida del `summary(lm( ))` i s'obté amb el sufix `$sigma`:

```{r}
summary(lm(tensió~sal))
summary(lm(tensió~sal))$sigma
sqrt(S2.sal)
```


Resulta que si se satisfan les condicions demanades sobre les variables $E_{x_i}$, aleshores coneixem els errors típics dels estimadors $b_1$ i $b_0$ i uns estadístics associats a aquests estimadors segueixen lleis t de Student que permeten calcular intervals de confiança per a $\beta_0$ i $\beta_1$. En concret:

* Pel que fa a $b_1$, 

    * El seu error típic és
$$
\frac{\sigma_E}{s_x\sqrt{n}}.
$$
    * L'estimació d'aquest error típic sobre una mostra concreta és
$$
\frac{S}{s_x\sqrt{n}}
$$
    * La fracció
$$
T_1=\frac{b_1-\beta_1}{\frac{S}{s_x\sqrt{n}}}
$$
         segueix una llei $t$ de Student amb $n-2$ graus de llibertat.

```{block2,type="rmdnote"}
Observau que l'error típic de $b_1$:

* Decreix amb $n$: com més gran és la mostra, menys incertesa esperam en l'estimació de $\beta_1$. Això ens ha passat en totes les estimacions del curs, no és cap sorpresa. En general, com més dades tenim, millor.
    
* Decreix amb $s_x$: com més dispersa és la mostra, menys incertesa esperam en l'estimació de $\beta_1$. Això és una novetat, en altres casos (per exemple, en estimar una mitjana) la incertesa creixia amb la desviació típica de la mostra. Però aquí és raonable. Pensau en termes físics: si voleu unir dos punts amb una recta, com és més fàcil que aquesta recta sigui estable, si els dos punts estan molt junts o si estan separats? Separats, no?
    
* Creix amb $\sigma_E$: com més variabilitat tenguin els errors residuals, més incertesa tendrem. Fixau-vos que si $\sigma_E=0$, aleshores no hi ha gens d'incertesa: significa que els punts $(x_i,y_i)$  estan sobre una recta i aquesta recta és la de regressió.

```



* Pel que fa a $b_0$, 

    * El seu error típic  és
$$
\frac{\sigma_E\sqrt{s_x^2+\overline{x}^2}}{s_x\sqrt{n}}
$$
    * L'estimació d'aquest error típic sobre una mostra concreta és
$$
\frac{S\sqrt{s_x^2+\overline{x}^2}}{s_x\sqrt{n}}
$$
    * La fracció
$$
T_0=\frac{b_0-\beta_0}{\frac{S\sqrt{s_x^2+\overline{x}^2}}{s_x\sqrt{n}}}
$$
         també segueix una llei $t$ de Student amb $n-2$ graus de llibertat.

```{block2,type="rmdnote"}
És raonable esperar que l'error típic de $b_0$ sigui més gran que el de $b_1$, perquè per calcular $b_0$ hem de calcular primer $b_1$ i a més emprar-hi les mitjanes $\overline{x}$ i  $\overline{y}$, la qual cosa fa que en estimar $\beta_0$ per mitjà de $b_0$ hi hagi més incertesa que en estimar $\beta_1$ per mitjà de $b_1$.
```

Com que els estadístics $T_1$ i $T_0$ tenen distribucions t de Student, operant amb ells com en el cas de l'interval de confiança per a la mitjana poblacional $\mu$ obtenim fórmules per als intervals de confiança per a $\beta_1$ i $\beta_1$  de la forma que ens agrada:
$$
\text{estimador}\pm \text{quantil}\times \text{error típic}
$$

En concret, sota les hipòtesis imposades al principi d'aquesta secció

* Un interval de confiança amb nivell de confiança $q$ per a $\beta_1$ és
$$
b_1\pm t_{n-2,(1+q)/2}\cdot \frac{S}{s_x\sqrt{n}}
$$

* Un interval de confiança amb nivell de confiança $q$ per a $\beta_0$ és
$$
b_0\pm t_{n-2,(1+q)/2}\cdot \frac{S\sqrt{s_x^2+\overline{x}^2}}{s_x\sqrt{n}}
$$


```{r,echo=FALSE}
n=length(edat)
x.b=mean(edat)
y.b=mean(alçada)
s2.x=var(edat)
s2.y=var(alçada)
s.xy=cov(edat,alçada)
b0.edat=lm(alçada~edat)$coefficients[1]
b1.edat=lm(alçada~edat)$coefficients[2]
S2.edat=SS.E/(n-1)
```

```{example,edatsIC}
Tornem a l'Exemple \@ref(exm:edats1). Hi  havíem obtingut la recta de regressió

```
$$
\widehat{Y}=`r round(b0.edat,3)`+`r round(b1.edat,3)`X
$$
i a més $n=7$ i havíem calculat que $\overline{x}=`r round(x.b,3)`$, $s_x^2=`r round(s2.x,3)`$ i  $S^2=`r round(S2.edat,3)`$.


Aleshores:

* Un interval de confiança al 95% per $\beta_1$ és
$$
\begin{array}{l}
\displaystyle b_1\pm t_{n-2,(1+0.95)/2}\cdot \frac{S}{s_x\sqrt{n}} =6.464\pm t_{5,0.975}\cdot \frac{\sqrt{8.314}}{4\sqrt{7}}\\[2ex]
\qquad =
6.464\pm 2.5706 \cdot 0.2724=6.464\pm  0.7
\end{array}
$$
    Obtenim l'interval $[5.764,7.164]$.

* Un interval de confiança al 95% per a $\beta_0$ és
$$
\begin{array}{l}
\displaystyle b_0\pm t_{n-2,(1+0.95)/2}\cdot\frac{S\sqrt{s_x^2+\overline{x}^2}}{s_x\sqrt{n}}
=72.321\pm t_{5,0.975}\cdot \frac{\sqrt{8.314}\cdot\sqrt{16+7^2}}{4\sqrt{7}}\\[2ex]
\qquad =
72.321\pm 2.5706 \cdot 2.1966=72.321\pm 5.647
\end{array}
$$
    Obtenim l'interval $[66.674,77.968]$.

Amb R aquests intervals de confiança s'obtenen amb la funció `confint` aplicada al resultat de la `lm`. El nivell de confiança s'hi indica amb el paràmetre `level` i el seu valor per defecte és, com sempre, 0.95.
```{r}
confint(lm(alçada~edat),level=0.95)
```

```{block2,type="rmdexercici"}
Calculau "a mà", amb les fórmules que hem donat, els intervals de confiança per als coeficients de la recta de regressió de l'Exemple \@ref(exm:sal). Us han de donar:
```

```{r}
confint(lm(tensió~sal))
```

### Intervals de confiança per a les estimacions de la variable dependent


També podem calcular intervals de confiança per al valor estimat de la $Y$ sobre els individus amb un valor de $X$ donat. En aquest cas, tenim dos intervals:

* L'interval per al **valor esperat** $\mu_{Y|x_0}$ de $Y$ sobre els individus en els que $X$ val $x_0$, és a dir, per al valor mitjà de la $Y$ sobre tots els individus de la població en els que $X$ valgui $x_0$.

* L'interval per al **valor predit** $y_0$ de $Y$ sobre un individu concret en el que $X$ valgui $x_0$.
 
Tot i que tant el valor esperat $\mu_{Y|x_0}$ com el valor $y_0$ de $Y$ sobre un individu concret en el que $X$ valgui $x_0$ tenen el mateix valor estimat, 
$$
\widehat{y}_0=b_0+b_1x_0,
$$
l'interval de confiança del valor esperat serà més estret que el del valor sobre un individu concret. Això reflecteix el fet que, naturalment, hi ha molta més incertesa en saber què val la $Y$ sobre un individu concret que en saber quin és el valor mitjà de $Y$ sobre tots els individus que tenguin el mateix valor de $X$ que aquest individu concret.

Bé passem a les fórmules. Sota les condicions sobre els errors que hem suposat al començament de la secció anterior (variables error normals de mitjana 0 i mateixa desviació típica, i incorrelades dues a dues):



* L'error típic de $\widehat{y}_0$ com a estimador de $\mu_{Y|x_0}$ és
$$
\sigma_E\cdot \sqrt{\frac{1}{n}+\frac{(x_0-\overline{x})^2}{ns^2_x}}
$$
    i la fracció
$$
\frac{\widehat{y}_0-\mu_{Y/x_0}}{S\cdot \sqrt{\frac{1}{n}+\frac{(x_0-\overline{x})^2}{n
s^2_x}}}
$$
    segueix una llei $t$ de Student amb $n-2$ graus de llibertat.

* L'error típic de $\widehat{y}_0$ com a estimador de $y_0$ és
$$
\sigma_E\cdot \sqrt{1+\frac{1}{n}+\frac{(x_0-\overline{x})^2}{ns^2_x}}
$$
    i la fracció
$$
\frac{\widehat{y}_0-y_0}{S\cdot \sqrt{1+\frac{1}{n}+\frac{(x_0-\overline{x})^2}{n
s^2_x}}}
$$
    segueix una llei $t$ de Student amb $n-2$ graus de llibertat.


```{block2,type="rmdimportant"}
Fixau-vos que l'error típic de l'estimació de $\mu_{Y|x_0}$ és més petit que el de l'estimació de $y_0$.
```



```{block2,type="rmdnote"}
Aquests errors típics creixen amb la distància entre $x_0$ i la mitjana de la mostra $\overline{x}$. És raonable: la recta passa per $(\overline{x},\overline{y})$ i a partir d'aquest punt, com més enfora estigui $x_0$, petites variacions en el valor de la pendent de la recta donaran lloc a diferències molt més grans en el valor de $b_0+b_1x_0$.
```


Per tant, sota aquestes hipòtesis,

* Un interval de confiança de nivell de confiança $q$ per a $\mu_{Y|x_0}$ és
$$
\widehat{y}_0\pm t_{n-2,(1+q)/2}\cdot S\cdot \sqrt{\frac{1}{n}+\frac{(x_0-\overline{x})^2}{n
s^2_x}}
$$

* Un interval de confiança  de nivell de confiança $q$ per a $y_0$ és
$$
\widehat{y}_0\pm t_{n-2,(1+q)/2}\cdot S\cdot \sqrt{1+\frac{1}{n}+\frac{(x_0-\overline{x})^2}{n
s^2_x}}
$$

```{example}
Tornem una altra vegada a l'Exemple \@ref(exm:edats1). Hi  havíem obtingut la recta de regressió

```
$$
\widehat{Y}=`r round(b0.edat,3)`+`r round(b1.edat,3)`X
$$
i a més $n=7$ i havíem calculat que $\overline{x}=`r round(x.b,3)`$, $s_x^2=`r round(s2.x,3)`$ i  $S^2=`r round(S2.edat,3)`$.


Suposem que volem estimar l'alçada $y_0$ d'un nin de $x_0=10$ anys. L'estimació amb la recta de regressió és 
$$
\widehat{y}_0=`r round(b0.edat,3)`+`r round(b1.edat,3)`\cdot 10=`r round(b0.edat+b1.edat*10,3)`
$$

Ara volem saber els intervals de confiança del 95% per a aquesta estimació:


* Un interval de confiança del 95% per a $y_0$ és
$$
\begin{array}{l}
\displaystyle
\widehat{y}_0\pm t_{n-2,(1+0.95)/2}\cdot S\sqrt{1+\frac{1}{n}+\frac{(x_0-\overline{x})^2}{ns^2_x}}
\\[2ex]
\displaystyle\qquad=136.961\pm t_{5,0.975}\cdot \sqrt{8.314}\cdot\sqrt{1+\frac{1}{7}+\frac{(10-7)^2}{7\cdot 16 }}\\[2ex]
\qquad =
136.961\pm 2.5706 \cdot  3.189=136.961\pm  8.198
\end{array}
$$
    Obtenim l'interval $[128.8,145.2]$. Per tant, estam molt segurs que si prenem un nin d'10 anys, la seva alçada estarà entre els 128.8 cm i els 145.2 cm.


* Un interval de confiança del 95% per al **valor esperat** $\mu_{Y|x_0}$ de $y_0$ és
$$
\begin{array}{l}
\displaystyle
\widehat{y}_0\pm t_{n-2,(1+0.95)/2}\cdot S\sqrt{\frac{1}{n}+\frac{(x_0-\overline{x})^2}{ns^2_x}}
\\[2ex]
\displaystyle\qquad=136.961\pm t_{5,0.975}\cdot \sqrt{8.314}\cdot\sqrt{\frac{1}{7}+\frac{(10-7)^2}{7\cdot 16 }}\\[2ex]
\qquad =
136.961\pm 2.5706 \cdot  1.362=136.961\pm  3.501
\end{array}
$$
Obtenim l'interval $[133.5, 140.5]$. Per tant, estam molt segurs que l'alçada mitjana dels nins  d'10 anys està entre els 133.5 cm i els 140.5 cm.

Si en canvi volguéssim emprar aquesta recta per estimar l'alçada d'un nin d'15 anys, els intervals que obtenim són (comprovau-ho):

* Per a $y_0$, $[`r round(predict.lm(lm(alçada~edat),data.frame(edat=15),interval="prediction")[2:3],1)`]$

* Per a $\mu_{Y|x_0}$, $[`r round(predict.lm(lm(alçada~edat),data.frame(edat=15),interval="confidence")[2:3],1)`]$

Com veieu, són molt més amples que els intervals de confiança per als 10 anys.


Amb R, aquests intervals es calculen amb la funció `predict.lm` aplicada a

* el resultat de la `lm`
* un *data frame* amb el valor (o els valors, si ho volem fer de cop per a més d'un valor) de $X$
* el paràmetre `interval` igualat al tipus d'interval que volem: 

    * `"prediction"` si és per al valor en un individu, 
    * `"confidence"` si és per al valor esperat

A més, s'hi pot entrar el nivell de significació amb el paràmetre `level`; si és 0.95, no cal. El resultat és un dataframe amb tres columnes: `fit`, el valor predit, i `lwr` i `upr`, els extrems inferior i superior de l'interval.

En el nostre exemple, primer hem de definir un data frame amb l'edat o les edats. Calcularem els intervals de confiança per als 10 i 15 anys. Per tant, definim un *data frame* format per dues observacions de la variable `edat`, que valguin 10 i 15:

```{r}
nin=data.frame(edat=c(10,15))
```

Aleshores, els intervals de confiança del 95% per a les alçades d'un nin d'10 anys i d'un nin d'15 anys són, respectivament,
```{r}
predict.lm(lm(alçada~edat),nin,interval="prediction")
```

i els intervals de confiança del 95% per a les alçades mitjanes dels nins d'10 i d'15 anys són, respectivament,

```{r}
predict.lm(lm(alçada~edat),nin,interval="confidence")
```

### Té sentit una regressió lineal?

Si $\beta_1=0$,  el model de regressió lineal no té sentit, perquè en aquest cas
$$
Y|x=\beta_0+E_x
$$
i les variacions en els valors de $Y$ es deuen tan sols als errors aleatoris.


El contrast
$$
\left\{\begin{array}{l}
H_0:\beta_1=0\\
H_1:\beta_1 \neq 0
\end{array}
\right.
$$
el podem realitzar amb l'interval de confiança per a $\beta_1$: si 0 no hi pertany, 
rebutjam la hipòtesi nul·la amb el nivell de significació corresponent al nivell de confiança de l'interval.

Per exemple, a l'Exemple \@ref(exm:edatsIC) hem obtingut l'IC 95% per a $\beta_1$   $[5.764,7.164]$. Com que no conté el 0, concloem (amb un nivell de significació de 0.05) que $\beta_1\neq 0$.

Si mirau la sortida del `summary(lm( ))`
```{r}
summary(lm(alçada~edat))
```

a la matriu `Coefficients`:

* Els `Estimate` són les estimacions $b_0$ i $b_1$
* Els `Std. Error` són (les estimacions de) els seus errors típics
* Els `t value` són els valors dels estadístics de contrast dels contrastos bilaterals amb hipòtesi nul·la $H_0:$ "coeficient $=0$"; aquests estadístics de contrast són justament els estadístics $T_0$ i $T_1$ que hem definit fa una estona (substituint-hi $\beta_0$ i $\beta_1$ pels valors que contrastam, 0)
* Els `Pr(>|t|)` són els  p-valors d'aquests  contrastos (que no us engani la notació, aquests p-valors es defineixen com toca: 
$$
2P(t_{n-2}\geqslant |T_0|),\quad 2P(t_{n-2}\geqslant |T_1|)
$$
respectivament).

    Com veiem, en aquest cas podem rebutjar amb $\alpha=0.05$ que $\beta_1=0$ (i també que $\beta_0=0$).


## Regressió lineal múltiple

Comencem amb un exemple.

```{example, mult}
Es postula que l'alçada esperada d'un nadó en cm ($Y$) té una relació  lineal amb la seva edat en dies ($X_1$), la seva alçada en néixer en cm ($X_2$), el seu pes en kg en néixer ($X_3$) i l'augment en tant per cent del seu pes actual respecte del seu pes en néixer ($X_4$). És a dir, es creu que existeixen coeficients $\beta_0,\ldots,\beta_4\in \mathbb{R}$ tals que el model
$$
\mu_{Y|x_1,x_2,x_3,x_4}=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3+\beta_4x_4
$$
és correcte, on $\mu_{Y|x_1,x_2,x_3,x_4}$ és l'alçada esperada, en cm, d'un nadó de $x_1$ dies que en néixer va fer $x_2$ cm i $x_3$ kg  i des de llavors el seu pes ha augmentat un $x_4$%. En una mostra de $n=9$ nins, els resultats varen ser els de la taula següent:


```

```{r, echo=FALSE}
y=c(57.5,52.8,61.3,67,53.5,62.7,56.2,68.5,79.2)
x1=c(78,69,77,88,67,80,74,94.0,102)
x2=c(8.2,45.5,46.3,49,43,48,48,53,58)
x3=c(2.75,2.15,4.41,5.52,3.21,4.32,2.31,4.3,3.71)
x4=c(29.5,26.3,32.2,36.5,27.2,27.7,28.3,30.3,28.7)
df.edat.mult=data.frame(y,x1,x2,x3,x4)
names(df.edat.mult)=c("Alçada (en cm)","Edat (en dies)", "Alçada en néixer (en cm)","Pes en néixer (en kg)","% d'increment de pes")
knitr::kable(df.edat.mult)
```



A partir d'aquesta mostra, volem estimar els coeficients $\beta_0,\ldots,\beta_4\in \mathbb{R}$ de la relació lineal predita.

Aquest és un problema de **regressió lineal múltiple**. Ara tenim $k$ variables  **independents**, o **de control** $X_1,\ldots, X_k$ (com al cas simple, no necessàriament aleatòries) i una
variable aleatòria **dependent**, o **de resposta**, $Y$. Suposam que el model
$$
\mu_{Y|x_1,\ldots,x_k}= \beta_0+\beta_1 x_1+\cdots+\beta_k x_k
$$
o, equivalentment,
$$
Y|x_1,\ldots,x_k=\beta_0+\beta_1 x_{1}+\cdots+\beta_{k} x_k+E_{x_1,\ldots,x_k}
$$
és correcte, on:

* $Y|x_1,\ldots,x_k$ és la variable aleatòria que dóna el valor de $Y$ sobre  individus en els quals $X_i=x_i$ per a cada $i=1,\ldots,k$ 

* $\mu_{Y|x_1,\ldots,x_k}$ és el valor esperat de $Y|x_1,\ldots,x_k$, és a dir, la mitjana dels valors de $Y$ sobre tots els individus de la població en els quals $X_i=x_i$ per a cada $i=1,\ldots,k$

* Les $E_{x_1,\ldots,x_k}$ són les variables aleatòries **error**, o **residu**, i representen l'error aleatori de la variable $Y$ sobre un individu en el qual $(X_1,\ldots,X_k)=(x_1,\ldots,x_k)$

* $\beta_0,\beta_1,\ldots,\beta_{k}\in \mathbb{R}$:

    * $\beta_0$ és el valor esperat de $Y$ quan $X_1=\cdots=X_k=0$

    * Cada $\beta_i$ és la variació del valor esperat de $Y$ quan $X_i$ augmenta una unitat i les altres variables $X_j$ no varien


Els paràmetres $\beta_0,\beta_1,\ldots,\beta_{k}$ són desconeguts, i els volem estimar a partir d'una mostra
$$
(x_{1i},x_{2i},\ldots,x_{ki},y_i)_{i=1,\ldots,n}
$$
d'observacions del vector aleatori $(X_1,\ldots,X_k,Y)$ sobre $n$ individus. Requerirem que
$n>k$ (el nombre d'observacions ha de ser més gran que el nombre de variables) a fi que el sistema d'equacions lineals amb incògnites els coeficients $\beta_0,\beta_1,\ldots,\beta_{k}$
$$
\left\{
\begin{array}{l}
y_1=\beta_0+\beta_1x_{11}+\cdots +\beta_kx_{k1}\\
\quad\vdots\\
y_n=\beta_0+\beta_1x_{1n}+\cdots +\beta_kx_{kn}
\end{array}
\right.
$$
no sigui indeterminat. Direm $b_0,b_1,\ldots,b_k$ a les estimacions dels paràmetres $\beta_0,\beta_1,\ldots,\beta_k$ a partir d'una mostra, i per escurçar escriurem $\underline{x}_i$ per indicar $(x_{1i},x_{2i},\ldots,x_{ki})$.

Per a cada $i=1,\ldots,n$, diguem
$$
\begin{array}{l}
\widehat{y}_i= b_0+b_1 x_{1i}+\cdots+b_{k} x_{ki}\\
e_i=y_i-\widehat{y}_i=y_i-(b_0+b_1 x_{1i}+\cdots+b_{k} x_{ki})
\end{array}
$$
Amb aquestes notacions:

* $\widehat{y}_i$ és el **valor predit** de $Y$ sobre l'individu $i$-èsim de la mostra a partir del seu vector de valors $\underline{x}_{i}$ i de les estimacions $b_0,b_1,\ldots,b_k$ dels paràmetres


* $e_i$ és l'**error** que es comet amb aquesta estimació sobre aquest individu 


Direm la **Suma de Quadrats dels Errors** a:
$$
\begin{array}{rl}
SS_E= &\displaystyle\sum\limits_{i=1}^n
e^2_i=\sum\limits_{i=1}^n (y_i-\widehat{y}_i)^2 \\
= &\displaystyle\sum\limits_{i=1}^n (y_i-b_0-b_1 x_{1i}-\cdots -b_{k} x_{ki})^2.
\end{array}
$$


### Mínims quadrats

Els estimadors de $\beta_0,\beta_1,\ldots, \beta_k$ pel **mètode de mínims quadrats** són els
valors $b_0,b_1,\ldots, b_k$ que minimitzen $SS_E$ sobre la nostra mostra. Per calcular-los, calculam les derivades parcials de $SS_E$ respecte de cada $b_i$, les igualam a 0, resolem el sistema resultant, comprovam que la solució $(b_0,\ldots,b_k)$ trobada dóna un mínim... Tot plegat, al final s'obté el resultat següent:


```{theorem}
Siguin
$$
\mathbf{y}=
\left(
\begin{array}{l}
y_1\\ y_2\\ \vdots\\ y_n
\end{array}
\right),\ \mathbf{X}=\left(
\begin{array}{lllll}
1&x_{11}&x_{21}&\ldots&x_{k1}\\
1&x_{12}&x_{22}&\ldots&x_{k2}\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
1&x_{1n}&x_{2n}&\ldots&x_{kn}
\end{array}
\right)
$$

Aleshores, els estimadors per mínims quadrats $\mathbf{b}=(b_0,b_1,\ldots,b_k)^t$ de $\beta_0,\beta_1,\ldots,\beta_k$ a partir de la mostra  $(\underline{x}_{i},y_i)_{i=1,2,\ldots,n}$ són donats per l'equació següent:
$$
\mathbf{b}=\left(\mathbf{X}^t\cdot \mathbf{X}\right)^{-1}\cdot \left(\mathbf{X}^t \cdot \mathbf{y}\right).
$$

```

```{block2,type="rmdcorbes"}
Amb una mica de paciència podeu comprovar que si $k=1$, aquesta fórmula dóna la de $(b_0,b_1)^t$ a la regressió lineal simple.
```
    

Com al cas simple, la funció resultant l'escriurem
$$
\widehat{Y}=b_0+b_1X_1+\cdots +b_kX_k
$$
i en direm la **funció de regressió lineal per mínims quadrats** de $Y$ en funció de $X_1,\ldots,X_k$.

```{example}
Tornem a l'Exemple \@ref(exm:mult). Recordau les dades:


```

```{r, echo=FALSE}
y=c(57.5,52.8,61.3,67,53.5,62.7,56.2,68.5,79.2)
x1=c(78,69,77,88,67,80,74,94.0,102)
x2=c(8.2,45.5,46.3,49,43,48,48,53,58)
x3=c(2.75,2.15,4.41,5.52,3.21,4.32,2.31,4.3,3.71)
x4=c(29.5,26.3,32.2,36.5,27.2,27.7,28.3,30.3,28.7)
df.edat.mult=data.frame(y,x1,x2,x3,x4)
names(df.edat.mult)=c("Alçada (en cm)","Edat (en dies)", "Alçada en néixer (en cm)","Pes en néixer (en kg)","% d'increment de pes")
knitr::kable(df.edat.mult)
```

Calculem la funció lineal de regressió per mínims quadrats de l'alçada en funció de les altres variables. Pel teorema anterior, si diem 
$$
\mathbf{X}=\left(
\begin{array}{ccccc}
1&78&48.2&2.75&29.5\\
1&69&45.5&2.15&26.3\\
1&77&46.3&4.41&32.2\\
1&88&49&5.52&36.5\\
1&67&43&3.21&27.2\\
1&80&48&4.32&27.7\\
1&74&48&2.31&28.3\\
1&94&53&4.3&30.3\\
1&102&58&3.71&28.7
\end{array}
\right),\
\mathbf{y}=\left(
\begin{array}{c}
57.5\\ 52.8\\ 61.3\\ 67\\ 53.5\\ 62.7\\ 56.2\\ 68.5\\ 79.2
\end{array}
\right)
$$

aleshores $(b_0,b_1,b_2,b_3,b_4)$ s'obté mitjançant 
$$
\left(\begin{array}{c} b_0 \\ \vdots \\ b_4\end{array}\right)=\left(\mathbf{X}^t\cdot \mathbf{X} \right)^{-1}\cdot \left(\mathbf{X}^t \cdot \mathbf{y}\right)
$$

Per calcular aquest vector, primer entram les dades i definim aquestes matrius

```{r}
y=c(57.5,52.8,61.3,67,53.5,62.7,56.2,68.5,79.2)
x1=c(78,69,77,88,67,80,74,94.0,102)
x2=c(8.2,45.5,46.3,49,43,48,48,53,58)
x3=c(2.75,2.15,4.41,5.52,3.21,4.32,2.31,4.3,3.71)
x4=c(29.5,26.3,32.2,36.5,27.2,27.7,28.3,30.3,28.7)
X=cbind(1,x1,x2,x3,x4)
X
```

Ara ja podem estimar els coeficients de la funció de regressió lineal:

```{r}
B=solve(t(X)%*%X)%*%(t(X)%*%y)
round(B,4)
```

```{r,echo=FALSE}
b0.em=B[1,1]
b1.em=B[2,1]
b2.em=B[3,1]
b3.em=B[4,1]
b4.em=B[5,1]
```

Obtenim
$$
\begin{array}{ccccc}
b_0 & b_1 & b_2 & b_3& b_4\\ \hline
`r round(b0.em,4)` & `r round(b1.em,4)` & `r round(b2.em,4)` & `r round(b3.em,4)` & `r round(b4.em,4)`
\end{array}
$$
i per tant la funció de regressió lineal per mínims quadrats
$$
\widehat{Y}=`r round(b0.em,4)`+`r round(b1.em,4)`X_1+`r round(b2.em,4)`X_2+`r round(b3.em,4)`X_3`r round(b4.em,4)`X_4
$$

Fixau-vos que les igualtats
$$
\widehat{y}_i=b_0+b_1x_{1i}+b_2x_{2i}+\cdots +b_nx_{ni}
$$
es tradueixen en la igualtat matricial
$$
\left(
\begin{array}{l}
\widehat{y}_1\\ \widehat{y}_2\\ \vdots\\ \widehat{y}_n
\end{array}
\right)=\left(
\begin{array}{lllll}
1&x_{11}&x_{21}&\ldots&x_{k1}\\
1&x_{12}&x_{22}&\ldots&x_{k2}\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
1&x_{1n}&x_{2n}&\ldots&x_{kn}
\end{array}
\right)\cdot
\left(
\begin{array}{l}
b_0 \\ b_1\\ b_2\\ \vdots\\ b_k
\end{array}
\right)
$$

En el nostre exemple, els valors estimats de $Y$ sobre els nins de la nostra mostra serien
```{r}
Y.cap=X%*%B
t(round(Y.cap,1))
```

i per tant els errors $e_i=y_i-\widehat{y}_i$ són
```{r}
e.i=y-Y.cap
t(e.i)
```

Amb R, la regressió lineal múltiple per mínims quadrats també es fa amb la funció `lm`, aplicada a la fórmula que agrupa la variable resposta en funció de **la suma** de les variables de control. Al nostre exemple \@ref(exm:mult) seria

```{r}
lm(y~x1+x2+x3+x4)
```

Obtenim la mateixa funció lineal de regressió que abans:
$$
\widehat{Y}=`r round(b0.em,4)`+`r round(b1.em,4)`X_1+`r round(b2.em,4)`X_2+`r round(b3.em,4)`X_3`r round(b4.em,4)`X_4
$$

A més, com al cas simple, aquesta funció també calcula els errors $e_i$:

```{r}
summary(lm(y~x1+x2+x3+x4))$residuals
```


La regressió lineal múltiple per mínims quadrats satisfà les mateixes propietats que la simple:


* La recta de regressió passa pel vector mitjà $(\overline{x}_1,\overline{x}_2,\ldots,\overline{x}_k,\overline{y})$:
$$
\overline{y}=b_0+b_1 \overline{x}_1+\cdots+b_k \overline{x}_k
$$

* La mitjana dels valors estimats és igual a la mitjana dels observats:
$$
\overline{\widehat{y}}=\overline{y}
$$

* Els errors $(e_i)_{i=1,\ldots,n}$ tenen mitjana 0 i variància
$$
s_e^2=\frac{SS_E}{n}
$$

* Si les variables aleatòries error $E_{\underline{x}_i}$ tenen totes mitjana 0 i la mateixa variància $\sigma^2_E$ i són, dues a dues, incorrelades, aleshores els $b_i$ són els estimadors lineals no esbiaixats òptims dels $\beta_i$ i
$$
S^2=\frac{SS_E}{n-k-1}
$$
    és un estimador no esbiaixat de $\sigma_E^2$
    
* Si **a més** les variables aleatòries error $E_{\underline{x}_i}$ són totes normals, aleshores
els $b_i$ són els estimadors màxim versemblants dels $\beta_i$


* Se satisfà la mateixa **identitat de les sumes de quadrats**
$$
SS_{Tot}=SS_R+SS_E
$$
     o, equivalentment,
$$
s^2_y=s^2_{\widehat{y}}+s^2_e
$$
    on:

     * $SS_{Tot}=\sum_{i=1}^n (y_i-\overline{y})^2$ és la **Suma de Quadrats Total**, que mesura la variabilitat dels valors observats $y_i$ de la $Y$  i satisfà que $SS_{Tot}=n\cdot s_y^2$, on $s_y^2$ és la variància de les  $y_i$.

    * $SS_R=\sum_{i=1}^n(\widehat{y}_i-\overline{y})^2$ és la **Suma de Quadrats de la Regressió**, que mesura la variabilitat de les estimacions $\widehat{y}_i$ de la $Y$ sobre la nostra mostra i satisfà que  $SS_R=n\cdot s_{\widehat{y}}^2$, on $s_{\widehat{y}}^2$ és la variància de les  $\widehat{y}_i$.

   * $SS_E=\sum_{i=1}^n (y_i-\widehat{y}_i)^2$ és la de **Suma de Quadrats dels Errors** que ja hem definit i satisfà que  $SS_E=n\cdot s_{e}^2$, on $s_{e}^2$ és la variància dels errors  $e_i$.

Com al cas simple, quan R calcula una funció de regressió lineal per mínims quadrats també calcula un munt de coses més:

```{r}
summary(lm(y~x1+x2+x3+x4))
```

D'aquesta caterva d'informació, ja sabem algunes coses que són: els coeficients de la funció (la columna `Estimate` de la matriu `Coefficients`), els residus (`Residuals`), l'arrel quadrada de la $S^2$ (`Residual standard error`). I algunes tenen el mateix significat que a la regressió lineal simple: la resta d'entrades de la matriu `Coefficients` o el `Multiple R-squared`. Els tres darrers valors que dóna R (`Adjusted R-squared`, `F-statistic` i `p-value`) tendran només interès a la regressió múltiple, com veurem.



### Coeficient de determinació múltiple

Com al cas simple, consideram que la funció lineal de regressió 
$$
\widehat{Y}=b_0+b_1X_1+\cdots+b_kX_k
$$
és una bona aproximació de $Y$ com a funció lineal de $X_1,\ldots,X_k$ sobre la nostra mostra quan la variabilitat dels valors estimats $\widehat{y}_i$ representa una fracció molt gran de la variabilitat dels valors observats $y_i$. Això es quantifica amb el **coeficient de determinació** (**múltiple**) $R^2$, que es defineix exactament igual que al cas simple i es calcula igual:
$$
R^2=\frac{SS_R}{SS_{Tot}}=\frac{s^2_{\widehat{y}}}{s^2_y}
$$

```{example}
Al nostre Exemple \@ref(exm:mult), el coeficient de determinació és

````

```{r}
summary(lm(y~x1+x2+x3+x4))$r.squared
```

```{block2,type="rmdnote"}
Suposam que encara recordau que, en el cas simple, el coeficient de determinació era el quadrat del coeficient de correlació de Pearson. En el cas múltiple, el que es fa és **definir** el 
**coeficient de correlació múltiple** d'un vector  $y$ respecte d'uns vectors $x_1,\ldots, x_k$ (tots ells mesures de diferents variables aleatòries sobre els mateixos individus) com
$$
R= \sqrt{R^2}
$$
i així també se té que **el coeficient de determinació múltiple és el quadrat del coeficient de correlació múltiple**.
```



### Coeficient de determinació ajustat


$R^2$ tendeix a créixer si afegim variables independents al model, fins i tot quan les variables que afegim són irrellevants. Vegem-ne un exemple.

```{example,multfake}
Imaginau que a la taula de dades de l'Exemple \@ref(exm:mult) li afegim una nova variable $X_5$ que mesura la distància (en km) a vol d'ocell de la llibreria on la mare sol comprar els llibres a la consulta del pediatra que ha mesurat l'alçada $Y$. Ens inventarem els valors d'aquesta nova variable, generant-los amb distribució normal $N(2000,1000)$

```

```{r,echo=FALSE}
set.seed(200)
```
```{r}
x5=round(rnorm(9,2000,1000))
x5
```

Per tant, les dades ara són
```{r, echo=FALSE}
y=c(57.5,52.8,61.3,67,53.5,62.7,56.2,68.5,79.2)
x1=c(78,69,77,88,67,80,74,94.0,102)
x2=c(8.2,45.5,46.3,49,43,48,48,53,58)
x3=c(2.75,2.15,4.41,5.52,3.21,4.32,2.31,4.3,3.71)
x4=c(29.5,26.3,32.2,36.5,27.2,27.7,28.3,30.3,28.7)
x5
df.edat.mult=data.frame(y,x1,x2,x3,x4,x5)
names(df.edat.mult)=c("Alçada (en cm)","Edat (en dies)", "Alçada en néixer (en cm)","Pes en néixer (en kg)","% d'increment de pes", "Distància llibreria-pediatra (en m)")
knitr::kable(df.edat.mult)
```

Ara calculem el $R^2$ de la regressió de $Y$ en funció de $X_1,\ldots,X_5$ i comparem-lo amb l'obtingut amb  $X_1,\ldots,X_4$:
```{r}
summary(lm(y~x1+x2+x3+x4+x5))$r.squared
summary(lm(y~x1+x2+x3+x4))$r.squared
```

Com veieu, la regressió tenint en compte la distància de ca'l llibreter a ca'l pediatra té coeficient de determinació més gran que sense tenir-lo en compte. Però imaginam que teniu clar que aquesta variable és irrellevant a l'hora d'explicar l'alçada d'un nin.

Per tenir en compte aquest fet i compensar el nombre de variables emprat en la regressió,  en lloc d'emprar el coeficient de determinació
$$
R^2=\frac{SS_R}{SS_{Tot}}=\frac{SS_{Tot}-SS_E}{SS_{Tot}}
$$
s'empra el **coeficient de determinació ajustat**
$$
R^2_{adj}=\frac{MS_{Total}-MS_E}{MS_{Total}}
$$
on 
$$
MS_{Total}=\frac{SS_{Tot}}{n-1}\text{ i } MS_E=\frac{SS_E}{n-k-1}.
$$

```{block2,type="rmdcaution"}
Fixau-vos que $MS_{Total}$ no és res més que $\widetilde{s}_y^2$ i que fa una estona a $MS_E$ li hem dit $S^2$: l'estimador no esbiaixat de la variància comuna de les variables error $E_{\underline{x}_i}$ quan totes aquestes variables tenen la mateixa variància.
```


Operant, queda
$$
R^2_{adj}=\frac{(n-1)R^2-k}{n-k-1}
$$

A la sortida del `summary(lm( ))` és el `Adjusted R-squared` de la penúltima línia:

```{r}
summary(lm(y~x1+x2+x3+x4))
```



Es considera que una regressió lineal múltiple per mínims quadrats és "millor" que una altra quan té el coeficient de determinació ajustat més gran. Això només té interès per a regressions amb diferents nombres de variables independents i la mateixa mostra d'individus, perquè fixats $n$ i $k$, la funció
$$
R^2\mapsto R^2_{adj}=\frac{(n-1)R^2-k}{n-k-1}
$$
és creixent, i per tant si fixam els valors de $n$ i de $k$, comparar $R^2$  és equivalent a comparar $R^2_{adj}$. 


Amb R es calcula amb el sufix `$adj.r.squared`. Calculem els del nostre exemple, amb i sense la variable "falsa",  a veure què passa:

```{r}
summary(lm(y~x1+x2+x3+x4))$adj.r.squared
summary(lm(y~x1+x2+x3+x4+x5))$adj.r.squared
```

Com veieu, sense tenir en compte la distància de la llibreria de capçalera de la mare a la consulta del pediatra obtenim un valor més gran de $R^2_{adj}$ i per tant consideram que és una regressió millor que tenint-la en compte. Ja que hi som, comprovem l'equació
$$
R^2_{adj}=\frac{(n-1)R^2-k}{n-k-1}
$$
per al model amb 4 variables independents:

```{r}
n=9
k=4
R2=summary(lm(y~x1+x2+x3+x4))$r.squared
((n-1)*R2-k)/(n-k-1)
```


### Intervals de confiança per als coeficients

Suposarem en el que queda de tema que les variables aleatòries error $E_i=E_{\underline{x}_{i}}$ són totes normals de mitjana 0 i la mateixa variància, $\sigma_E^2$, i dues a dues incorrelades. Recordem que, sota aquestes hipòtesis: 

* Els estimadors $b_0,\ldots, b_k$ de $\beta_0,\ldots,\beta_k$ són màxim versemblants i a més no esbiaixats òptims.

* Un estimador no esbiaixat de $\sigma_E^2$ és
$$
S^2(=MS_E)=\frac{SS_E}{n-k-1}
$$

A l'Exemple \@ref(exm:mult), aquesta estimació de la variància comuna dels errors $\sigma_E^2$ és
```{r}
S2=sum(e.i^2)/(n-k-1)
S2
```

Com al cas simple, la sortida de `summary(lm( ))` dóna el valor de la $S$ (és a dir, l'arrel quadrada de $S^2$, que  per tant estima la desviació típica comuna de les variables error) com a `Residual standard error` 
i s'obté amb el sufix `$sigma`:

```{r}
summary(lm(y~x1+x2+x3+x4))
summary(lm(y~x1+x2+x3+x4))$sigma
(summary(lm(y~x1+x2+x3+x4))$sigma)^2
```

Resulta que sota les condicions imposades al principi d'aquesta secció sobre les variables $E_i$, podem calcular intervals de confiança per als coeficients $\beta_i$ de la funció de regressió lineal.

```{block2,type="rmdmercifulgod"}
Donam les fórmules per pura completesa, no esperam que calculeu aquests intervals a mà mai.
```

* L'error típic de cada estimador $b_i$ és l'arrel quadrada de la $i$-èsima entrada de la diagonal de la matriu $\sigma_E^2\cdot (\mathbf{X}^t \mathbf{X})^{-1}$, començant a comptar amb  $i=0$:
$$
\sqrt{(\sigma_E^2\cdot (X^t X)^{-1})_{ii}}
$$
     L'estimam sobre una mostra substituint-hi $\sigma_E^2$ per $S^2$. R ens dóna aquestes estimacions a la columna `Std. Error` de la matriu `Coefficients` a la sortida de `summary(lm( ))`:
```{r}
summary(lm(y~x1+x2+x3+x4))
summary(lm(y~x1+x2+x3+x4))$coefficients[,2]
```


* Cada fracció 
$$
T_i=\frac{b_i-\beta_i}{\sqrt{(S^2\cdot (X^t X)^{-1})_{ii}}}
$$
segueix un llei t de Student amb $n-k-1$ graus de llibertat


* Un interval de confiança de nivell de confiança $q$ per a $\beta_i$ és
$$
b_i\pm t_{n-k-1,(1+q)/2}\cdot \sqrt{(S^2\cdot (X^t X)^{-1})_{ii}}
$$


Amb R, aquests intervals de confiança s'obtenen com al cas simple, aplicant la  funció `confint` al resultat de la `lm`:

```{r}
confint(lm(y~x1+x2+x3+x4))
```

Els podem calcular "a mà" a partir de les estimacions dels errors típics que dóna `summary(lm( ))` amb la fórmula genèrica "estimació $\pm$ quantil per error típic". Per exemple, l'IC 95% per a $\beta_1$ seria
```{r}
n=9
k=4
b1=lm(y~x1+x2+x3+x4)$coefficients[2]
Error.Tip.b1=summary(lm(y~x1+x2+x3+x4))$coefficients[2,2]
IC=b1+qt(0.975,n-k-1)*Error.Tip.b1*c(-1,1)
IC
```


### Intervals de confiança per a les estimacions de la variable resposta

Si les variables error $E_i$ satisfan les condicions imposades al principi de la secció anterior, també podem calcular intervals de confiança per al valor estimat de la $Y$ sobre els individus amb valors de $(X_1,\ldots,X_k)$ donats. Com al cas simple, tenim dos intervals:

* L'interval per al **valor esperat** $\mu_{Y|x_{10},\ldots,x_{k0}}$ de $Y$ sobre els individus en els que $(X_1,\ldots,X_k)$ val $(x_{10},\ldots,x_{k0})$, és a dir, per al valor mitjà de la $Y$ sobre tots els individus de la població en els que $(X_1,\ldots,X_k)$ valgui $(x_{10},\ldots,x_{k0})$

* L'interval per al **valor predit** $y_0$ de $Y$ sobre un individu concret en el que $(X_1,\ldots,X_k)$ valgui $(x_{10},\ldots,x_{k0})$.

La discussió sobre les diferències entre una i altra estimació i per què el segon interval és més ample que el primer són les mateixes que al cas simple, no la repetirem aquí. I, contràriament, al cas simple, us estalviarem les fórmules. Simplement heu de saber que  aquests intervals també es calculen amb R amb la funció `predict.lm` aplicada a:

* un data frame amb els valors de les variables independents sobre l'individu, o els individus, per al que volem estimar la $Y$
* el resultat de la `lm`
* el paràmetre `interval` igualat al tipus d'interval que volem: `"prediction"` si és per al valor en un individu, `"confidence"` si és per al valor esperat



```{example}
Suposem que volem estimar amb un 95% de confiança l'alçada d'un infant de $X_1=69$ dies que en néixer va fer $X_2=45.5$ cm i va pesar $X_3=2.55$ kg i des de llavors el seu pes ha augmentat un $X_4=26.3$%. 

```
Primer definim un data frame que reculli les dades d'aquest infant:
```{r}
infant=data.frame(x1=69,x2=45.5,x3=2.55,x4=26.3)
```

Aleshores:

* Un interval de confiança del 95% per al valor estimat de l'alçada d'aquest infant és

```{r}
predict.lm(lm(y~x1+x2+x3+x4), infant, interval="prediction")
```


* Un interval de confiança del 95% per al valor estimat de l'alçada mitjana de tots els infants amb els mateixos valors de $X_1,\ldots,X_4$ que aquest infant és
```{r}
predict.lm(lm(y~x1+x2+x3+x4), infant, interval="confidence")
```

```{r,echo=FALSE}
est=round(predict.lm(lm(y~x1+x2+x3+x4), infant, interval="prediction")[1,1],1)
IC1=round(c(predict.lm(lm(y~x1+x2+x3+x4), infant, interval="prediction")[1,2],predict.lm(lm(y~x1+x2+x3+x4), infant,interval="prediction")[1,3]),1) 
IC2=round(c(predict.lm(lm(y~x1+x2+x3+x4), infant, interval="confidence")[1,2],predict.lm(lm(y~x1+x2+x3+x4), infant, interval="confidence")[1,3]),1)
```


Obtenim que:

* Estimam que l'alçada d'aquest infant és `r est` cm.

* Estimam amb un 95% de confiança que l'alçada d'aquest infant està entre els `r IC1[1]` i els `r IC1[2]` cm.

* Estimam amb un 95% de confiança que l'alçada mitjana dels infants amb les mateixes característiques que aquest està entre els `r IC2[1]` i els `r IC2[2]` cm. És a dir que, de mitjana, els infants com aquest fan entre `r IC2[1]` i els `r IC2[2]` cm.

### L'ANOVA de la regressió lineal múltiple

Com en el cas simple, en una regressió lineal múltiple ens interessa realitzar el contrast
$$
\left\{\begin{array}{l} H_0: \beta_1=\beta_2=\cdots=\beta_k=0 \\
H_1: \text{hi ha qualque }\beta_i\not= 0 \end{array}
\right.
$$
perquè si $\beta_1=\beta_2=\cdots=\beta_k=0$, el model esdevé
$$
Y|{x_1,\ldots,x_k}=\beta_0+E_{x_1,\ldots,x_k}
$$
i la $Y$ no depèn de les $X_i$, de manera que el model lineal no és adequat.

Això es pot fer amb $k$ contrastos
$$
\left\{\begin{array}{l} H_0: \beta_i=0 \\
H_1: \beta_i\neq 0 \end{array}
\right.
$$
emprant els estadístics $T_i$ que hem definit fa una estona,  que segueixen  lleis t de Student amb $n-k-1$ graus de llibertat. Els p-valors d'aquests contrastos són els de la columna `Pr(>|t|)` a la matriu de `Coefficients` de la sortida de `summary(lm( ))`. No cal ajustar-los, el nombre $n-k-1$ de graus de llibertat ja té en compte que fem $k+1$ contrastos.

```{r}
summary(lm(y~x1+x2+x3+x4))
```

Només obtenim evidència estadística que $\beta_1\neq 0$.


Una altra possibilitat és emprar una ANOVA. Fixau-vos que si 
$$
\beta_1=\beta_2=\cdots=\beta_k=0,
$$ 
aleshores 
$$
\mu_{Y|x_{11},\ldots,x_{k1}}=\cdots=\mu_{Y|x_{1n},\ldots,x_{kn}}=\beta_0
$$
Per tant, si al contrast
$$
\left\{\begin{array}{l}
H_0:\mu_{Y|x_{11},\ldots,x_{1k}}=\cdots=\mu_{Y|x_{n1},\ldots,x_{nk}}\\
H_1:\text{no és veritat que }\mu_{Y|x_{11},\ldots,x_{1k}}=\cdots=\mu_{Y|x_{n1},\ldots,x_{nk}}
\end{array}
\right.
$$
rebutjam la hipòtesi nul·la, això implicarà que podem rebutjar que $\beta_1=\beta_2=\cdots=\beta_k=0$ i podrem concloure que el model lineal té sentit.

La **taula** d'aquesta ANOVA és
$$
\begin{array}{llllll}\hline
\text{Font de} & \text{Graus de} & \text{Suma de} & \text{Quadrats} & \text{Estadístic} & \text{p-valor}\\
\text{variació} & \text{llibertat} &  \text{quadrats} & \text{mitjans}      &\text{de contrast}  & \\\hline
\text{Regressió} & k & SS_R  & MS_R & F &  p\\
\text{Error} & n-k-1 & SS_E  & MS_E & &\\
\hline 
\end{array}
$$
on 
$$
MS_R=\frac{SS_R}{k},\quad MS_E=\frac{SS_E}{n-k-1}
$$
Com a les altres ANOVA, l'estadístic de contrast $F$ és
$$
F=\frac{MS_R}{MS_E}
$$
Si la hipòtesi nul·la és vertadera (i els errors satisfan les condicions establertes al començament de la secció anterior), aquest estadístic de contrast segueix una llei F de Fisher amb $k$ i $n-k-1$ graus de llibertat i té valor proper a 1, de manera que se pren com a p-valor
$$
\text{p-valor}=P(F_{k,n-k-1}\geqslant F).
$$

Calculem la taula ANOVA de l'Exemple \@ref(exm:mult)
```{r}
SS.R=sum((Y.cap-mean(y))^2)
SS.R
SS.E=sum(lm(y~x1+x2+x3+x4)$residuals^2)
SS.E
MS.R=SS.R/k
MS.R
MS.E=SS.E/(n-k-1)
MS.E
F=MS.R/MS.E
F
p.val=1-pf(F,k,n-k-1)
p.val
```


$$
\begin{array}{llllll}\hline
\text{Font de} & \text{Graus de} & \text{Suma de} & \text{Quadrats} & \text{Estadístic} & \text{p-valor}\\
\text{variació} & \text{llibertat} &  \text{quadrats} & \text{mitjans}      &\text{de contrast}  & \\\hline
\text{Regressió} & 4 & `r round(SS.R,3)`  & `r round(MS.R,3)` &  `r round(F,2)` &  `r round(p.val,3)`\\
\text{Error} & 4 & `r round(SS.E,3)`  & `r round(MS.E,3)` & &\\
\hline 
\end{array}
$$

Hem trobat evidència estadística que el model lineal és adequat.

```{block2,type="rmdromans"}
Podeu creure que no hi ha cap funció de cap paquet de R que calculi aquesta taula per a la regressió lineal múltiple? 
```



L'estadístic de contrast i el p-valor d'aquesta ANOVA els podeu trobar a la darrera línia de la sortida `summary(lm( ))`; són el `F-statistic` (i a més us diu els graus de llibertat, `DF`, de la seva distribució) i el `p-value`:


```{r}
summary(lm(y~x1+x2+x3+x4))
```

```{block2,type="rmdromans"}
Podeu creure que no podem extreure el p-valor del `summary(lm(y~x1+x2+x3+x4))` amb un sufix, sinó que l'hem de calcular? Per sort és fàcil, emprant el contingut de `summary(lm( ))$fstatistic`, que és un vector de 3 entrades: el valor de l'estadístic $F$ i els seus dos graus de llibertat. Per tant, per calcular el p-valor, podem fer el següent:
```

```{r}
FF=summary(lm(y~x1+x2+x3+x4))$fstatistic
1-pf(FF[1],FF[2],FF[3])
```

```{block2,type="rmdnote"}
Una altra opció  és emprar la funció `glance` del paquet **broom**, que aplicada a un objecte dóna un *dataframe* (de fet, una adaptació del concepte de *dataframe* a l'anàlisi de dades massives, un **tibble**) amb els seus continguts més importants; vaja, que us permet "donar una ullada" a l'objecte.
```

```{r}
library(broom)
glance(lm(y~x1+x2+x3+x4))
glance(lm(y~x1+x2+x3+x4))$p.value
```


## Test de la lliçó 11

**(1)**   En una regressió lineal d'una variable Y respecte d'una variable independent X, hem emprat les observacions (x,y) d'100 individus i hem obtingut la recta de regressió per mínims quadrats y=3.02-0.96x.  El valor mitjà de la variable X a la mostra que hem emprat ha estat 0.8. Quines de les afirmacions següents són vertaderes?

1. Amb les dades que ens donen, no podem saber el valor mitjà de la variable Y a la mostra que hem emprat.
1. El valor mitjà de la variable Y a la mostra que hem emprat ha estat 2.252.
1. El valor mitjà de la variable Y a la mostra que hem emprat ha estat 3.02.
!. Segons aquest model, en els individus de la població objecte d'estudi, quan la  variable X augmenta una unitat, el valor esperat de la  variable Y augmenta 0.96
1. Segons aquest model, en els individus de la població objecte d'estudi, quan la variable X augmenta una unitat, el valor esperat de la variable Y disminueix 0.96
1. Cap de les altres afirmacions és correcta.


**(2)** En una regressió lineal per mínims quadrats d'una variable Y respecte d'una variable independent X, hem emprat les observacions (x,y) d'100 individus. A la nostra mostra, la mitjana de X ha estat 0.05, la mitjana de Y ha estat -1.3, la variància de X ha estat 2.56, la variància de Y ha estat 8.36 i la covariància de X i Y ha estat 4.54. Què val el coeficient de determinació de la regressió per mínims quadrats de Y respecte de X (arrodonit a 3 xifres decimals)?

1. 0.212
1. 0.963
1. 0.981
1. 1.773
1. No el podem calcular amb les dades que ens donen
1. Cap de les altres respostes és correcta




**(3)** Quines de les afirmacions següents sobre la regressió per mínims quadrats és vertadera?

1. Quan es fa a mà, primer s'estima la pendent de la recta i després el terme independent
1. Quan es fa a mà, primer s'estima el terme independent de la recta i després  la pendent
1. La suma de quadrats total és la suma de quadrats dels tractaments més la suma de quadrats dels errors
1. La suma de quadrats total és la suma de quadrats de la regressió més la suma de quadrats dels errors
1. Els coeficients de la recta són els que fan mínima la suma de quadrats de la regressió
1. Els coeficients de la recta són els que fan màxima la suma de quadrats de la regressió
1. Cap de les altres respostes és correcta

**(4)** Sota les hipòtesis adients per als errors, els estimadors $b_0$ i $b_1$ de $\beta_0$ i $\beta_1$ són màxim versemblants. Però, ... què significa que un estimador sigui màxim versemblant? 

1. Que aplicat a una m.a.s. sempre dóna el p-valor més gran en els contrastos bilaterals del paràmetre que volem estimar
1. Que aplicat a una m.a.s. sempre dóna el p-valor més petit en els contrastos bilaterals del paràmetre que volem estimar
1. Que aplicat a una m.a.s. sempre dóna el valor  més probable del paràmetre.
1. Que aplicat a una m.a.s. sempre dóna el valor  del paràmetre que fa més probable la m.a.s.
1. Que aplicat a una m.a.s. sempre dóna el valor més probable de l'estimador.
1. Que té valor esperat el valor real del paràmetre

**(5)** Quines de les afirmacions següents sobre el coeficient de determinació en una regressió per mínims quadrats són vertaderes?

1. El seu quadrat és la covariància dels vectors x, y de la mostra
1. El seu quadrat és la correlació dels vectors x, y de la mostra
1. La seva arrel quadrada positiva és la covariància dels vectors x, y de la mostra
1. La seva arrel quadrada positiva és la correlació dels vectors x, y de la mostra
1. Cap de les altres respostes és correcta


**(6)** Quines de les afirmacions següents sobre el coeficient de determinació de la regressió lineal per mínims quadrats són vertaderes? Marcau totes les vertaderes.

1. En el cas de la regressió lineal simple, el seu quadrat és la correlació entre els valors de la variable independent i els de la variable dependent a la nostra mostra
1. En el cas de la regressió lineal simple, és el quadrat de la correlació entre els valors de la variable independent i els de la variable dependent a la nostra mostra
1. En el cas de la regressió lineal múltiple, si afegim al nostre model variables independents, el valor del coeficient de determinació no  decreix.
1. En el cas de la regressió lineal múltiple, si empram un nombre més gran de variables independents (però no necessàriament mantenint les variables anteriors) el valor del coeficient de determinació no decreix.
1. Cap de les altres respostes és correcta.






**(7)** En fer la regressió lineal d'una variable Y respecte d'una variable X, hem obtingut la recta y=0.8+1.2x. Quina de les conclusions següents és la més correcta segons aquest model?

1. Un augment d'1 unitat de la variable X causa un augment d'1.2 unitats de la variable Y
1. Un augment d'1 unitat de la variable X està associada a un augment d'1.2 unitats de la variable Y
1. Un augment d'1 unitat de la variable X causa un augment de 0.8 unitats de la variable Y
1. Un augment d'1 unitat de la variable X està associada a un augment de 0.8 unitats de la variable Y





**(8)** Quines de les afirmacions següents sobre l'estimació de la variable dependent Y a partir de la variable independent X per mitjà d'una recta de regressió per mínims quadrats són vertaderes (quan els errors satisfan les condicions adients)?

1. L'estimació puntual del valor de Y sobre un individu concret on X valgui x~0~ i l'estimació de valor mitjà de Y sobre tots els individus on X valgui x~0~ sempre coincideixen
1. L'estimació puntual del valor de Y sobre un individu concret on X valgui x~0~ sempre és estrictament més gran que l'estimació del valor mitjà de Y sobre tots els individus on X valgui x~0~
1. L'interval de confiança del 95% per al valor de Y sobre un individu concret on X valgui x~0~ i l'interval de confiança del 95% per al valor mitjà de Y sobre tots els individus on X valgui x~0~ sempre coincideixen
1. L'interval de confiança del 95% per al valor de Y sobre un individu concret on X valgui x~0~ sempre és  més ample que l'interval de confiança del 95% per al valor mitjà de Y sobre tots els individus on X valgui x~0~
1. L'interval de confiança del 95% per al valor de Y sobre un individu concret on X valgui x~0~ sempre és  més estret que l'interval de confiança del 95% per al valor mitjà de Y sobre tots els individus on X valgui x~0~
1. Cap de les altres respostes és correcta.



**(9)** Emprant un model de regressió lineal per mínims quadrats d'una variable Y respecte d'una variable X, hem estimat el valor de Y quan X=5 amb la funció `predict.lm` i els dos valors possibles del paràmetre `interval` i hem obtingut els intervals de confiança del 95% [2,5] i [3,9], però no sabem quin és l'interval de confiança per al valor predit de Y sobre un individu per al qual X=5 i quin és l'interval de confiança per al valor mitjà de Y sobre els individus per al quals X=5.

1. [2,5] és l'interval de confiança per al valor predit de Y sobre un individu per al qual X=5 i [3,9] és l'interval de confiança per al valor mitjà de Y sobre els individus per al quals X=5.
1. [2,5] és l'interval de confiança per al valor mitjà de Y sobre els individus per al quals X=5 i [3,9] és l'interval de confiança per al valor predit de Y sobre un individu per al qual X=5. 
1. No podem saber quin interval és quin
1. En realitat, hi ha un error: és impossible que  [2,5] i [3,9] (en l'ordre que sigui) siguin intervals de confiança per al valor predit de Y sobre un individu per al qual X=5 i  per al valor mitjà de Y sobre els individus per al quals X=5




**(10)** Emprant un model de regressió lineal per mínims quadrats d'una variable Y respecte d'una variable X, hem estimat el valor de Y quan X=5 amb la funció `predict.lm` i els dos valors possibles del paràmetre `interval` i hem obtingut els intervals de confiança del 95% [2,5] i [1,6], però no sabem quin és l'interval de confiança per al valor predit de Y sobre un individu per al qual X=5 i quin és l'interval de confiança per al valor mitjà de Y sobre els individus per al quals X=5.

1. [2,5] és l'interval de confiança per al valor predit de Y sobre un individu per al qual X=5 i [1,6] és l'interval de confiança per al valor mitjà de Y sobre els individus per al quals X=5.
1. [2,5] és l'interval de confiança per al valor mitjà de Y sobre els individus per al quals X=5 i [1,6] és l'interval de confiança per al valor predit de Y sobre un individu per al qual X=5. 
1. No podem saber quin interval és quin
1. En realitat, hi ha un error: és impossible que  [2,5] i [1,6] (en l'ordre que sigui) siguin intervals de confiança per al valor predit de Y sobre un individu per al qual X=5 i  per al valor mitjà de Y sobre els individus per al quals X=5


**(11)** Quines de les afirmacions següents sobre la recta de regressió per mínims quadrats és vertadera?

1. Sempre passa per l'origen de coordenades
1. Sempre és creixent
1. El coeficient $b_1$ no canvia si canviam les unitats amb les que mesuram la variable independent multiplicant-les per una constant (com ara si canviam cm per m), però no canviam les unitats amb les que mesuram la variable dependent
1. El coeficient $b_0$ no canvia si canviam les unitats amb les que mesuram la variable independent multiplicant-les per una constant  (com ara si canviam cm per m), però no canviam les unitats amb les que mesuram la variable dependent
1. Sempre passa pel punt mig de les observacions que formen la mostra
1. Els valors de $b_0$ i $b_1$ determinen el valor de $R^2$
1. Cap de les altres respostes és correcta





**(12)** Quines de les afirmacions següents sobre el coeficient de determinació en una regressió per mínims quadrats són vertaderes?

1. El seu signe és sempre el del coeficient $b_1$ de la recta de regressió
1. El seu signe és sempre el de la covariància dels vectors x i y de la mostra
1. És el quadrat de la correlació poblacional de les variables X i Y
1. Si sabeu el seu valor i la recta de regressió, podeu saber el valor de la correlació dels vectors x i y de la mostra
1. Cap de les altres respostes és correcta

**(13)** Hem mesurat dues variables X i Y sobre 100 subjectes. La variància de la mostra x de X obtinguda ha estat 3 i la variància de la mostra x de X ha estat 12. Quin, o quins, dels valors següents no poden ser el de la covariància de x i y?

1. 0
1. 1.5
1. 6
1. 9
1. $-\sqrt{2}$
1. -6
1. -9
1. La covariància d'aquests vectors x i y pot ser qualsevol número real



**(14)** En una regressió lineal simple per mínims quadrats, en el contrast sobre si $\beta_0=0$ o no obtenim un p-valor 0.018 i en el contrast sobre si $\beta_1=0$ o no obtenim un p-valor 0.005. Si se satisfan les condicions sobre les variables error perquè aquests p-valors siguin fiables, quines de les afirmacions següents són correctes?

1. Hem obtingut evidència estadística que  $\beta_0=0$ i $\beta_1=0$
1. No hem obtingut evidència estadística que  $\beta_0=0$ i $\beta_1=0$
1. Hem obtingut evidència estadística que  $\beta_0\neq 0$ i $\beta_1\neq 0$
1. No hem obtingut evidència estadística que  $\beta_0\neq 0$ i $\beta_1\neq 0$
1. En la realitat, la probabilitat que $\beta_0=0$ és més petita que la de $\beta_1=0$
1. En la realitat, la probabilitat que $\beta_0=0$ és més gran que la de $\beta_1=0$
1. Estam segurs que $b_0$ i $b_1$ han donat diferent de 0
1. Cap de les altres respostes és correcta




**(15)** En una regressió lineal simple per mínims quadrats d'una variable Y respecte d'una variable X, hem obtingut $b_1=0.18$ i un interval de confiança del 95% per a $\beta_1$ entre 0.1 i 0.26. Si se satisfan les condicions sobre les variables error perquè aquest interval de confiança sigui fiable, quines de les afirmacions següents són correctes?

1. Com que 0.18 pertany a  [0.1,0.26], estam molt segurs que $\beta_1=0.18$
1. Amb nivell de significació 0.05 segur que rebutjarem que  $\beta_1\neq 0$
1. Si calculàssim l'IC del 90% per a $\beta_1$ amb la mateixa mostra, seria més ample que el del 95%
1. Si calculàssim l'IC del 90% per a $\beta_1$ amb la mateixa mostra, seria més estret que el del 95%
1. Per a un 95% de les mostres de la mateixa mida que la nostra, el coeficient $b_1$ estaria entre 0.1 i 0.26
1. Amb nivell de significació 0.05, podem concloure que el model lineal té sentit, en el sentit que la variable Y|x no és la variable error Ex més una constant 
1. Cap de les altres respostes és correcta




**(16)** Quines de les afirmacions següents sobre la regressió lineal múltiple per mínims quadrats són vertaderes? Marcau totes les vertaderes.

1. La funció que s'obté sempre passa per l'origen de coordenades
1. La funció que s'obté sempre  passa pel punt mig de les observacions que formen la mostra
1. La mitjana dels valors estimats de la variable dependent sobre els subjectes de la mostra coincideix amb la mitjana dels valors observats d'aquesta variable sobre aquests subjectes
1. La mitjana dels valors estimats de la variable dependent sobre els subjectes de la mostra coincideix amb la variància dels valors observats d'aquesta variable sobre aquests subjectes
1. La suma de quadrats total és la suma de quadrats dels tractaments més la suma de quadrats dels errors
1. La suma de quadrats total és la suma de quadrats de la regressió més la suma de quadrats dels errors
1. Els coeficients de la funció lineal són els que fan mínima la suma de quadrats dels errors
1. La mitjana dels valors estimats de la variable dependent sobre els subjectes de la mostra coincideix amb la mitjana dels errors en l'estimació d'aquesta variable sobre aquests subjectes
1. La mitjana  dels errors en l'estimació de la variable dependent sobre els subjectes de la mostra és 0
1. La variància  dels errors en l'estimació de la variable dependent sobre els subjectes de la mostra és 0





**(17)** En una regressió lineal, hem obtingut la funció de regressió lineal $Y=2+3X_1-5X_2+4X_3$. Quines de les afirmacions són vertaderes?

1. Segons aquest model, si en un subjecte A el valor de $X_1$ és més gran que en un subjecte B, el valor esperat de $Y$ en el subjecte A és més gran que el valor esperat de $Y$ en el subjecte B
1. Segons aquest model, si en un subjecte A el valor de $X_1$ és més gran que en un subjecte B, pot passar que el valor esperat de $Y$ en el subjecte A sigui més petit que el valor esperat de $Y$ en el subjecte B
1. Segons aquest model, si $X_1$ i $X_3$ romanen constants i $X_2$ augmenta en 1 unitat, el valor esperat de $Y$ disminueix 3 unitats
1. Segons aquest model, si $X_1$ i $X_3$ romanen constants i $X_2$ augmenta en 1 unitat, el valor esperat de $Y$ disminueix 5 unitats
1. Segons aquest model, si $X_1$ i $X_3$ romanen constants i $X_2$ augmenta en 1 unitat, el valor esperat de $Y$ augmenta 5 unitats
1. Cap de les altres respostes és vertadera





**(18)** Quines de les afirmacions següents sobre el coeficient de determinació de la regressió lineal per mínims quadrats són vertaderes? Marcau totes les vertaderes.

1. En el cas de la regressió lineal simple, el seu quadrat és la correlació entre els valors de la variable independent i els de la variable dependent a la nostra mostra
1. En el cas de la regressió lineal simple, és el quadrat de la correlació entre els valors de la variable independent i els de la variable dependent a la nostra mostra
1. En el cas de la regressió lineal múltiple, si afegim al nostre model variables independents, el valor del coeficient de determinació no  decreix.
1. En el cas de la regressió lineal múltiple, si empram un nombre més gran de variables independents (però no necessàriament mantenint les variables anteriors) el valor del coeficient de determinació no decreix.
1. No pot valer 0
1. No pot ser estrictament negatiu
1. Cap de les altres respostes és correcta.


**(19)** En la regressió lineal múltiple de Y en funció de les variables independents $X_1,X_2,X_3,X_4,X_5$ a partir d'una certa mostra hem obtingut que $R^2=0.92$ i $R^2_{adj}=0.82$, mentre que si fem la regressió lineal múltiple de Y en funció només de les variables independents $X_1,X_2,X_3$ a partir de la mateixa mostra hem obtingut que $R^2=0.91$ i $R^2_{adj}=0.865$. Quina de les afirmacions següents és vertadera?

1. Això no pot ser, perquè $R^2_{adj}$ sempre és més gran o igual que R2.
1. Això no pot ser, perquè quan $R^2$ decreix, $R^2_{adj}$ també decreix.
1. Això no pot ser, perquè quan eliminam variables independents, la $R^2$ sempre creix.
1. Això sí que pot ser, i llavors consideram que el model amb 5 variables independents és millor, perquè té el valor de $R^2$ més gran
1. Això sí que pot ser, i llavors consideram que el model amb 3 variables independents és millor, perquè té el valor de $R^2_{adj}$ més gran
1. Això sí que pot ser, i llavors consideram que el model amb 3 variables independents és millor, perquè té el valor de $R^2$ més petit
1. Cap de les altres respostes és correcta.






**(20)** Quines de les afirmacions següents sobre $R^2_{adj}$ són vertaderes?

1. Sempre és més petit que  $R^2$
1. Pot ser més petit o més gran que  $R^2$
1. Pot ser estrictament negatiu
1. Pot valer 0
1. Si afegim variables independents a la regressió lineal, el seu valor no  decreix
1. Si afegim variables independents a la regressió lineal, el seu valor no  creix
1. Cap de les altres respostes és vertadera







**(21)** En el contrast d’hipòtesis sobre si un coeficient $\beta_i$ és 0 o no, la conclusió amb nivell de significació 0.1 ha estat que hem d’acceptar la hipòtesi nul·la. Quines de les afirmacions següents són vertadera en aquesta situació? 

1. Hem acceptat la hipòtesi nul·la perquè hem obtingut evidència estadística que la hipòtesi nul·la és vertadera.
1. Hem acceptat la hipòtesi nul·la perquè no hem obtingut evidència estadística que la hipòtesi nul·la sigui falsa.
1. Hem acceptat que $\beta_i=0$
1. Hem acceptat la hipòtesi nul·la perquè hem obtingut evidència estadística que la probabilitat que la hipòtesi alternativa sigui vertadera és 0.1 o menys.
1. Assumim una probabilitat d’equivocar-nos acceptant la hipòtesi nul·la quan és falsa de 0.1.
1. Assumim una probabilitat d’equivocar-nos rebutjant la hipòtesi nul·la quan és vertadera de 0.1.
1. Cap de les altres respostes és correcta



**(22)** Quina o quines de les afirmacions següents sobre la regressió lineal per mínims quadrats d’una variable Y respecte d’un conjunt de variables independents són vertaderes? 


1. Si el nombre de variables independents és el mateix, consideram més bona una funció de regressió lineal de Y com més gran és el quocient de la mitjana dels valors de Y dels punts de la mostra entre la mitjana dels valors estimats de Y sobre els punts de la mostra.
1. Si el nombre de variables independents és el mateix, consideram més bona una funció de regressió lineal de Y com més gran és el quocient de la variància dels valors de Y dels punts de la mostra entre la variància dels valors estimats de Y sobre els punts de la mostra.
1. Si el nombre de variables independents és el mateix, consideram més bona una funció de regressió lineal de Y  com més gran és el quocient de la variància dels valors estimats de Y sobre els punts de la mostra entre la variància dels valors de Y dels punts de la mostra
1. Independentment del nombre de variables independents, consideram més bona una funció de regressió lineal de Y com més gran és el quocient de la mitjana dels valors de Y dels punts de la mostra entre la mitjana dels valors estimats de Y sobre els punts de la mostra.
1. Independentment del nombre de variables independents, consideram més bona una funció de regressió lineal de Y com més gran és el quocient de la variància dels valors de Y dels punts de la mostra entre la variància dels valors estimats de Y sobre els punts de la mostra.
1. Independentment del nombre de variables independents, consideram més bona una funció de regressió lineal de Y  com més gran és el quocient de la variància dels valors estimats de Y sobre els punts de la mostra entre la variància dels valors de Y dels punts de la mostra




**(23)**  Roda el món i torna al Born. Quan diem que el 0.975-quantil d'una variable aleatòria normal estàndard Z és 1.96, això significa que:

1. P(Z=0.975)=1.96
1. P(Z<0.975)=1.96
1. P(Z>0.975)=1.96
1. P(Z=1.96)=0.975
1. P(Z<1.96)=0.975
1. P(Z>1.96)=0.975
1. En realitat no significa res de tot això.



