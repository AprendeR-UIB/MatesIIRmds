<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="1.1 Regressió lineal simple | Matemàtiques II v.2021" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Apunts Matemàtiques II bookdown::gitbook." />
<meta name="github-repo" content="cescrossello/MatesII" />


<meta name="date" content="2021-09-17" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Apunts Matemàtiques II bookdown::gitbook.">

<title>1.1 Regressió lineal simple | Matemàtiques II v.2021</title>

<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation-1.1/tabsets.js"></script>
p.caption {
  color: #777;
  margin-top: 10px;
}
p code {
  white-space: inherit;
}
pre {
  word-break: normal;
  word-wrap: normal;
}
pre code {
  white-space: inherit;
}
.rmdcaution, .rmdimportant, .rmdnote, .rmdrecordau {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  background: #f5f5f5 5px center/3em no-repeat;
}
.rmdmercifulgod {
  padding: 1em 1em 1em 6em;
  margin-bottom: 10px;
  background: #f5f5f5 5px center/5em no-repeat;
}
.rmdromans {
  padding: 1em 1em 2em 6em;
  margin-bottom: 10px;
  background: #f5f5f5 5px center/5em no-repeat;
}
.rmdexercici {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  background: #f9f9f9 5px center/3em no-repeat;
}
.rmderror {
  padding: 1em 1em 2em 7em;
  margin-bottom: 10px;
  background: #f5f5f5 5px center/6em no-repeat;
}
.rmderrorpetit {
  padding: 1em 1em 2em 4em;
  margin-bottom: 10px;
  background: #f5f5f5 5px center/3em no-repeat;
}
.rmdcorbes {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  background: #f9f9f9  5px center/3em  no-repeat;
}
.rmdcaution {
  background-image: url("Bioestadistica-II_files/figure-html/caution.png");
}
.rmdimportant {
  background-image: url("Bioestadistica-II_files/figure-html/important.png");
}
.rmdnote {
  background-image: url("Bioestadistica-II_files/figure-html/note.png");
}
.rmdcorbes {
  background-image: url("Bioestadistica-II_files/figure-html/corbes.png");
}
.rmdrecordau {
  background-image: url("Bioestadistica-II_files/figure-html/recordau.png");
}
.rmderror {
  background-image: url("Bioestadistica-II_files/figure-html/error.png");
}
.rmderrorpetit{
  background-image: url("Bioestadistica-II_files/figure-html/error.png");
}
.rmdmercifulgod {
  background-image: url("Bioestadistica-II_files/figure-html/mercifulgod.png");
}
.rmdromans {
  background-image: url("Bioestadistica-II_files/figure-html/romanos.png");
}
.rmdexercici {
  background-image: url("Bioestadistica-II_files/figure-html/exercici.png");
}


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if boostrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#presentació">Presentació</a></li>
<li><a href="1-regressió-lineal.html#regressió-lineal"><span class="toc-section-number">1</span> Regressió lineal</a>
<ul>
<li><a href="1.1-regressió-lineal-simple.html#regressió-lineal-simple"><span class="toc-section-number">1.1</span> Regressió lineal simple</a>
<ul>
<li><a href="1.1-regressió-lineal-simple.html#el-model"><span class="toc-section-number">1.1.1</span> El model</a></li>
<li><a href="1.1-regressió-lineal-simple.html#mínims-quadrats"><span class="toc-section-number">1.1.2</span> Mínims quadrats</a></li>
<li><a href="1.1-regressió-lineal-simple.html#coeficient-de-determinació"><span class="toc-section-number">1.1.3</span> Coeficient de determinació</a></li>
<li><a href="1.1-regressió-lineal-simple.html#intervals-de-confiança-dels-coeficients"><span class="toc-section-number">1.1.4</span> Intervals de confiança dels coeficients</a></li>
<li><a href="1.1-regressió-lineal-simple.html#intervals-de-confiança-per-a-les-estimacions-de-la-variable-dependent"><span class="toc-section-number">1.1.5</span> Intervals de confiança per a les estimacions de la variable dependent</a></li>
<li><a href="1.1-regressió-lineal-simple.html#té-sentit-una-regressió-lineal"><span class="toc-section-number">1.1.6</span> Té sentit una regressió lineal?</a></li>
</ul></li>
<li><a href="1.2-regressió-lineal-múltiple.html#regressió-lineal-múltiple"><span class="toc-section-number">1.2</span> Regressió lineal múltiple</a>
<ul>
<li><a href="1.2-regressió-lineal-múltiple.html#mínims-quadrats-1"><span class="toc-section-number">1.2.1</span> Mínims quadrats</a></li>
<li><a href="1.2-regressió-lineal-múltiple.html#coeficient-de-determinació-múltiple"><span class="toc-section-number">1.2.2</span> Coeficient de determinació múltiple</a></li>
<li><a href="1.2-regressió-lineal-múltiple.html#coeficient-de-determinació-ajustat"><span class="toc-section-number">1.2.3</span> Coeficient de determinació ajustat</a></li>
<li><a href="1.2-regressió-lineal-múltiple.html#intervals-de-confiança-per-als-coeficients"><span class="toc-section-number">1.2.4</span> Intervals de confiança per als coeficients</a></li>
<li><a href="1.2-regressió-lineal-múltiple.html#intervals-de-confiança-per-a-les-estimacions-de-la-variable-resposta"><span class="toc-section-number">1.2.5</span> Intervals de confiança per a les estimacions de la variable resposta</a></li>
<li><a href="1.2-regressió-lineal-múltiple.html#lanova-de-la-regressió-lineal-múltiple"><span class="toc-section-number">1.2.6</span> L’ANOVA de la regressió lineal múltiple</a></li>
</ul></li>
<li><a href="1.3-test-de-la-lliçó-11.html#test-de-la-lliçó-11"><span class="toc-section-number">1.3</span> Test de la lliçó 11</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="regressió-lineal-simple" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Regressió lineal simple</h2>
<p>El problema plantejat a l’Exemple <a href="1-regressió-lineal.html#exm:edats1">1.1</a> és una instància de la situació general en la qual tenim parelles d’observacions de dues variables <span class="math inline">\(X\)</span> i <span class="math inline">\(Y\)</span> sobre una mostra de <span class="math inline">\(n\geqslant 2\)</span> subjectes,
<span class="math display">\[
(x_i,y_i)_{i=1,2,\ldots,n},
\]</span>
i volem estudiar com depèn el valor de la variable <span class="math inline">\(Y\)</span> del de <span class="math inline">\(X\)</span>. En aquest context:</p>
<ul>
<li><p>Direm que <span class="math inline">\(X\)</span> és la variable <strong>de control</strong> o <strong>independent</strong></p></li>
<li><p>Direm que <span class="math inline">\(Y\)</span> l’anomenam la variable <strong>de resposta</strong> o <strong>dependent</strong></p></li>
</ul>

<div class="rmdnote">
La variable de control no té per què ser aleatòria: nosaltres podem fixar el seu valor sobre els subjectes. Per exemple, la variable <span class="math inline">\(X\)</span> podria ser la dosi d’una medicació i que nosaltres decidíssim a cada individu, de manera planificada i gens aleatòria, quina dosi li administram. En canvi, la variable de resposta ha de ser aleatòria. Si no, no té sentit estimar res sobre ella.
</div>
<p>En general, volem trobar la millor relació funcional (el millor <strong>model estadístic</strong>, amb la terminologia introduïda en el tema anterior) que expliqui la variable <span class="math inline">\(Y\)</span> en funció de la variable <span class="math inline">\(X\)</span>. En aquest tema, cercarem un <strong>model lineal</strong>. Les tècniques que es fan servir per resoldre aquest problema s’anomenen genèricament de <strong>regressió lineal</strong>. Nosaltres n’estudiarem una de concreta: la <strong>regressió lineal per mínims quadrats</strong>.</p>

<div class="rmdnote">
El nom “regressió” per parlar del tipus de tècniques que permeten ajustar una recta a un conjunt de punts prové del títol d’un article de Galton d’1886, <em>Regression Towards Mediocrity in Hereditary Stature</em>. En aquest article hi va analitzar les alçades d’una mostra de 928 adults i les alçades mitjanes dels seus pares. Hi observà que els pares alts tendien a tenir fills més baixos que ells i que els pares baixos tendien a tenir fills més alts que ells. D’aquest efecte en digué “regressió a la mediocritat” al títol de l’article. D’aquí s’adoptà el terme “regressió” per descriure la tècnica que emprà per obtenir la recta vermella del gràfic següent, amb la qual suportava la seva conclusió comparant-la amb la diagonal (la línia discontínua), i amb el temps el nom de l’efecte que observà es canvià al menys ofensiu “regressió a la mitjana”. Més endavant tornarem sobre aquest exemple.
</div>
<p><img src="Bioestadistica-II_files/figure-html/unnamed-chunk-15-1.png" width="50%" style="display: block; margin: auto;" /></p>
<div id="el-model" class="section level3" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> El model</h3>
<p>En el <strong>model de regressió lineal</strong> suposam que existeixen <span class="math inline">\(\beta_0,\beta_1\in \mathbb{R}\)</span> tals que
<span class="math display">\[
\mu_{Y|x}=\beta_0+\beta_1 x
\]</span>
on <span class="math inline">\(\mu_{Y|x}\)</span> és el valor esperat de <span class="math inline">\(Y\)</span> sobre els subjectes per als quals <span class="math inline">\(X\)</span> val <span class="math inline">\(x\)</span>. Volem estimar aquests paràmetres <span class="math inline">\(\beta_0\)</span> (el <strong>terme independent</strong> del model) i <span class="math inline">\(\beta_1\)</span> (la <strong>pendent</strong> del model) a partir d’una mostra.</p>

<div class="rmdimportant">
<p>Recordau la interpretació d’una funció lineal <span class="math inline">\(y=a_0+a_1x\)</span>:</p>
<ul>
<li><p>El terme independent <span class="math inline">\(a_0\)</span> és el valor de <span class="math inline">\(y\)</span> quan <span class="math inline">\(x=0\)</span></p></li>
<li><p>La pendent <span class="math inline">\(a_1\)</span> és la variació de <span class="math inline">\(y\)</span> quan <span class="math inline">\(x\)</span> augmenta en 1 unitat</p></li>
</ul>
<p>Per tant, en el nostre model de regressió lineal:</p>
<ul>
<li><p><span class="math inline">\(\beta_0\)</span> és el valor esperat de <span class="math inline">\(Y\)</span> en els subjectes en els quals <span class="math inline">\(X\)</span> val 0</p></li>
<li><p><span class="math inline">\(\beta_1\)</span> és la variació del valor esperat de <span class="math inline">\(Y\)</span> quan el valor de <span class="math inline">\(X\)</span> augmenta 1 unitat</p></li>
</ul>
</div>
<p>Amb una mostra <span class="math inline">\((x_i,y_i)_{i=1,2,\ldots,n}\)</span>, calcularem estimacions <span class="math inline">\(b_0\)</span> i <span class="math inline">\(b_1\)</span> de
<span class="math inline">\(\beta_0\)</span> i <span class="math inline">\(\beta_1\)</span>. Això ens donarà la <strong>recta de regressió</strong> per a la nostra mostra:
<span class="math display">\[
\widehat{Y}=b_0+b_1 X.
\]</span>
Aquesta recta, donat un valor <span class="math inline">\(x_0\)</span> de <span class="math inline">\(X\)</span>, permet estimar el valor <span class="math inline">\(\widehat{y}_0=b_0+b_1 x_0\)</span> de <span class="math inline">\(Y\)</span> sobre un subjecte en el qual <span class="math inline">\(X\)</span> valgui <span class="math inline">\(x_0\)</span>. Hi empram <span class="math inline">\(\widehat{Y}\)</span> a la dreta per posar èmfasi que no és que <span class="math inline">\(Y\)</span> sigui <span class="math inline">\(b_0+b_1X\)</span>, sinó que això darrer estima el valor de <span class="math inline">\(Y\)</span> a partir del valor de <span class="math inline">\(X\)</span>. En concret, si <span class="math inline">\(\widehat{y}_0=b_0+b_1 x_0\)</span>, direm a <span class="math inline">\(\widehat{y}_0\)</span> el <strong>valor estimat</strong> de <span class="math inline">\(Y\)</span> quan <span class="math inline">\(X=x_0\)</span>.</p>

<div class="rmdcaution">
<p>Fixau-vos que, d’aquesta manera, donada una observació <span class="math inline">\((x_i,y_i)\)</span> de la nostra mostra, distingim entre</p>
<ul>
<li><p><span class="math inline">\(y_i\)</span>: el valor de <span class="math inline">\(Y\)</span> sobre l’individu corresponent</p></li>
<li><p><span class="math inline">\(\widehat{y}_i=b_0+b_1 x_i\)</span>: l’estimació del valor de <span class="math inline">\(Y\)</span> sobre l’individu corresponent a partir del seu valor de <span class="math inline">\(X\)</span> i la recta de regressió obtinguda</p></li>
</ul>
</div>
<p>El model anterior el reescrivim com a
<span class="math display">\[
Y|x=\mu_{Y|x}+ E_x=\beta_0+\beta_1 x+ E_x
\]</span>
on</p>
<ul>
<li><p><span class="math inline">\(Y|x\)</span> és la variable aleatòria <strong>“valor de <span class="math inline">\(Y\)</span> quan <span class="math inline">\(X\)</span> val <span class="math inline">\(x\)</span>”</strong>: Prenem un subjecte en el qual <span class="math inline">\(X\)</span> val <span class="math inline">\(x\)</span> i hi mesuram <span class="math inline">\(Y\)</span></p></li>
<li><p><span class="math inline">\(\mu_{Y|x}\)</span> és el valor esperat de <span class="math inline">\(Y|x\)</span>, és a dir, la mitjana dels valors de <span class="math inline">\(Y\)</span> sobre tots els individus en els quals <span class="math inline">\(X\)</span> valgui <span class="math inline">\(x\)</span></p></li>
<li><p><span class="math inline">\(E_x=Y|x -\mu_{Y|x}\)</span> és la variable aleatòria <strong>error</strong> o <strong>residu</strong>, que dóna la diferència entre el valor de <span class="math inline">\(Y\)</span> en un individu amb <span class="math inline">\(X=x\)</span> i el seu valor esperat</p></li>
</ul>
<p>Prenent valors esperats als dos costats de la igualtat <span class="math inline">\(Y|x=\mu_{Y|x}+ E_x\)</span> obtenim que <span class="math inline">\(\mu_{Y|x}=\mu_{Y|x}+ \mu_{E_x}\)</span> i per tant que <span class="math inline">\(\mu_{E_x}=0\)</span>. Així doncs, aquest model implica que <strong>els valors esperats de les variables error <span class="math inline">\(E_x\)</span> són tots 0</strong>.</p>
</div>
<div id="mínims-quadrats" class="section level3" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Mínims quadrats</h3>
<p>L’<strong>error</strong> que cometem amb l’estimació <span class="math inline">\(\widehat{y}_i=b_0+b_1x_i\)</span> a cada observació <span class="math inline">\((x_i,y_i)\)</span> de la mostra és
<span class="math display">\[
e_i=y_i-\widehat{y}_i=y_i-(b_0+b_1 x_i)
\]</span></p>
<p>La <strong>Suma dels Quadrats dels Errors</strong> d’aquesta estimació és
<span class="math display">\[
SS_E=\sum_{i=1}^n e_i^2=\sum_{i=1}^n (y_i-b_0-b_1 x_i)^2
\]</span>
A la <strong>regressió lineal per mínims quadrats</strong>, s’estimen <span class="math inline">\(\beta_0\)</span> i <span class="math inline">\(\beta_1\)</span> per mitjà dels valors de <span class="math inline">\(b_0\)</span> i <span class="math inline">\(b_1\)</span> que minimitzen aquesta <span class="math inline">\(SS_E\)</span>. Aquests valors són donats pel resultat següent:</p>

<div class="theorem">
<span id="thm:RLS" class="theorem"><strong>Teorema 1.1  </strong></span>Els estimadors <span class="math inline">\(b_0\)</span> i <span class="math inline">\(b_1\)</span> per mínims quadrats de <span class="math inline">\(\beta_0\)</span> i <span class="math inline">\(\beta_1\)</span> són
<span class="math display">\[
b_1 =\frac{{s}_{xy}}{{s}_x^2}=\frac{\widetilde{s}_{xy}}{\widetilde{s}_x^2},\quad b_0 = \overline{y}-b_1 \overline{x}.
\]</span>
</div>

<div class="rmdcorbes">
<p>Per trobar-los, empram que els valors de <span class="math inline">\(b_0,b_1\)</span> que fan mínim
<span class="math display">\[
SS_E=\sum_{i=1}^n (y_i-b_0-b_1 x_i)^2
\]</span>
anul·len les derivades de <span class="math inline">\(SS_E\)</span> respecte de <span class="math inline">\(b_0\)</span> i <span class="math inline">\(b_1\)</span>.</p>
<p>Derivem:
<span class="math display">\[
\begin{array}{l}
\displaystyle\dfrac{\partial SS_E}{\partial b_0}=-2\sum\limits_{i=1}^n (y_i -b_0-b_1 x_i)\\[2ex]
\displaystyle\dfrac{\partial SS_E}{\partial b_1}=-2\sum\limits_{i=1}^n (y_i -b_0-b_1 x_i) x_i 
\end{array}
\]</span>
El <span class="math inline">\((b_0,b_1)\)</span> que cercam satisfà
<span class="math display">\[
\begin{array}{l}
\displaystyle 2\sum\limits_{i=1}^n (y_i -b_0-b_1 x_i)=0\\[2ex]
\displaystyle 2\sum\limits_{i=1}^n (y_i -b_0-b_1 x_i) x_i =0
\end{array}
\]</span>
Ho reescrivim:
<span class="math display">\[
\begin{array}{rl}
\displaystyle n b_0 + \Big(\sum\limits_{i=1}^n x_i\Big) b_1 &amp; =\sum\limits_{i=1}^n y_i\\[1ex]
\displaystyle \Big(\sum\limits_{i=1}^n x_i\Big) b_0 + \Big(\sum\limits_{i=1}^n x_i^2\Big) b_1 &amp;=\sum\limits_{i=1}^n x_iy_i
\end{array}
\]</span>
Les solucions són
<span class="math display">\[
\begin{array}{rl}
b_1&amp; \displaystyle=\frac{n \sum\limits_{i=1}^n x_i y_i-\sum\limits_{i=1}^n x_i\sum\limits_{i=1}^n y_i} {n\sum\limits_{i=1}^n
x_i^2-\big(\sum\limits_{i=1}^n x_i\big)^2}\\[6ex]
b_0&amp; \displaystyle=\frac{\sum\limits_{i=1}^n y_i -b_1 \sum\limits_{i=1}^n x_i}{n}
\end{array}
\]</span>
i es pot comprovar que donen el mínim de <span class="math inline">\(SS_E\)</span>.</p>
Ara, recordant que
<span class="math display">\[
\begin{array}{l}
\displaystyle\overline{x}=\frac{1}{n}\sum\limits_{i=1}^n x_i,
\quad \overline{y}=\frac{1}{n} \sum\limits_{i=1}^n y_i\\[2ex]
\displaystyle s_x^2  =\frac{1}{n}\Big(\sum_{i=1}^n x_i^2\Big) -\overline{x}^2,\quad
\displaystyle s_y^2  =\frac{1}{n}\Big(\sum_{i=1}^n y_i^2\Big) -\overline{y}^2\\[2ex]
\displaystyle s_{xy}  =\frac{1}{n}\Big(\sum_{i=1}^n x_i y_i\Big)-\overline{x}\cdot\overline{y}
\end{array}
\]</span>
s’obté finalment que
<span class="math display">\[
b_1 =\frac{{s}_{xy}}{{s}_x^2},\quad b_0 = \overline{y}-b_1 \overline{x}
\]</span>
</div>
<p>La igualtat
<span class="math display">\[
\frac{{s}_{xy}}{{s}_x^2}=\frac{\widetilde{s}_{xy}}{\widetilde{s}_x^2}
\]</span>
és conseqüència que, a les dues fraccions, els denominadors del numerador i el denominador se cancel·len:
<span class="math display">\[
\frac{{s}_{xy}}{{s}_x^2}=\frac{\frac{\sum_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})}{n}}{\frac{\sum_{i=1}^n (x_i-\overline{x})^2}{n}}=\frac{{\sum_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})}}{{\sum_{i=1}^n (x_i-\overline{x})^2}}=\frac{\frac{\sum_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})}{n-1}}{\frac{\sum_{i=1}^n (x_i-\overline{x})^2}{n-1}}=
\frac{\widetilde{s}_{xy}}{\widetilde{s}_x^2}
\]</span></p>
<p>Aquests <span class="math inline">\(b_0\)</span> i <span class="math inline">\(b_1\)</span> són els que calcula la funció <code>lm</code>.</p>

<div class="example">
<p><span id="exm:edats" class="example"><strong>Exemple 1.2  </strong></span>Calculem la recta de regressió per mínims quadrats de les edats i alçades de l’Exemple <a href="1-regressió-lineal.html#exm:edats1">1.1</a>, que eren</p>
</div>
<table>
<thead>
<tr>
<th style="text-align:right;">
edat
</th>
<th style="text-align:right;">
alçada
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
75
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
92
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
108
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
121
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
130
</td>
</tr>
<tr>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
142
</td>
</tr>
<tr>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
155
</td>
</tr>
</tbody>
</table>
<p>Començarem trobant els estadístics que ens calen per calcular els coeficients <span class="math inline">\(b_0\)</span> i <span class="math inline">\(b_1\)</span>. Ja que hi som, també trobarem la variància de les alçades, que per calcular <span class="math inline">\(b_0\)</span> i <span class="math inline">\(b_1\)</span> no ens fa falta però que més tard sí que necessitarem saber-la:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="1.1-regressió-lineal-simple.html#cb4-1" aria-hidden="true" tabindex="-1"></a>edat<span class="ot">=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">11</span>, <span class="dv">13</span>)</span>
<span id="cb4-2"><a href="1.1-regressió-lineal-simple.html#cb4-2" aria-hidden="true" tabindex="-1"></a>alçada<span class="ot">=</span><span class="fu">c</span>(<span class="dv">75</span>, <span class="dv">92</span>, <span class="dv">108</span>, <span class="dv">121</span>, <span class="dv">130</span> , <span class="dv">142</span>, <span class="dv">155</span>)</span>
<span id="cb4-3"><a href="1.1-regressió-lineal-simple.html#cb4-3" aria-hidden="true" tabindex="-1"></a>x.b<span class="ot">=</span><span class="fu">mean</span>(edat)</span>
<span id="cb4-4"><a href="1.1-regressió-lineal-simple.html#cb4-4" aria-hidden="true" tabindex="-1"></a>y.b<span class="ot">=</span><span class="fu">mean</span>(alçada)</span>
<span id="cb4-5"><a href="1.1-regressió-lineal-simple.html#cb4-5" aria-hidden="true" tabindex="-1"></a>s2.x<span class="ot">=</span><span class="fu">var</span>(edat)</span>
<span id="cb4-6"><a href="1.1-regressió-lineal-simple.html#cb4-6" aria-hidden="true" tabindex="-1"></a>s2.y<span class="ot">=</span><span class="fu">var</span>(alçada)</span>
<span id="cb4-7"><a href="1.1-regressió-lineal-simple.html#cb4-7" aria-hidden="true" tabindex="-1"></a>s.xy<span class="ot">=</span><span class="fu">cov</span>(edat,alçada)</span>
<span id="cb4-8"><a href="1.1-regressió-lineal-simple.html#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">c</span>(x.b,y.b,s2.x,s2.y,s.xy),<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1]   7.000 117.571  18.667 786.952 120.667</code></pre>
<p>Obtenim
<span class="math display">\[
\begin{array}{cccccccc}
\overline{x} &amp;  \overline{y} &amp; \widetilde{s}_x^2 &amp; \widetilde{s}_y^2 &amp; \widetilde{s}_{xy}\\ \hline
7 &amp; 117.571 &amp; 18.667 &amp; 786.952 &amp; 120.667
\end{array}
\]</span>
Aleshores
<span class="math display">\[
\begin{array}{l}
\displaystyle b_1 =\frac{\widetilde{s}_{xy}}{\widetilde{s}_x^2}=\frac{120.667}{18.667}=6.464\\[2ex]
\displaystyle b_0 = \overline{y}-b_1 \overline{x} =117.571-6.464\cdot 7=72.321
\end{array}
\]</span>
Trobam la recta de regressió
<span class="math display">\[
\widehat{Y}=72.321+6.464 X
\]</span>
que coincideix amb la recta que calcula <code>lm</code>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="1.1-regressió-lineal-simple.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(alçada<span class="sc">~</span>edat)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = alçada ~ edat)
## 
## Coefficients:
## (Intercept)         edat  
##      72.321        6.464</code></pre>
<p>Els coeficients <span class="math inline">\(b_0,b_1\)</span> s’obtenen, respectivament, afegint els sufixos <code>$coefficients[1]</code> i <code>$coefficients[2]</code> al resultat de la funció <code>lm</code>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="1.1-regressió-lineal-simple.html#cb8-1" aria-hidden="true" tabindex="-1"></a>b0.edat<span class="ot">=</span><span class="fu">lm</span>(alçada<span class="sc">~</span>edat)<span class="sc">$</span>coefficients[<span class="dv">1</span>]</span>
<span id="cb8-2"><a href="1.1-regressió-lineal-simple.html#cb8-2" aria-hidden="true" tabindex="-1"></a>b0.edat</span></code></pre></div>
<pre><code>## (Intercept) 
##    72.32143</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="1.1-regressió-lineal-simple.html#cb10-1" aria-hidden="true" tabindex="-1"></a>b1.edat<span class="ot">=</span><span class="fu">lm</span>(alçada<span class="sc">~</span>edat)<span class="sc">$</span>coefficients[<span class="dv">2</span>]</span>
<span id="cb10-2"><a href="1.1-regressió-lineal-simple.html#cb10-2" aria-hidden="true" tabindex="-1"></a>b1.edat</span></code></pre></div>
<pre><code>##     edat 
## 6.464286</code></pre>
<p>Segons aquesta estimació, l’alçada mitjana dels nins augmenta 6.46 cm anuals, partint d’una alçada mitjana de 72.3 cm en néixer.</p>

<div class="rmdcaution">
Els càlculs involucrats en la regressió lineal són molt poc robusts, en el sentit que els arrodoniments poden influir molt en el resultat final. A <a href="\url%7Bhttp://en.wikipedia.org/wiki/Simple_linear_regression%7D">l’entrada sobre regressió lineal de la Wikipedia</a> hi trobareu un exemple detallat d’una regressió de pes en funció d’alçada. Calculada en metres dóna:
<span class="math display">\[
\widehat{Y}=61.272X-39.062
\]</span>
Si es passen les alçades a polzades, s’arrodoneixen, es calcula la recta de regressió, i es torna a passar el resultat a metres, dóna
<span class="math display">\[
\widehat{Y}=61.675X-39.746
\]</span>
La moralitat d’aquesta història és que, si feu els càlculs a mà, procurau no arrodonir fins al resultat final.
</div>

<div class="example">
<p><span id="exm:sal" class="example"><strong>Exemple 1.3  </strong></span>En un experiment on es volia estudiar l’associació entre el consum de sal i la tensió arterial, a alguns individus se’ls assignà aleatòriament una quantitat diària constant de sal en la seva dieta, i al cap d’un mes se’ls mesurà la tensió mitjana. Alguns resultats varen ser els següents:</p>
</div>
<table>
<thead>
<tr>
<th style="text-align:right;">
X (sal, en g)
</th>
<th style="text-align:right;">
Y (pressió, en mm de Hg)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1.8
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:right;">
2.2
</td>
<td style="text-align:right;">
98
</td>
</tr>
<tr>
<td style="text-align:right;">
3.5
</td>
<td style="text-align:right;">
110
</td>
</tr>
<tr>
<td style="text-align:right;">
4.0
</td>
<td style="text-align:right;">
110
</td>
</tr>
<tr>
<td style="text-align:right;">
4.3
</td>
<td style="text-align:right;">
112
</td>
</tr>
<tr>
<td style="text-align:right;">
5.0
</td>
<td style="text-align:right;">
120
</td>
</tr>
</tbody>
</table>
<p>Volem trobar la recta de regressió lineal per mínims quadrats de <span class="math inline">\(Y\)</span> en funció de <span class="math inline">\(X\)</span> a partir d’aquesta mostra.</p>
<p>Calculem els estadístics que necessitam:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="1.1-regressió-lineal-simple.html#cb12-1" aria-hidden="true" tabindex="-1"></a>sal<span class="ot">=</span><span class="fu">c</span>(<span class="fl">1.8</span>, <span class="fl">2.2</span>,<span class="fl">3.5</span>,<span class="fl">4.0</span>,<span class="fl">4.3</span>,<span class="fl">5.0</span>)</span>
<span id="cb12-2"><a href="1.1-regressió-lineal-simple.html#cb12-2" aria-hidden="true" tabindex="-1"></a>tensió<span class="ot">=</span><span class="fu">c</span>(<span class="dv">100</span>,<span class="dv">98</span>,<span class="dv">110</span>,<span class="dv">110</span>,<span class="dv">112</span>,<span class="dv">120</span>)</span>
<span id="cb12-3"><a href="1.1-regressió-lineal-simple.html#cb12-3" aria-hidden="true" tabindex="-1"></a>x.b<span class="ot">=</span><span class="fu">mean</span>(sal)</span>
<span id="cb12-4"><a href="1.1-regressió-lineal-simple.html#cb12-4" aria-hidden="true" tabindex="-1"></a>y.b<span class="ot">=</span><span class="fu">mean</span>(tensió)</span>
<span id="cb12-5"><a href="1.1-regressió-lineal-simple.html#cb12-5" aria-hidden="true" tabindex="-1"></a>s2.x<span class="ot">=</span><span class="fu">var</span>(sal)</span>
<span id="cb12-6"><a href="1.1-regressió-lineal-simple.html#cb12-6" aria-hidden="true" tabindex="-1"></a>s2.y<span class="ot">=</span><span class="fu">var</span>(tensió)</span>
<span id="cb12-7"><a href="1.1-regressió-lineal-simple.html#cb12-7" aria-hidden="true" tabindex="-1"></a>s.xy<span class="ot">=</span><span class="fu">cov</span>(sal,tensió)</span>
<span id="cb12-8"><a href="1.1-regressió-lineal-simple.html#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">c</span>(x.b,y.b,s2.x,s2.y,s.xy),<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1]   3.467 108.333   1.543  66.267   9.773</code></pre>
<p><span class="math display">\[
\begin{array}{ccccc}
\overline{x} &amp;  \overline{y} &amp; \widetilde{s}_x^2 &amp; \widetilde{s}_y^2 &amp; \widetilde{s}_{xy}\\ \hline
3.467 &amp; 108.333 &amp; 1.543 &amp; 66.267 &amp; 9.773
\end{array}
\]</span></p>
<p>Per tant els coeficients de la recta de regressió lineal per mínims quadrats de <span class="math inline">\(Y\)</span> (la tensió) en funció de <span class="math inline">\(X\)</span> (la quantitat de sal) són</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="1.1-regressió-lineal-simple.html#cb14-1" aria-hidden="true" tabindex="-1"></a>b1.sal<span class="ot">=</span>s.xy<span class="sc">/</span>s2.x</span>
<span id="cb14-2"><a href="1.1-regressió-lineal-simple.html#cb14-2" aria-hidden="true" tabindex="-1"></a>b0.sal<span class="ot">=</span>y.b<span class="sc">-</span>b1.sal<span class="sc">*</span>x.b</span>
<span id="cb14-3"><a href="1.1-regressió-lineal-simple.html#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">c</span>(b0.sal,b1.sal),<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 86.371  6.335</code></pre>
<p>Obtenim la recta
<span class="math display">\[
\widehat{Y}= 86.371+6.335 X
\]</span>
Segons aquest model, a un augment d’1 g de sal consumida li correspon un augment mitjà de 6.3 mm Hg de pressió arterial.</p>
<p>Així mateix, amb aquest model estimam, per exemple, que la pressió arterial d’una persona que consumeix 3 g diaris de sal és
<span class="math display">\[
86.371+6.335 \cdot 3=105.377\text{ mm Hg}
\]</span></p>
<p>Comprovem que aquesta és la recta que obtenim amb la funció <code>lm</code>:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="1.1-regressió-lineal-simple.html#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(tensió<span class="sc">~</span>sal)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)         sal 
##    86.37079     6.33535</code></pre>

<div class="example">
<span id="exm:Galton" class="example"><strong>Exemple 1.4  </strong></span>Estimem la recta de regressió de les alçades dels fills en funció de les dels pares emprant les dades recollides per Galton. Aquestes dades formen el dataframe <code>Galton</code> del paquet <strong>HistData</strong>.
</div>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="1.1-regressió-lineal-simple.html#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(HistData)</span>
<span id="cb18-2"><a href="1.1-regressió-lineal-simple.html#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(Galton)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    928 obs. of  2 variables:
##  $ parent: num  70.5 68.5 65.5 64.5 64 67.5 67.5 67.5 66.5 66.5 ...
##  $ child : num  61.7 61.7 61.7 61.7 61.7 62.2 62.2 62.2 62.2 62.2 ...</code></pre>
<p>Cada filera del dataframe correspon a un adult: la variable <code>child</code> dóna la seva alçada i la variable <code>parent</code> la mitjana de les alçades dels seus pares, totes dues en polzades (recordau que 1 polzada són 2.54 cm). Calculem a mà i amb R la recta de regressió de la variable de resposta <code>child</code> en funció de la variable de control <code>parent</code>:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="1.1-regressió-lineal-simple.html#cb20-1" aria-hidden="true" tabindex="-1"></a>x.b<span class="ot">=</span><span class="fu">mean</span>(Galton<span class="sc">$</span>parent)</span>
<span id="cb20-2"><a href="1.1-regressió-lineal-simple.html#cb20-2" aria-hidden="true" tabindex="-1"></a>y.b<span class="ot">=</span><span class="fu">mean</span>(Galton<span class="sc">$</span>child)</span>
<span id="cb20-3"><a href="1.1-regressió-lineal-simple.html#cb20-3" aria-hidden="true" tabindex="-1"></a>s2.x<span class="ot">=</span><span class="fu">var</span>(Galton<span class="sc">$</span>parent)</span>
<span id="cb20-4"><a href="1.1-regressió-lineal-simple.html#cb20-4" aria-hidden="true" tabindex="-1"></a>s2.y<span class="ot">=</span><span class="fu">var</span>(Galton<span class="sc">$</span>child)</span>
<span id="cb20-5"><a href="1.1-regressió-lineal-simple.html#cb20-5" aria-hidden="true" tabindex="-1"></a>s.xy<span class="ot">=</span><span class="fu">cov</span>(Galton<span class="sc">$</span>parent,Galton<span class="sc">$</span>child)</span>
<span id="cb20-6"><a href="1.1-regressió-lineal-simple.html#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">c</span>(x.b,y.b,s2.x,s2.y,s.xy),<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 68.308 68.088  3.195  6.340  2.065</code></pre>
<p><span class="math display">\[
\begin{array}{ccccc}
\overline{x} &amp;  \overline{y} &amp; \widetilde{s}_x^2 &amp; \widetilde{s}_y^2 &amp; \widetilde{s}_{xy}\\ \hline
68.308 &amp; 68.088 &amp; 3.195 &amp; 6.34 &amp; 2.065
\end{array}
\]</span></p>
<p>Per tant els coeficients de la recta de regressió lineal per mínims quadrats de <span class="math inline">\(Y\)</span> (l’alçada dels fills) en funció de <span class="math inline">\(X\)</span> (la mitjana de les alçades dels pares) són</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="1.1-regressió-lineal-simple.html#cb22-1" aria-hidden="true" tabindex="-1"></a>b1.Galton<span class="ot">=</span>s.xy<span class="sc">/</span>s2.x</span>
<span id="cb22-2"><a href="1.1-regressió-lineal-simple.html#cb22-2" aria-hidden="true" tabindex="-1"></a>b0.Galton<span class="ot">=</span>y.b<span class="sc">-</span>b1.Galton<span class="sc">*</span>x.b</span>
<span id="cb22-3"><a href="1.1-regressió-lineal-simple.html#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">c</span>(b0.Galton,b1.Galton),<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 23.942  0.646</code></pre>
<p>Obtenim la recta
<span class="math display">\[
\widehat{Y}= 23.942+0.646 X
\]</span>
Segons aquest model, a un augment d’1 polzada (2.54 cm) en l’alçada mitjana dels pares li correspon, de mitjana, un augment de l’alçada del fill de només 0.646 polzades (1.6 cm).</p>
<p>Amb la funció <code>lm</code> obtenim la mateixa recta. Observau la sintaxi per especificar-hi el dataframe</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="1.1-regressió-lineal-simple.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(child<span class="sc">~</span>parent, <span class="at">data=</span>Galton)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)      parent 
##  23.9415302   0.6462906</code></pre>
<p>El fet que la pendent d’aquesta recta sigui més petita que 1 és el que dóna l’efecte de “regressió a la mediocritat” que observà Galton. En efecte, calculem per a quines alçades mitjanes dels pares esperam que els fills siguin més baixos que ells. Si resolem la desigualtat “alçada dels pares més gran que l’alçada esperada dels fills”
<span class="math display">\[
X\geqslant \widehat{Y}=  23.942+0.646 X
\]</span>
obtenim
<span class="math display">\[
X\geqslant \frac{23.942}{1-0.646}=67.69
\]</span>
i això ens diu que si l’alçada mitjana dels pares és més gran que 67.69 polzades, uns 1.72 m, esperam que els fills siguin més baixos que els pares, mentre que, pel contrari, si l’alçada mitjana dels pares està per davall dels 1.72 m, esperam que els fills siguin més alts que els pares.</p>
<p>Algunes de les propietats importants de la regressió per mínims quadrats són:</p>
<ul>
<li><p>Tal i com hem calculat el terme independent <span class="math inline">\(b_0\)</span>, la recta de regressió passa pel punt mitjà <span class="math inline">\((\overline{x},\overline{y})\)</span> de la mostra:
<span class="math display">\[
b_0+b_1 \overline{x}=\overline{y}
\]</span></p></li>
<li><p>La mitjana dels valors estimats de la variable <span class="math inline">\(Y\)</span> als nostres punts és igual a la mitjana dels valors observats:
<span class="math display">\[
\overline{\widehat{y}}=\frac{1}{n}\sum_{i=1}^n\widehat{y}_i
=\frac{1}{n}\sum_{i=1}^n(b_0+b_1x_i)= b_0+b_1 \overline{x}=\overline{y}
\]</span></p></li>
<li><p>Els errors <span class="math inline">\((e_i)_{i=1,\ldots,n}\)</span> de la mostra tenen mitjana 0:
<span class="math display">\[
\begin{array}{l}
\overline{e}
 &amp; \displaystyle =\frac{1}{n}\sum_{i=1}^n e_i
=\frac{1}{n}\sum_{i=1}^n (y_i-b_0-b_1x)
=\frac{1}{n}\sum_{i=1}^n (y_i-\widehat{y}_i)\\[2ex]
&amp; \displaystyle
=\frac{1}{n}\sum_{i=1}^n{y}_i-\frac{1}{n}\sum_{i=1}^n\widehat{y}_i=
\overline{y}-\overline{\widehat{y}}
=0
\end{array}
\]</span></p></li>
<li><p>Els errors <span class="math inline">\((e_i)_{i=1,\ldots,n}\)</span> de la mostra tenen variància
<span class="math display">\[
s_e^2=\frac{1}{n}\Big(\sum_{i=1}^{n}
e^2_i\Big)-\overline{e}^2=\frac{\sum_{i=1}^{n}
e^2_i}{n}=\frac{SS_E}{n}
\]</span>
perquè <span class="math inline">\(\overline{e}=0\)</span> (i recordau que hem dit a <span class="math inline">\(\sum_{i=1}^{n} e^2_i\)</span> la <strong>Suma de Quadrats dels Errors</strong>,<br />
<span class="math inline">\(SS_E\)</span>).</p></li>
</ul>
<p>El teorema següent recull les propietats de la regressió lineal per mínims quadrats com a tècnica d’estimació dels coeficients <span class="math inline">\(\beta_0\)</span> i <span class="math inline">\(\beta_1\)</span>:</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-32" class="theorem"><strong>Teorema 1.2  </strong></span>Si les variables aleatòries error <span class="math inline">\(E_{x_i}\)</span> tenen totes mitjana 0 i la mateixa variància <span class="math inline">\(\sigma^2_E\)</span> i són, dues a dues, incorrelades, aleshores:</p>
<ul>
<li><p><span class="math inline">\(b_0\)</span> i <span class="math inline">\(b_1\)</span> són els estimadors lineals no esbiaixats més eficients (<strong>òptims</strong>) de <span class="math inline">\(\beta_0\)</span> i <span class="math inline">\(\beta_1\)</span></p></li>
<li><p>Un estimador no esbiaixat de <span class="math inline">\(\sigma_E^2\)</span> és
<span class="math display">\[
S^2=\frac{SS_E}{n-2}
\]</span></p></li>
</ul>
<p>Si <strong>a més</strong> les variables aleatòries error <span class="math inline">\(E_{x_i}\)</span> són totes <strong>normals</strong>, aleshores:</p>
<ul>
<li><span class="math inline">\(b_0\)</span> i <span class="math inline">\(b_1\)</span> són els estimadors màxim versemblants de <span class="math inline">\(\beta_0\)</span> i <span class="math inline">\(\beta_1\)</span> (a més de no esbiaixats òptims).
</div></li>
</ul>

<div class="example">
<p><span id="exm:edats5" class="example"><strong>Exemple 1.5  </strong></span>Si suposam a l’Exemple <a href="1-regressió-lineal.html#exm:edats1">1.1</a> que els errors tenen la mateixa variància i són incorrelats, podem estimar aquesta variància de la manera següent:</p>
</div>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="1.1-regressió-lineal-simple.html#cb26-1" aria-hidden="true" tabindex="-1"></a>n<span class="ot">=</span><span class="fu">length</span>(edat)</span>
<span id="cb26-2"><a href="1.1-regressió-lineal-simple.html#cb26-2" aria-hidden="true" tabindex="-1"></a>alçada.cap<span class="ot">=</span>b0.edat<span class="sc">+</span>b1.edat<span class="sc">*</span>edat   <span class="co">#Els valors estimats</span></span>
<span id="cb26-3"><a href="1.1-regressió-lineal-simple.html#cb26-3" aria-hidden="true" tabindex="-1"></a>errors.edat<span class="ot">=</span>alçada<span class="sc">-</span>alçada.cap  <span class="co">#Els errors</span></span>
<span id="cb26-4"><a href="1.1-regressió-lineal-simple.html#cb26-4" aria-hidden="true" tabindex="-1"></a>SS.E<span class="ot">=</span><span class="fu">sum</span>(errors.edat<span class="sc">^</span><span class="dv">2</span>)  <span class="co">#La suma dels quadrats dels errors</span></span>
<span id="cb26-5"><a href="1.1-regressió-lineal-simple.html#cb26-5" aria-hidden="true" tabindex="-1"></a>S2.edat<span class="ot">=</span>SS.E<span class="sc">/</span>(n<span class="dv">-2</span>)  <span class="co">#L&#39;estimació de la variància</span></span>
<span id="cb26-6"><a href="1.1-regressió-lineal-simple.html#cb26-6" aria-hidden="true" tabindex="-1"></a>S2.edat</span></code></pre></div>
<pre><code>## [1] 8.314286</code></pre>
<p>Tenim que <span class="math inline">\(S^2=8.314\)</span>, i estimam que <span class="math inline">\(\sigma_E^2\)</span> val això.</p>

<div class="rmdnote">
No sabem si us hi heu fixat, però perquè la regressió lineal per mínims quadrats tengui bones propietats, <strong>no cal que la variable <span class="math inline">\(Y\)</span></strong> (i molt menys la <span class="math inline">\(X\)</span>, que no cal ni que sigui aleatòria) <strong>sigui normal</strong>. Qui han de ser normals (i amb mitjana 0, la mateixa variància i dues a dues incorrelades) han de ser les variables error.
</div>
<p>Bé, fins ara hem explicat com s’estimen per mínims quadrats els coeficients <span class="math inline">\(\beta_0\)</span> i <span class="math inline">\(\beta_1\)</span> al model
<span class="math display">\[
\mu_{Y|x}=\beta_0+\beta_1 x
\]</span>
però ens pot interessar més:</p>
<ul>
<li><p>Com és de significativa l’estimació obtinguda?</p></li>
<li><p>Quin és l’error típic d’aquests estimadors?</p></li>
<li><p>Quins serien els intervals de confiança d’aquests coeficients per a un nivell de confiança donat?</p></li>
<li><p>Com obtenim un interval de confiança per al valor estimat de <span class="math inline">\(Y\)</span> sobre un subjecte a partir del seu valor de <span class="math inline">\(X\)</span>?</p></li>
</ul>
<p>Amb la funció <code>lm</code>, R calcula molt més que els coeficients de la recta:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="1.1-regressió-lineal-simple.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(alçada<span class="sc">~</span>edat))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = alçada ~ edat)
## 
## Residuals:
##       1       2       3       4       5       6       7 
## -3.7857  0.2857  3.3571  3.4286 -0.5000 -1.4286 -1.3571 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  72.3214     2.1966   32.92 4.86e-07 ***
## edat          6.4643     0.2725   23.73 2.48e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.883 on 5 degrees of freedom
## Multiple R-squared:  0.9912, Adjusted R-squared:  0.9894 
## F-statistic: 562.9 on 1 and 5 DF,  p-value: 2.477e-06</code></pre>
<p>Veurem què és tot això que ens dóna R i per què serveix.</p>
<p>D’entrada, pot ser útil saber que el vector <code>Residuals</code> (que s’obté amb el sufix <code>$residuals</code>) conté el vector dels errors <span class="math inline">\((e_i)_i\)</span>. Comprovem-ho amb les dades de l’Exemple <a href="1-regressió-lineal.html#exm:edats1">1.1</a>, els residus de les quals hem calculat a l’Exemple <a href="1.1-regressió-lineal-simple.html#exm:edats5">1.5</a>:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="1.1-regressió-lineal-simple.html#cb30-1" aria-hidden="true" tabindex="-1"></a>errors.edat</span></code></pre></div>
<pre><code>## [1] -3.7857143  0.2857143  3.3571429  3.4285714 -0.5000000 -1.4285714 -1.3571429</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="1.1-regressió-lineal-simple.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(alçada<span class="sc">~</span>edat))<span class="sc">$</span>residuals</span></code></pre></div>
<pre><code>##          1          2          3          4          5          6          7 
## -3.7857143  0.2857143  3.3571429  3.4285714 -0.5000000 -1.4285714 -1.3571429</code></pre>
</div>
<div id="coeficient-de-determinació" class="section level3" number="1.1.3">
<h3><span class="header-section-number">1.1.3</span> Coeficient de determinació</h3>
<p>Una primera pregunta que ens hem de fer és si la recta de regressió lineal que hem obtingut s’ajusta bé a la mostra obtinguda. Amb un enfocament proper al de l’ANOVA,</p>
<blockquote>
<p>Consideram que la recta de regressió <span class="math inline">\(\widehat{Y}=b_0+b_1X\)</span> ens dóna una bona aproximació de <span class="math inline">\(Y\)</span> com a funció lineal de <span class="math inline">\(X\)</span> sobre la nostra mostra quan la variabilitat dels valors estimats <span class="math inline">\(\widehat{y}_i\)</span> representa una fracció molt gran de la variabilitat dels valors observats <span class="math inline">\(y_i\)</span>.</p>
</blockquote>
<p>Això es quantifica amb el <strong>coeficient de determinació</strong> <span class="math inline">\(R^2\)</span> que tot seguit definim.</p>
<p>Siguin:</p>
<ul>
<li><p><span class="math inline">\(SS_{Tot} =\sum\limits_{i=1}^n(y_i-\overline{y})^2\)</span>: és la <strong>Suma Total de Quadrats</strong> i representa la <strong>variabilitat dels valors observats <span class="math inline">\(y_i\)</span></strong>. Fixau-vos que
<span class="math display">\[
SS_{Tot}=n\cdot s_y^2
\]</span></p></li>
<li><p><span class="math inline">\(SS_R=\sum\limits_{i=1}^n(\widehat{y}_i-\overline{y})^2\)</span>: és la <strong>Suma de Quadrats de la Regressió</strong> i representa la <strong>variabilitat dels valors estimats <span class="math inline">\(\widehat{y}_i\)</span></strong>. Fixau-vos que
<span class="math display">\[
SS_R=n\cdot s_{\widehat{y}}^2
\]</span></p></li>
</ul>
<p>Considerarem que la recta <span class="math inline">\(\widehat{y}=b_0+b_1x\)</span> és una bona aproximació de <span class="math inline">\(Y\)</span> com a funció lineal de <span class="math inline">\(X\)</span> sobre la nostra mostra quan <span class="math inline">\(s^2_{\widehat{y}}\)</span> sigui molt proper a <span class="math inline">\(s^2_y\)</span>. Per mesurar-ho, emprarem el <strong>coeficient de determinació</strong> <span class="math inline">\(R^2\)</span>, que és simplement el seu quocient:
<span class="math display">\[
R^2=\frac{SS_R}{SS_{Tot}}=\frac{s_{\widehat{y}}^2}{s_y^2}
\]</span></p>
<p>Recordau ara que hem definit la <strong>Suma de Quadrats dels Errors</strong> <span class="math inline">\(SS_E=\sum\limits_{i=1}^n(y_i-\widehat{y}_i)^2\)</span> i que
<span class="math display">\[
SS_E=n\cdot s_e^2
\]</span>
on <span class="math inline">\(s_e^2\)</span> és la variància dels errors. A la regressió lineal per mínims quadrats s’hi satisfà la <strong>identitat de les sumes de quadrats</strong> següent:</p>

<div class="theorem">
<span id="thm:unnamed-chunk-37" class="theorem"><strong>Teorema 1.3  </strong></span>En una regressió lineal pel mètode de mínims quadrats,
<span class="math display">\[
SS_{Tot}=SS_R+SS_E
\]</span>
o equivalentment (dividint per la mida <span class="math inline">\(n\)</span> de la mostra),
<span class="math display">\[
s^2_y=s^2_{\widehat{y}}+s^2_e.
\]</span>
</div>

<div class="example">
<p><span id="exm:unnamed-chunk-38" class="example"><strong>Exemple 1.6  </strong></span>Comprovem aquesta igualtat amb les dades de l’Exemple <a href="1-regressió-lineal.html#exm:edats1">1.1</a>:</p>
</div>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="1.1-regressió-lineal-simple.html#cb34-1" aria-hidden="true" tabindex="-1"></a>SS.Tot<span class="ot">=</span><span class="fu">sum</span>((alçada<span class="sc">-</span><span class="fu">mean</span>(alçada))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb34-2"><a href="1.1-regressió-lineal-simple.html#cb34-2" aria-hidden="true" tabindex="-1"></a>SS.R<span class="ot">=</span><span class="fu">sum</span>((alçada.cap<span class="sc">-</span><span class="fu">mean</span>(alçada))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb34-3"><a href="1.1-regressió-lineal-simple.html#cb34-3" aria-hidden="true" tabindex="-1"></a>SS.E<span class="ot">=</span><span class="fu">sum</span>(errors.edat<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb34-4"><a href="1.1-regressió-lineal-simple.html#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(SS.Tot,SS.R,SS.E)</span></code></pre></div>
<pre><code>## [1] 4721.71429 4680.14286   41.57143</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="1.1-regressió-lineal-simple.html#cb36-1" aria-hidden="true" tabindex="-1"></a>SS.R<span class="sc">+</span>SS.E</span></code></pre></div>
<pre><code>## [1] 4721.714</code></pre>
<p>Així, doncs, a la regressió per mínims quadrats</p>
<blockquote>
<p>la variabilitat dels valors observats <span class="math inline">\(y_i\)</span> de <span class="math inline">\(Y\)</span> és igual a la suma de la variabilitat dels valors estimats <span class="math inline">\(\widehat{y}_i\)</span> de <span class="math inline">\(Y\)</span> més la variabilitat dels errors.</p>
</blockquote>
<p>Aleshores, si la regressió lineal és per mínims quadrats,
<span class="math display">\[
R^2=\frac{SS_R}{SS_{Tot}}=\frac{SS_{Tot}-SS_E}{SS_{Tot}}=1-\frac{SS_E}{SS_{Tot}}=1-\frac{s_e^2}{s_y^2}
\]</span>
En particular:</p>

<div class="rmdimportant">
En una regressió per mínims quadrats, <span class="math inline">\(R^2\leqslant 1\)</span>, i <span class="math inline">\(R^2= 1\)</span> exactament quan tots els <span class="math inline">\(e_i\)</span> són 0, és a dir, quan <span class="math inline">\(\widehat{y}_i=y_i\)</span> per a tot <span class="math inline">\(i=1,\ldots,n\)</span>. Com més gran (més proper a 1) sigui <span class="math inline">\(R^2\)</span>, més bona entendrem que és la regressió lineal.
</div>

<div class="rmdnote">
Per si de cas no hi heu caigut, observau que <span class="math inline">\(R^2\geqslant 0\)</span>, perquè és un quocient de quadrats. Només val 0 quan <span class="math inline">\(s^2_{\widehat{y}}=0\)</span>, és a dir, quan tots els valors estimats <span class="math inline">\(\widehat{y}_i\)</span> són iguals (i això només passa quan <span class="math inline">\(b_1=0\)</span>, és a dir, quan <span class="math inline">\(s_{x,y}=0\)</span>).
</div>
<p>R dóna el <span class="math inline">\(R^2\)</span> en el <code>summary(lm( ))</code>: és el valor <code>Multiple R-squared</code> a la penúltima línia de la seva sortida:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="1.1-regressió-lineal-simple.html#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(alçada<span class="sc">~</span>edat))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = alçada ~ edat)
## 
## Residuals:
##       1       2       3       4       5       6       7 
## -3.7857  0.2857  3.3571  3.4286 -0.5000 -1.4286 -1.3571 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  72.3214     2.1966   32.92 4.86e-07 ***
## edat          6.4643     0.2725   23.73 2.48e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.883 on 5 degrees of freedom
## Multiple R-squared:  0.9912, Adjusted R-squared:  0.9894 
## F-statistic: 562.9 on 1 and 5 DF,  p-value: 2.477e-06</code></pre>
<p>S’obté directament del <code>summary(lm( ))</code> amb el sufix <code>$r.squared</code></p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="1.1-regressió-lineal-simple.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(alçada<span class="sc">~</span>edat))<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.9911957</code></pre>
<p>El resultat següent ja l’anunciàrem al Tema <a href="#chap:ED"><strong>??</strong></a>.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-44" class="theorem"><strong>Teorema 1.4  </strong></span>En una regressió lineal per mínims quadrats, el coeficient de determinació és el quadrat de la correlació de Pearson de les mostres de les dues variables:
<span class="math display">\[
R^2=r_{x,y}^2
\]</span>
</div>

<div class="rmdcorbes">
<p>En efecte:
<span class="math display">\[
\begin{array}{rl}
R^2 &amp; \displaystyle =\frac{SS_R}{SS_{Tot}}=\frac{\sum\limits_{i=1}^n(b_1x_i+b_0-\overline{y})^2}{ns_y^2}\\[2ex] 
&amp; \displaystyle =\frac{\sum\limits_{i=1}^n\Big(\dfrac{s_{xy}}{s_x^2}x_i-\dfrac{s_{xy}}{s_x^2}\overline{x}\Big)^2}{ns_y^2} =\frac{\dfrac{s_{xy}^2}{s_x^4}\sum\limits_{i=1}^n(x_i-\overline{x})^2}{ns_y^2}\\[2ex] &amp; \displaystyle =\dfrac{s_{xy}^2}{s_x^4}\cdot \frac{s_x^2}{s_y^2}=\frac{s_{xy}^2}{s_x^2\cdot s_y^2}=r_{xy}^2
\end{array}
\]</span></p>
</div>

<div class="example">
<p><span id="exm:unnamed-chunk-46" class="example"><strong>Exemple 1.7  </strong></span>Comprovem-ho a l’Exemple <a href="1-regressió-lineal.html#exm:edats1">1.1</a>:</p>
</div>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="1.1-regressió-lineal-simple.html#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(alçada<span class="sc">~</span>edat))<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.9911957</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="1.1-regressió-lineal-simple.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(edat,alçada)<span class="sc">^</span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 0.9911957</code></pre>

<div class="example">
<p><span id="exm:unnamed-chunk-48" class="example"><strong>Exemple 1.8  </strong></span>Comprovem ara la identitat de les sumes de quadrats i la igualtat <span class="math inline">\(R^2=r^2\)</span> a l’Exemple <a href="1.1-regressió-lineal-simple.html#exm:sal">1.3</a>:</p>
</div>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="1.1-regressió-lineal-simple.html#cb46-1" aria-hidden="true" tabindex="-1"></a>tensió.cap<span class="ot">=</span>b0.sal<span class="sc">+</span>b1.sal<span class="sc">*</span>sal <span class="co">#Els valors estimats</span></span>
<span id="cb46-2"><a href="1.1-regressió-lineal-simple.html#cb46-2" aria-hidden="true" tabindex="-1"></a>SS.Tot<span class="ot">=</span><span class="fu">sum</span>((tensió<span class="sc">-</span><span class="fu">mean</span>(tensió))<span class="sc">^</span><span class="dv">2</span>) <span class="co">#La Suma Total de Quadrats</span></span>
<span id="cb46-3"><a href="1.1-regressió-lineal-simple.html#cb46-3" aria-hidden="true" tabindex="-1"></a>SS.Tot</span></code></pre></div>
<pre><code>## [1] 331.3333</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="1.1-regressió-lineal-simple.html#cb48-1" aria-hidden="true" tabindex="-1"></a>SS.R<span class="ot">=</span><span class="fu">sum</span>((tensió.cap<span class="sc">-</span><span class="fu">mean</span>(tensió))<span class="sc">^</span><span class="dv">2</span>) <span class="co">#La Suma de Quadrats de la Regressió</span></span>
<span id="cb48-2"><a href="1.1-regressió-lineal-simple.html#cb48-2" aria-hidden="true" tabindex="-1"></a>SS.R</span></code></pre></div>
<pre><code>## [1] 309.5874</code></pre>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="1.1-regressió-lineal-simple.html#cb50-1" aria-hidden="true" tabindex="-1"></a>SS.E<span class="ot">=</span><span class="fu">sum</span>((tensió<span class="sc">-</span>tensió.cap)<span class="sc">^</span><span class="dv">2</span>) <span class="co">#La suma de Quadrats dels Errors</span></span>
<span id="cb50-2"><a href="1.1-regressió-lineal-simple.html#cb50-2" aria-hidden="true" tabindex="-1"></a>SS.E</span></code></pre></div>
<pre><code>## [1] 21.74589</code></pre>
<p>Vegem que <span class="math inline">\(SS_R+SS_E\)</span> és igual a <span class="math inline">\(SS_{Tot}\)</span>:</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="1.1-regressió-lineal-simple.html#cb52-1" aria-hidden="true" tabindex="-1"></a>SS.R<span class="sc">+</span>SS.E</span></code></pre></div>
<pre><code>## [1] 331.3333</code></pre>
<p>Calculem ara <span class="math inline">\(R^2=SS_R/SS_{Tot}\)</span> i comprovem que coincideix amb el valor que dóna R i amb el quadrat de la correlació de Pearson de les mostres de quantitats de sal i tensions:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="1.1-regressió-lineal-simple.html#cb54-1" aria-hidden="true" tabindex="-1"></a>R2<span class="ot">=</span>SS.R<span class="sc">/</span>SS.Tot</span>
<span id="cb54-2"><a href="1.1-regressió-lineal-simple.html#cb54-2" aria-hidden="true" tabindex="-1"></a>R2</span></code></pre></div>
<pre><code>## [1] 0.9343685</code></pre>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="1.1-regressió-lineal-simple.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(tensió<span class="sc">~</span>sal))<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.9343685</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="1.1-regressió-lineal-simple.html#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(sal,tensió)<span class="sc">^</span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 0.9343685</code></pre>

<div class="rmdnote">
Fixau-vos que si coneixeu <span class="math inline">\(\widetilde{s}_y^2\)</span> (<code>var(y)</code>) i <span class="math inline">\(r_{x,y}\)</span> (<code>cor(x,y)</code>), llavors
<span class="math display">\[
r_{x,y}^2=R^2=1-\frac{s_e^2}{s_y^2}\Longrightarrow  s_e^2=s_y^2(1-r_{x,y}^2)
\]</span>
i per tant podeu calcular la <span class="math inline">\(S^2\)</span> que estima la variància comuna dels errors <span class="math inline">\(E_{x_i}\)</span> de la manera següent:
<span class="math display">\[
S^2=\frac{SS_E}{n-2}=\frac{n s_e^2}{n-2}=\frac{ns_y^2(1-r_{x,y}^2)}{n-2}=\frac{(n-1)\widetilde{s}_y^2(1-r_{x,y}^2)}{n-2}
\]</span>
Això us pot ser útil als exercicis.
</div>

<div class="rmdcaution">
El valor de <span class="math inline">\(R^2\)</span> no és suficient per valorar la bondat d’un model de regressió lineal. És sempre convenient també dibuixar els punts i la recta de regressió i donar una ullada.
</div>
<p>Un exemple clàssic de les mancances del <span class="math inline">\(R^2\)</span> són els quatre conjunts de dades <span class="math inline">\((x_{1,i},y_{1,i})_{i=1,\ldots,11}\)</span>, <span class="math inline">\((x_{2,i},y_{2,i})_{i=1,\ldots,11}\)</span>, <span class="math inline">\((x_{3,i},y_{3,i})_{i=1,\ldots,11}\)</span>, <span class="math inline">\((x_{4,i},y_{4,i})_{i=1,\ldots,11}\)</span> que formen el <em>dataframe</em> <code>anscombe</code> de R i que ja empràrem al Tema <a href="#chap:ED"><strong>??</strong></a>:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="1.1-regressió-lineal-simple.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(anscombe)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    11 obs. of  8 variables:
##  $ x1: num  10 8 13 9 11 14 6 4 12 7 ...
##  $ x2: num  10 8 13 9 11 14 6 4 12 7 ...
##  $ x3: num  10 8 13 9 11 14 6 4 12 7 ...
##  $ x4: num  8 8 8 8 8 8 8 19 8 8 ...
##  $ y1: num  8.04 6.95 7.58 8.81 8.33 ...
##  $ y2: num  9.14 8.14 8.74 8.77 9.26 8.1 6.13 3.1 9.13 7.26 ...
##  $ y3: num  7.46 6.77 12.74 7.11 7.81 ...
##  $ y4: num  6.58 5.76 7.71 8.84 8.47 7.04 5.25 12.5 5.56 7.91 ...</code></pre>
<p>Les rectes de regressió per mínims quadrats dels quatre conjunts de dades són gairebé iguals i donen valors de <span class="math inline">\(R^2\)</span> molt semblants:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="1.1-regressió-lineal-simple.html#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(y1<span class="sc">~</span>x1,<span class="at">data=</span>anscombe)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)          x1 
##   3.0000909   0.5000909</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="1.1-regressió-lineal-simple.html#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(y1<span class="sc">~</span>x1,<span class="at">data=</span>anscombe))<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.6665425</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="1.1-regressió-lineal-simple.html#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(y2<span class="sc">~</span>x2,<span class="at">data=</span>anscombe)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)          x2 
##    3.000909    0.500000</code></pre>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="1.1-regressió-lineal-simple.html#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(y2<span class="sc">~</span>x2,<span class="at">data=</span>anscombe))<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.666242</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="1.1-regressió-lineal-simple.html#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(y3<span class="sc">~</span>x3,<span class="at">data=</span>anscombe)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)          x3 
##   3.0024545   0.4997273</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="1.1-regressió-lineal-simple.html#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(y3<span class="sc">~</span>x3,<span class="at">data=</span>anscombe))<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.666324</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="1.1-regressió-lineal-simple.html#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(y4<span class="sc">~</span>x4,<span class="at">data=</span>anscombe)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)          x4 
##   3.0017273   0.4999091</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="1.1-regressió-lineal-simple.html#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(y4<span class="sc">~</span>x4,<span class="at">data=</span>anscombe))<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.6667073</code></pre>
<p>Però si els dibuixam veureu que els seus ajusts a la recta de regressió són molt diferents:</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="1.1-regressió-lineal-simple.html#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb78-2"><a href="1.1-regressió-lineal-simple.html#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(anscombe<span class="sc">$</span>x1,anscombe<span class="sc">$</span>y1,<span class="at">main=</span><span class="st">&quot;Conjunt de dades 1&quot;</span>,<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb78-3"><a href="1.1-regressió-lineal-simple.html#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(y1<span class="sc">~</span>x1,<span class="at">data=</span>anscombe),<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="fl">1.5</span>)</span>
<span id="cb78-4"><a href="1.1-regressió-lineal-simple.html#cb78-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(anscombe<span class="sc">$</span>x2,anscombe<span class="sc">$</span>y2,<span class="at">data=</span>anscombe,<span class="at">main=</span><span class="st">&quot;Conjunt de dades 2&quot;</span>,<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb78-5"><a href="1.1-regressió-lineal-simple.html#cb78-5" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(y2<span class="sc">~</span>x2,<span class="at">data=</span>anscombe),<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="fl">1.5</span>)</span>
<span id="cb78-6"><a href="1.1-regressió-lineal-simple.html#cb78-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(anscombe<span class="sc">$</span>x3,anscombe<span class="sc">$</span>y3,<span class="at">main=</span><span class="st">&quot;Conjunt de dades 3&quot;</span>,<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb78-7"><a href="1.1-regressió-lineal-simple.html#cb78-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(y3<span class="sc">~</span>x3,<span class="at">data=</span>anscombe),<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="fl">1.5</span>)</span>
<span id="cb78-8"><a href="1.1-regressió-lineal-simple.html#cb78-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(anscombe<span class="sc">$</span>x4,anscombe<span class="sc">$</span>y4,<span class="at">main=</span><span class="st">&quot;Conjunt de dades 4&quot;</span>,<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb78-9"><a href="1.1-regressió-lineal-simple.html#cb78-9" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(y4<span class="sc">~</span>x4,<span class="at">data=</span>anscombe),<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="Bioestadistica-II_files/figure-html/unnamed-chunk-56-1.png" width="90%" style="display: block; margin: auto;" /></p>

<div class="rmdnote">
Al Tema <a href="#chap:ED"><strong>??</strong></a> ja us parlàrem del paquet <strong>datasaurus</strong>, les funcions del qual us permeten crear conjunts de punts de “formes” diferents i els mateixos estadístics. Vegem com el nostres dinosaure i estrella tenen rectes de regressió i valors de <span class="math inline">\(R^2\)</span> molt semblants.
</div>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="1.1-regressió-lineal-simple.html#cb79-1" aria-hidden="true" tabindex="-1"></a>datasaure<span class="ot">=</span><span class="fu">read.table</span>(<span class="st">&quot;https://raw.githubusercontent.com/AprendeR-UIB/MatesII/master/Dades/Datasaurus.txt&quot;</span>,<span class="at">header=</span><span class="cn">TRUE</span>,<span class="at">sep=</span><span class="st">&quot;</span><span class="sc">\t</span><span class="st">&quot;</span>)</span>
<span id="cb79-2"><a href="1.1-regressió-lineal-simple.html#cb79-2" aria-hidden="true" tabindex="-1"></a>dino<span class="ot">=</span>datasaure[datasaure<span class="sc">$</span>dataset<span class="sc">==</span><span class="st">&quot;dino&quot;</span>,<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>]</span>
<span id="cb79-3"><a href="1.1-regressió-lineal-simple.html#cb79-3" aria-hidden="true" tabindex="-1"></a>star<span class="ot">=</span>datasaure[datasaure<span class="sc">$</span>dataset<span class="sc">==</span><span class="st">&quot;star&quot;</span>,<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>]</span>
<span id="cb79-4"><a href="1.1-regressió-lineal-simple.html#cb79-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(dino<span class="sc">$</span>y<span class="sc">~</span>dino<span class="sc">$</span>x)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)      dino$x 
##  53.3353196  -0.1011268</code></pre>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="1.1-regressió-lineal-simple.html#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(dino<span class="sc">$</span>y<span class="sc">~</span>dino<span class="sc">$</span>x))<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.0039641</code></pre>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="1.1-regressió-lineal-simple.html#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(star<span class="sc">$</span>y<span class="sc">~</span>star<span class="sc">$</span>x)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)      star$x 
##   53.326679   -0.101113</code></pre>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="1.1-regressió-lineal-simple.html#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(star<span class="sc">$</span>y<span class="sc">~</span>star<span class="sc">$</span>x))<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.0039641</code></pre>
<p><img src="Bioestadistica-II_files/figure-html/unnamed-chunk-59-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="intervals-de-confiança-dels-coeficients" class="section level3" number="1.1.4">
<h3><span class="header-section-number">1.1.4</span> Intervals de confiança dels coeficients</h3>
<p>Suposarem d’ara endavant que <strong>cada <span class="math inline">\(E_{x_i}\)</span> segueix una distribució normal amb mitjana <span class="math inline">\(\mu_{E_{x_i}}=0\)</span> i totes amb la mateixa variància <span class="math inline">\(\sigma_E^2\)</span>, i que <span class="math inline">\(\sigma_{E_{x_i},E_{x_j}}=0\)</span> per a cada parella <span class="math inline">\(i,j\)</span></strong>. Recordau que sota aquestes condicions, els estimadors per mínims quadrats <span class="math inline">\(b_0\)</span> i <span class="math inline">\(b_1\)</span> de <span class="math inline">\(\beta_0\)</span> i <span class="math inline">\(\beta_1\)</span> són màxim versemblants i no esbiaixats òptims.</p>
<p>Si tenim molt pocs valors <span class="math inline">\(y\)</span> per a cada <span class="math inline">\(x\)</span> a la mostra, això no es pot contrastar amb un mínim raonable de potència, però si és veritat, implica que els <span class="math inline">\((e_i)_{i=1,\ldots,n}\)</span> s’ajusten a una variable <span class="math inline">\(N(0,\sigma_E^2)\)</span>, amb <span class="math inline">\(\sigma_E^2\)</span> estimada per <span class="math inline">\(S^2\)</span>, i això sí que ho podem contrastar. Si ho podem rebutjar, hem de rebutjar que els <span class="math inline">\(E_{x_i}\)</span> satisfan les condicions requerides.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-60" class="example"><strong>Exemple 1.9  </strong></span>A l’Exemple <a href="1.1-regressió-lineal-simple.html#exm:edats">1.2</a>:</p>
</div>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="1.1-regressió-lineal-simple.html#cb87-1" aria-hidden="true" tabindex="-1"></a>SS.E.edat<span class="ot">=</span><span class="fu">sum</span>(errors.edat<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb87-2"><a href="1.1-regressió-lineal-simple.html#cb87-2" aria-hidden="true" tabindex="-1"></a>S2.edat<span class="ot">=</span>SS.E.edat<span class="sc">/</span>(<span class="fu">length</span>(edat)<span class="sc">-</span><span class="dv">2</span>)  <span class="co">#L&#39;estimació de la variància comuna dels errors</span></span>
<span id="cb87-3"><a href="1.1-regressió-lineal-simple.html#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ks.test</span>(errors.edat,<span class="st">&quot;pnorm&quot;</span>,<span class="dv">0</span>,<span class="fu">sqrt</span>(S2.edat))</span></code></pre></div>
<pre><code>## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  errors.edat
## D = 0.17482, p-value = 0.958
## alternative hypothesis: two-sided</code></pre>
<p>Podem acceptar que els errors s’ajusten a una variable normal de mitjana 0.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-62" class="example"><strong>Exemple 1.10  </strong></span>A l’Exemple <a href="1.1-regressió-lineal-simple.html#exm:sal">1.3</a>:</p>
</div>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="1.1-regressió-lineal-simple.html#cb89-1" aria-hidden="true" tabindex="-1"></a>errors.sal<span class="ot">=</span><span class="fu">summary</span>(<span class="fu">lm</span>(tensió<span class="sc">~</span>sal))<span class="sc">$</span>residuals</span>
<span id="cb89-2"><a href="1.1-regressió-lineal-simple.html#cb89-2" aria-hidden="true" tabindex="-1"></a>SS.E.sal<span class="ot">=</span><span class="fu">sum</span>(errors.sal<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb89-3"><a href="1.1-regressió-lineal-simple.html#cb89-3" aria-hidden="true" tabindex="-1"></a>S2.sal<span class="ot">=</span>SS.E.sal<span class="sc">/</span>(<span class="fu">length</span>(sal)<span class="sc">-</span><span class="dv">2</span>)</span>
<span id="cb89-4"><a href="1.1-regressió-lineal-simple.html#cb89-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ks.test</span>(errors.sal,<span class="st">&quot;pnorm&quot;</span>,<span class="dv">0</span>,<span class="fu">sqrt</span>(S2.sal))</span></code></pre></div>
<pre><code>## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  errors.sal
## D = 0.25544, p-value = 0.7472
## alternative hypothesis: two-sided</code></pre>
<p>També podem acceptar que els errors s’ajusten a una variable normal de mitjana 0.</p>
<p>Per cert, R calcula la <span class="math inline">\(S\)</span>, l’arrel quadrada d’aquesta <span class="math inline">\(S^2\)</span>, en fer la <code>lm</code>. És el <code>Residual standard error</code> de la tercera línia començant per avall a la sortida del <code>summary(lm( ))</code> i s’obté amb el sufix <code>$sigma</code>:</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="1.1-regressió-lineal-simple.html#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(tensió<span class="sc">~</span>sal))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = tensió ~ sal)
## 
## Residuals:
##      1      2      3      4      5      6 
##  2.226 -2.309  1.455 -1.712 -1.613  1.952 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  86.3708     3.0621  28.206  9.4e-06 ***
## sal           6.3354     0.8395   7.546  0.00165 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.332 on 4 degrees of freedom
## Multiple R-squared:  0.9344, Adjusted R-squared:  0.918 
## F-statistic: 56.95 on 1 and 4 DF,  p-value: 0.001652</code></pre>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="1.1-regressió-lineal-simple.html#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(tensió<span class="sc">~</span>sal))<span class="sc">$</span>sigma</span></code></pre></div>
<pre><code>## [1] 2.331625</code></pre>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="1.1-regressió-lineal-simple.html#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(S2.sal)</span></code></pre></div>
<pre><code>## [1] 2.331625</code></pre>
<p>Resulta que si se satisfan les condicions demanades sobre les variables <span class="math inline">\(E_{x_i}\)</span>, aleshores coneixem els errors típics dels estimadors <span class="math inline">\(b_1\)</span> i <span class="math inline">\(b_0\)</span> i uns estadístics associats a aquests estimadors segueixen lleis t de Student que permeten calcular intervals de confiança per a <span class="math inline">\(\beta_0\)</span> i <span class="math inline">\(\beta_1\)</span>. En concret:</p>
<ul>
<li><p>Pel que fa a <span class="math inline">\(b_1\)</span>,</p>
<ul>
<li>El seu error típic és
<span class="math display">\[
\frac{\sigma_E}{s_x\sqrt{n}}.
\]</span></li>
<li>L’estimació d’aquest error típic sobre una mostra concreta és
<span class="math display">\[
\frac{S}{s_x\sqrt{n}}
\]</span></li>
<li>La fracció
<span class="math display">\[
T_1=\frac{b_1-\beta_1}{\frac{S}{s_x\sqrt{n}}}
\]</span>
segueix una llei <span class="math inline">\(t\)</span> de Student amb <span class="math inline">\(n-2\)</span> graus de llibertat.</li>
</ul></li>
</ul>

<div class="rmdnote">
<p>Observau que l’error típic de <span class="math inline">\(b_1\)</span>:</p>
<ul>
<li><p>Decreix amb <span class="math inline">\(n\)</span>: com més gran és la mostra, menys incertesa esperam en l’estimació de <span class="math inline">\(\beta_1\)</span>. Això ens ha passat en totes les estimacions del curs, no és cap sorpresa. En general, com més dades tenim, millor.</p></li>
<li><p>Decreix amb <span class="math inline">\(s_x\)</span>: com més dispersa és la mostra, menys incertesa esperam en l’estimació de <span class="math inline">\(\beta_1\)</span>. Això és una novetat, en altres casos (per exemple, en estimar una mitjana) la incertesa creixia amb la desviació típica de la mostra. Però aquí és raonable. Pensau en termes físics: si voleu unir dos punts amb una recta, com és més fàcil que aquesta recta sigui estable, si els dos punts estan molt junts o si estan separats? Separats, no?</p></li>
<li><p>Creix amb <span class="math inline">\(\sigma_E\)</span>: com més variabilitat tenguin els errors residuals, més incertesa tendrem. Fixau-vos que si <span class="math inline">\(\sigma_E=0\)</span>, aleshores no hi ha gens d’incertesa: significa que els punts <span class="math inline">\((x_i,y_i)\)</span> estan sobre una recta i aquesta recta és la de regressió.</p></li>
</ul>
</div>
<ul>
<li><p>Pel que fa a <span class="math inline">\(b_0\)</span>,</p>
<ul>
<li>El seu error típic és
<span class="math display">\[
\frac{\sigma_E\sqrt{s_x^2+\overline{x}^2}}{s_x\sqrt{n}}
\]</span></li>
<li>L’estimació d’aquest error típic sobre una mostra concreta és
<span class="math display">\[
\frac{S\sqrt{s_x^2+\overline{x}^2}}{s_x\sqrt{n}}
\]</span></li>
<li>La fracció
<span class="math display">\[
T_0=\frac{b_0-\beta_0}{\frac{S\sqrt{s_x^2+\overline{x}^2}}{s_x\sqrt{n}}}
\]</span>
també segueix una llei <span class="math inline">\(t\)</span> de Student amb <span class="math inline">\(n-2\)</span> graus de llibertat.</li>
</ul></li>
</ul>

<div class="rmdnote">
És raonable esperar que l’error típic de <span class="math inline">\(b_0\)</span> sigui més gran que el de <span class="math inline">\(b_1\)</span>, perquè per calcular <span class="math inline">\(b_0\)</span> hem de calcular primer <span class="math inline">\(b_1\)</span> i a més emprar-hi les mitjanes <span class="math inline">\(\overline{x}\)</span> i <span class="math inline">\(\overline{y}\)</span>, la qual cosa fa que en estimar <span class="math inline">\(\beta_0\)</span> per mitjà de <span class="math inline">\(b_0\)</span> hi hagi més incertesa que en estimar <span class="math inline">\(\beta_1\)</span> per mitjà de <span class="math inline">\(b_1\)</span>.
</div>
<p>Com que els estadístics <span class="math inline">\(T_1\)</span> i <span class="math inline">\(T_0\)</span> tenen distribucions t de Student, operant amb ells com en el cas de l’interval de confiança per a la mitjana poblacional <span class="math inline">\(\mu\)</span> obtenim fórmules per als intervals de confiança per a <span class="math inline">\(\beta_1\)</span> i <span class="math inline">\(\beta_1\)</span> de la forma que ens agrada:
<span class="math display">\[
\text{estimador}\pm \text{quantil}\times \text{error típic}
\]</span></p>
<p>En concret, sota les hipòtesis imposades al principi d’aquesta secció</p>
<ul>
<li><p>Un interval de confiança amb nivell de confiança <span class="math inline">\(q\)</span> per a <span class="math inline">\(\beta_1\)</span> és
<span class="math display">\[
b_1\pm t_{n-2,(1+q)/2}\cdot \frac{S}{s_x\sqrt{n}}
\]</span></p></li>
<li><p>Un interval de confiança amb nivell de confiança <span class="math inline">\(q\)</span> per a <span class="math inline">\(\beta_0\)</span> és
<span class="math display">\[
b_0\pm t_{n-2,(1+q)/2}\cdot \frac{S\sqrt{s_x^2+\overline{x}^2}}{s_x\sqrt{n}}
\]</span></p></li>
</ul>

<div class="example">
<p><span id="exm:edatsIC" class="example"><strong>Exemple 1.11  </strong></span>Tornem a l’Exemple <a href="1-regressió-lineal.html#exm:edats1">1.1</a>. Hi havíem obtingut la recta de regressió</p>
</div>
<p><span class="math display">\[
\widehat{Y}=72.321+6.464X
\]</span>
i a més <span class="math inline">\(n=7\)</span> i havíem calculat que <span class="math inline">\(\overline{x}=7\)</span>, <span class="math inline">\(s_x^2=18.667\)</span> i <span class="math inline">\(S^2=3.624\)</span>.</p>
<p>Aleshores:</p>
<ul>
<li><p>Un interval de confiança al 95% per <span class="math inline">\(\beta_1\)</span> és
<span class="math display">\[
\begin{array}{l}
\displaystyle b_1\pm t_{n-2,(1+0.95)/2}\cdot \frac{S}{s_x\sqrt{n}} =6.464\pm t_{5,0.975}\cdot \frac{\sqrt{8.314}}{4\sqrt{7}}\\[2ex]
\qquad =
6.464\pm 2.5706 \cdot 0.2724=6.464\pm  0.7
\end{array}
\]</span>
Obtenim l’interval <span class="math inline">\([5.764,7.164]\)</span>.</p></li>
<li><p>Un interval de confiança al 95% per a <span class="math inline">\(\beta_0\)</span> és
<span class="math display">\[
\begin{array}{l}
\displaystyle b_0\pm t_{n-2,(1+0.95)/2}\cdot\frac{S\sqrt{s_x^2+\overline{x}^2}}{s_x\sqrt{n}}
=72.321\pm t_{5,0.975}\cdot \frac{\sqrt{8.314}\cdot\sqrt{16+7^2}}{4\sqrt{7}}\\[2ex]
\qquad =
72.321\pm 2.5706 \cdot 2.1966=72.321\pm 5.647
\end{array}
\]</span>
Obtenim l’interval <span class="math inline">\([66.674,77.968]\)</span>.</p></li>
</ul>
<p>Amb R aquests intervals de confiança s’obtenen amb la funció <code>confint</code> aplicada al resultat de la <code>lm</code>. El nivell de confiança s’hi indica amb el paràmetre <code>level</code> i el seu valor per defecte és, com sempre, 0.95.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="1.1-regressió-lineal-simple.html#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(<span class="fu">lm</span>(alçada<span class="sc">~</span>edat),<span class="at">level=</span><span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>##                 2.5 %    97.5 %
## (Intercept) 66.674769 77.968088
## edat         5.763904  7.164668</code></pre>

<div class="rmdexercici">
Calculau “a mà”, amb les fórmules que hem donat, els intervals de confiança per als coeficients de la recta de regressió de l’Exemple <a href="1.1-regressió-lineal-simple.html#exm:sal">1.3</a>. Us han de donar:
</div>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="1.1-regressió-lineal-simple.html#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(<span class="fu">lm</span>(tensió<span class="sc">~</span>sal))</span></code></pre></div>
<pre><code>##                 2.5 %    97.5 %
## (Intercept) 77.869064 94.872509
## sal          4.004434  8.666266</code></pre>
</div>
<div id="intervals-de-confiança-per-a-les-estimacions-de-la-variable-dependent" class="section level3" number="1.1.5">
<h3><span class="header-section-number">1.1.5</span> Intervals de confiança per a les estimacions de la variable dependent</h3>
<p>També podem calcular intervals de confiança per al valor estimat de la <span class="math inline">\(Y\)</span> sobre els individus amb un valor de <span class="math inline">\(X\)</span> donat. En aquest cas, tenim dos intervals:</p>
<ul>
<li><p>L’interval per al <strong>valor esperat</strong> <span class="math inline">\(\mu_{Y|x_0}\)</span> de <span class="math inline">\(Y\)</span> sobre els individus en els que <span class="math inline">\(X\)</span> val <span class="math inline">\(x_0\)</span>, és a dir, per al valor mitjà de la <span class="math inline">\(Y\)</span> sobre tots els individus de la població en els que <span class="math inline">\(X\)</span> valgui <span class="math inline">\(x_0\)</span>.</p></li>
<li><p>L’interval per al <strong>valor predit</strong> <span class="math inline">\(y_0\)</span> de <span class="math inline">\(Y\)</span> sobre un individu concret en el que <span class="math inline">\(X\)</span> valgui <span class="math inline">\(x_0\)</span>.</p></li>
</ul>
<p>Tot i que tant el valor esperat <span class="math inline">\(\mu_{Y|x_0}\)</span> com el valor <span class="math inline">\(y_0\)</span> de <span class="math inline">\(Y\)</span> sobre un individu concret en el que <span class="math inline">\(X\)</span> valgui <span class="math inline">\(x_0\)</span> tenen el mateix valor estimat,
<span class="math display">\[
\widehat{y}_0=b_0+b_1x_0,
\]</span>
l’interval de confiança del valor esperat serà més estret que el del valor sobre un individu concret. Això reflecteix el fet que, naturalment, hi ha molta més incertesa en saber què val la <span class="math inline">\(Y\)</span> sobre un individu concret que en saber quin és el valor mitjà de <span class="math inline">\(Y\)</span> sobre tots els individus que tenguin el mateix valor de <span class="math inline">\(X\)</span> que aquest individu concret.</p>
<p>Bé passem a les fórmules. Sota les condicions sobre els errors que hem suposat al començament de la secció anterior (variables error normals de mitjana 0 i mateixa desviació típica, i incorrelades dues a dues):</p>
<ul>
<li><p>L’error típic de <span class="math inline">\(\widehat{y}_0\)</span> com a estimador de <span class="math inline">\(\mu_{Y|x_0}\)</span> és
<span class="math display">\[
\sigma_E\cdot \sqrt{\frac{1}{n}+\frac{(x_0-\overline{x})^2}{ns^2_x}}
\]</span>
i la fracció
<span class="math display">\[
\frac{\widehat{y}_0-\mu_{Y/x_0}}{S\cdot \sqrt{\frac{1}{n}+\frac{(x_0-\overline{x})^2}{n
s^2_x}}}
\]</span>
segueix una llei <span class="math inline">\(t\)</span> de Student amb <span class="math inline">\(n-2\)</span> graus de llibertat.</p></li>
<li><p>L’error típic de <span class="math inline">\(\widehat{y}_0\)</span> com a estimador de <span class="math inline">\(y_0\)</span> és
<span class="math display">\[
\sigma_E\cdot \sqrt{1+\frac{1}{n}+\frac{(x_0-\overline{x})^2}{ns^2_x}}
\]</span>
i la fracció
<span class="math display">\[
\frac{\widehat{y}_0-y_0}{S\cdot \sqrt{1+\frac{1}{n}+\frac{(x_0-\overline{x})^2}{n
s^2_x}}}
\]</span>
segueix una llei <span class="math inline">\(t\)</span> de Student amb <span class="math inline">\(n-2\)</span> graus de llibertat.</p></li>
</ul>

<div class="rmdimportant">
Fixau-vos que l’error típic de l’estimació de <span class="math inline">\(\mu_{Y|x_0}\)</span> és més petit que el de l’estimació de <span class="math inline">\(y_0\)</span>.
</div>

<div class="rmdnote">
Aquests errors típics creixen amb la distància entre <span class="math inline">\(x_0\)</span> i la mitjana de la mostra <span class="math inline">\(\overline{x}\)</span>. És raonable: la recta passa per <span class="math inline">\((\overline{x},\overline{y})\)</span> i a partir d’aquest punt, com més enfora estigui <span class="math inline">\(x_0\)</span>, petites variacions en el valor de la pendent de la recta donaran lloc a diferències molt més grans en el valor de <span class="math inline">\(b_0+b_1x_0\)</span>.
</div>
<p>Per tant, sota aquestes hipòtesis,</p>
<ul>
<li><p>Un interval de confiança de nivell de confiança <span class="math inline">\(q\)</span> per a <span class="math inline">\(\mu_{Y|x_0}\)</span> és
<span class="math display">\[
\widehat{y}_0\pm t_{n-2,(1+q)/2}\cdot S\cdot \sqrt{\frac{1}{n}+\frac{(x_0-\overline{x})^2}{n
s^2_x}}
\]</span></p></li>
<li><p>Un interval de confiança de nivell de confiança <span class="math inline">\(q\)</span> per a <span class="math inline">\(y_0\)</span> és
<span class="math display">\[
\widehat{y}_0\pm t_{n-2,(1+q)/2}\cdot S\cdot \sqrt{1+\frac{1}{n}+\frac{(x_0-\overline{x})^2}{n
s^2_x}}
\]</span></p></li>
</ul>

<div class="example">
<p><span id="exm:unnamed-chunk-73" class="example"><strong>Exemple 1.12  </strong></span>Tornem una altra vegada a l’Exemple <a href="1-regressió-lineal.html#exm:edats1">1.1</a>. Hi havíem obtingut la recta de regressió</p>
</div>
<p><span class="math display">\[
\widehat{Y}=72.321+6.464X
\]</span>
i a més <span class="math inline">\(n=7\)</span> i havíem calculat que <span class="math inline">\(\overline{x}=7\)</span>, <span class="math inline">\(s_x^2=18.667\)</span> i <span class="math inline">\(S^2=3.624\)</span>.</p>
<p>Suposem que volem estimar l’alçada <span class="math inline">\(y_0\)</span> d’un nin de <span class="math inline">\(x_0=10\)</span> anys. L’estimació amb la recta de regressió és
<span class="math display">\[
\widehat{y}_0=72.321+6.464\cdot 10=136.964
\]</span></p>
<p>Ara volem saber els intervals de confiança del 95% per a aquesta estimació:</p>
<ul>
<li><p>Un interval de confiança del 95% per a <span class="math inline">\(y_0\)</span> és
<span class="math display">\[
\begin{array}{l}
\displaystyle
\widehat{y}_0\pm t_{n-2,(1+0.95)/2}\cdot S\sqrt{1+\frac{1}{n}+\frac{(x_0-\overline{x})^2}{ns^2_x}}
\\[2ex]
\displaystyle\qquad=136.961\pm t_{5,0.975}\cdot \sqrt{8.314}\cdot\sqrt{1+\frac{1}{7}+\frac{(10-7)^2}{7\cdot 16 }}\\[2ex]
\qquad =
136.961\pm 2.5706 \cdot  3.189=136.961\pm  8.198
\end{array}
\]</span>
Obtenim l’interval <span class="math inline">\([128.8,145.2]\)</span>. Per tant, estam molt segurs que si prenem un nin d’10 anys, la seva alçada estarà entre els 128.8 cm i els 145.2 cm.</p></li>
<li><p>Un interval de confiança del 95% per al <strong>valor esperat</strong> <span class="math inline">\(\mu_{Y|x_0}\)</span> de <span class="math inline">\(y_0\)</span> és
<span class="math display">\[
\begin{array}{l}
\displaystyle
\widehat{y}_0\pm t_{n-2,(1+0.95)/2}\cdot S\sqrt{\frac{1}{n}+\frac{(x_0-\overline{x})^2}{ns^2_x}}
\\[2ex]
\displaystyle\qquad=136.961\pm t_{5,0.975}\cdot \sqrt{8.314}\cdot\sqrt{\frac{1}{7}+\frac{(10-7)^2}{7\cdot 16 }}\\[2ex]
\qquad =
136.961\pm 2.5706 \cdot  1.362=136.961\pm  3.501
\end{array}
\]</span>
Obtenim l’interval <span class="math inline">\([133.5, 140.5]\)</span>. Per tant, estam molt segurs que l’alçada mitjana dels nins d’10 anys està entre els 133.5 cm i els 140.5 cm.</p></li>
</ul>
<p>Si en canvi volguéssim emprar aquesta recta per estimar l’alçada d’un nin d’15 anys, els intervals que obtenim són (comprovau-ho):</p>
<ul>
<li><p>Per a <span class="math inline">\(y_0\)</span>, <span class="math inline">\([159.6, 179]\)</span></p></li>
<li><p>Per a <span class="math inline">\(\mu_{Y|x_0}\)</span>, <span class="math inline">\([163, 175.6]\)</span></p></li>
</ul>
<p>Com veieu, són molt més amples que els intervals de confiança per als 10 anys.</p>
<p>Amb R, aquests intervals es calculen amb la funció <code>predict.lm</code> aplicada a</p>
<ul>
<li><p>el resultat de la <code>lm</code></p></li>
<li><p>un <em>data frame</em> amb el valor (o els valors, si ho volem fer de cop per a més d’un valor) de <span class="math inline">\(X\)</span></p></li>
<li><p>el paràmetre <code>interval</code> igualat al tipus d’interval que volem:</p>
<ul>
<li><code>"prediction"</code> si és per al valor en un individu,</li>
<li><code>"confidence"</code> si és per al valor esperat</li>
</ul></li>
</ul>
<p>A més, s’hi pot entrar el nivell de significació amb el paràmetre <code>level</code>; si és 0.95, no cal. El resultat és un dataframe amb tres columnes: <code>fit</code>, el valor predit, i <code>lwr</code> i <code>upr</code>, els extrems inferior i superior de l’interval.</p>
<p>En el nostre exemple, primer hem de definir un data frame amb l’edat o les edats. Calcularem els intervals de confiança per als 10 i 15 anys. Per tant, definim un <em>data frame</em> format per dues observacions de la variable <code>edat</code>, que valguin 10 i 15:</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="1.1-regressió-lineal-simple.html#cb101-1" aria-hidden="true" tabindex="-1"></a>nin<span class="ot">=</span><span class="fu">data.frame</span>(<span class="at">edat=</span><span class="fu">c</span>(<span class="dv">10</span>,<span class="dv">15</span>))</span></code></pre></div>
<p>Aleshores, els intervals de confiança del 95% per a les alçades d’un nin d’10 anys i d’un nin d’15 anys són, respectivament,</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="1.1-regressió-lineal-simple.html#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict.lm</span>(<span class="fu">lm</span>(alçada<span class="sc">~</span>edat),nin,<span class="at">interval=</span><span class="st">&quot;prediction&quot;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 136.9643 128.7665 145.1620
## 2 169.2857 159.5809 178.9905</code></pre>
<p>i els intervals de confiança del 95% per a les alçades mitjanes dels nins d’10 i d’15 anys són, respectivament,</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="1.1-regressió-lineal-simple.html#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict.lm</span>(<span class="fu">lm</span>(alçada<span class="sc">~</span>edat),nin,<span class="at">interval=</span><span class="st">&quot;confidence&quot;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 136.9643 133.4624 140.4662
## 2 169.2857 163.0213 175.5501</code></pre>
</div>
<div id="té-sentit-una-regressió-lineal" class="section level3" number="1.1.6">
<h3><span class="header-section-number">1.1.6</span> Té sentit una regressió lineal?</h3>
<p>Si <span class="math inline">\(\beta_1=0\)</span>, el model de regressió lineal no té sentit, perquè en aquest cas
<span class="math display">\[
Y|x=\beta_0+E_x
\]</span>
i les variacions en els valors de <span class="math inline">\(Y\)</span> es deuen tan sols als errors aleatoris.</p>
<p>El contrast
<span class="math display">\[
\left\{\begin{array}{l}
H_0:\beta_1=0\\
H_1:\beta_1 \neq 0
\end{array}
\right.
\]</span>
el podem realitzar amb l’interval de confiança per a <span class="math inline">\(\beta_1\)</span>: si 0 no hi pertany,
rebutjam la hipòtesi nul·la amb el nivell de significació corresponent al nivell de confiança de l’interval.</p>
<p>Per exemple, a l’Exemple <a href="1.1-regressió-lineal-simple.html#exm:edatsIC">1.11</a> hem obtingut l’IC 95% per a <span class="math inline">\(\beta_1\)</span> <span class="math inline">\([5.764,7.164]\)</span>. Com que no conté el 0, concloem (amb un nivell de significació de 0.05) que <span class="math inline">\(\beta_1\neq 0\)</span>.</p>
<p>Si mirau la sortida del <code>summary(lm( ))</code></p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="1.1-regressió-lineal-simple.html#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(alçada<span class="sc">~</span>edat))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = alçada ~ edat)
## 
## Residuals:
##       1       2       3       4       5       6       7 
## -3.7857  0.2857  3.3571  3.4286 -0.5000 -1.4286 -1.3571 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  72.3214     2.1966   32.92 4.86e-07 ***
## edat          6.4643     0.2725   23.73 2.48e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.883 on 5 degrees of freedom
## Multiple R-squared:  0.9912, Adjusted R-squared:  0.9894 
## F-statistic: 562.9 on 1 and 5 DF,  p-value: 2.477e-06</code></pre>
<p>a la matriu <code>Coefficients</code>:</p>
<ul>
<li><p>Els <code>Estimate</code> són les estimacions <span class="math inline">\(b_0\)</span> i <span class="math inline">\(b_1\)</span></p></li>
<li><p>Els <code>Std. Error</code> són (les estimacions de) els seus errors típics</p></li>
<li><p>Els <code>t value</code> són els valors dels estadístics de contrast dels contrastos bilaterals amb hipòtesi nul·la <span class="math inline">\(H_0:\)</span> “coeficient <span class="math inline">\(=0\)</span>”; aquests estadístics de contrast són justament els estadístics <span class="math inline">\(T_0\)</span> i <span class="math inline">\(T_1\)</span> que hem definit fa una estona (substituint-hi <span class="math inline">\(\beta_0\)</span> i <span class="math inline">\(\beta_1\)</span> pels valors que contrastam, 0)</p></li>
<li><p>Els <code>Pr(&gt;|t|)</code> són els p-valors d’aquests contrastos (que no us engani la notació, aquests p-valors es defineixen com toca:
<span class="math display">\[
2P(t_{n-2}\geqslant |T_0|),\quad 2P(t_{n-2}\geqslant |T_1|)
\]</span>
respectivament).</p>
<p>Com veiem, en aquest cas podem rebutjar amb <span class="math inline">\(\alpha=0.05\)</span> que <span class="math inline">\(\beta_1=0\)</span> (i també que <span class="math inline">\(\beta_0=0\)</span>).</p></li>
</ul>
</div>
</div>
<p style="text-align: center;">
<a href="1-regressió-lineal.html"><button class="btn btn-default">Previous</button></a>
<a href="1.2-regressió-lineal-múltiple.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
